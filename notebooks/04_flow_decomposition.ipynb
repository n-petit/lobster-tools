{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETF flow decomposition\n",
    "\n",
    "> Nearest neighbor functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp flow_decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n",
    "from lobster_tools.preprocessing import *\n",
    "from lobster_tools.querying import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from typing import Literal, Optional, get_args\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.neighbors import KDTree\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "tolerances = [\"500us\", \"1ms\"]\n",
    "resample_freq = \"5T\"\n",
    "equities = [\"AIG\", \"AAPL\"]\n",
    "etfs = [\"WMT\", \"GE\"]\n",
    "date_range = \"2019-01-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_times(df: pd.DataFrame) -> NDArray[np.datetime64]:\n",
    "    #TODO better way to rewrite this?\n",
    "    if df.index.values.dtype == \"datetime64[ns]\":\n",
    "        return df.index.values.reshape(-1, 1)\n",
    "    if \"datetime\" not in df.columns:\n",
    "        raise ValueError(\"No datetime column found in df, or datetimes in index\")\n",
    "    if df[\"datetime\"].dtype != \"datetime64[ns]\":\n",
    "        raise ValueError(\"datetime column is not datetime64[ns] dtype\")\n",
    "    else:\n",
    "        return df[\"datetime\"].values.reshape(-1, 1)\n",
    "\n",
    "def str_to_time(time: str, convert_to: str) -> int:\n",
    "    return pd.Timedelta(time) / pd.Timedelta(1, unit=convert_to)\n",
    "\n",
    "\n",
    "# TODO: find a better way to compose without lambda\n",
    "str_to_nanoseconds = lambda x: int(str_to_time(x, convert_to=\"ns\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def add_neighbors(\n",
    "    etf_executions: pd.DataFrame,\n",
    "    equity_executions: pd.DataFrame,\n",
    "    tolerance: str | list[str],\n",
    "):\n",
    "    \"\"\"Annotate the etf execution dataframe with the indices of the neighbouring equity executions.\n",
    "    Note: Building the KDTree on the equity dataframe.\n",
    "    \"\"\"\n",
    "    # new addition so it's not inplace    \n",
    "    etf_executions = etf_executions.copy()\n",
    "\n",
    "    etf_executions_times = get_times(etf_executions)\n",
    "    equity_executions_times = get_times(equity_executions)\n",
    "    equity_tree = KDTree(equity_executions_times, metric=\"l1\")\n",
    "\n",
    "    def _add_neighbors_col(etf_executions, tolerance_str):\n",
    "        tolerance_in_nanoseconds = str_to_nanoseconds(tolerance_str)\n",
    "        etf_executions[f\"_{tolerance_str}_neighbors\"] = equity_tree.query_radius(etf_executions_times, r=tolerance_in_nanoseconds)\n",
    "\n",
    "        etf_executions[f\"_{tolerance_str}_neighbors\"] = etf_executions[f\"_{tolerance_str}_neighbors\"].apply(\n",
    "            lambda x: None if x.size == 0 else x\n",
    "        )\n",
    "\n",
    "    if isinstance(tolerance, str):\n",
    "        _add_neighbors_col(etf_executions, tolerance)\n",
    "\n",
    "    elif isinstance(tolerance, list):\n",
    "        for tolerance in tolerance:\n",
    "            _add_neighbors_col(etf_executions, tolerance)\n",
    "    else:\n",
    "        raise ValueError(\"tolerance_str must be a string or a list of strings\")\n",
    "    \n",
    "    return etf_executions\n",
    "\n",
    "\n",
    "def drop_all_neighbor_cols(df: pd.DataFrame):\n",
    "    \"Drop neighbor columns inplace.\"\n",
    "    neighbor_column_names = df.filter(regex=\"neighbors\").columns\n",
    "    df.drop(columns=neighbor_column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def col_to_dtype_inputing_mapping(col, col_to_dtype_dict):\n",
    "    for k, v in col_to_dtype_dict.items():\n",
    "        if k in col:\n",
    "            return v\n",
    "\n",
    "\n",
    "col_to_dtype = partial(\n",
    "    col_to_dtype_inputing_mapping,\n",
    "    col_to_dtype_dict={\n",
    "        \"notional\": pd.SparseDtype(float, 0),\n",
    "        \"num_trades\": pd.SparseDtype(int, 0),\n",
    "        \"distinct_tickers\": pd.SparseDtype(int, 0),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "features = [\"distinct_tickers\", \"notional\", \"num_trades\"]\n",
    "all_index = [\"_\".join(t) for t in product(features, [\"ss\", \"os\"], [\"bf\", \"af\"])]\n",
    "\n",
    "empty_series = pd.Series(index=all_index, dtype=\"Sparse[float]\").fillna(0)\n",
    "empty_series = pd.Series(index=all_index, dtype=\"float\").fillna(0)\n",
    "\n",
    "\n",
    "def multi_index_to_single_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.index = [\"_\".join(index_tuple) for index_tuple in df.index]\n",
    "    return df\n",
    "\n",
    "\n",
    "def groupby_index_to_series(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Hierachical groupby index with one column to flattened series. Prepending the column name to the index.\"\"\"\n",
    "    return df.stack().reorder_levels([-1, 0, 1]).pipe(multi_index_to_single_index)\n",
    "\n",
    "\n",
    "def compute_features(\n",
    "    etf_trade_time, etf_trade_direction, neigh: Optional[np.ndarray], equity_executions: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    # NOTE: filtering didn't work out great\n",
    "\n",
    "    if neigh is None:\n",
    "        # is this the right thing to do\n",
    "        return empty_series\n",
    "    elif isinstance(neigh, np.ndarray):\n",
    "        df = equity_executions.iloc[neigh].assign(\n",
    "            bf_af=lambda df: df.index < etf_trade_time,\n",
    "            ss_os=lambda df: df.direction == etf_trade_direction,\n",
    "        )\n",
    "        df[\"ss_os\"] = (\n",
    "            df[\"ss_os\"].apply(lambda x: \"ss\" if x else \"os\").astype(\"category\")\n",
    "        )\n",
    "        df[\"bf_af\"] = (\n",
    "            df[\"bf_af\"].apply(lambda x: \"bf\" if x else \"af\").astype(\"category\")\n",
    "        )\n",
    "\n",
    "        df_subset = df[[\"ticker\", \"ss_os\", \"bf_af\", \"price\", \"size\"]]\n",
    "\n",
    "        # TODO: think if there's a way to do one chained line. I'm grouping on three vs two..hmm\n",
    "\n",
    "        # notional value and num trades\n",
    "        notional_and_num_trades = (\n",
    "            df_subset.eval('notional = price * size.astype(\"int64\")')\n",
    "            .groupby([\"ss_os\", \"bf_af\"])\n",
    "            .agg(notional=(\"notional\", \"sum\"), num_trades=(\"size\", \"count\"))\n",
    "            .pipe(groupby_index_to_series)\n",
    "        )\n",
    "\n",
    "        # distinct tickers\n",
    "        distinct_tickers = (\n",
    "            df_subset.drop(columns=\"size\")\n",
    "            .groupby([\"ticker\", \"ss_os\", \"bf_af\"])\n",
    "            .count()\n",
    "            .applymap(lambda x: x > 0)\n",
    "            .groupby([\"ss_os\", \"bf_af\"])\n",
    "            .sum()\n",
    "            .rename(columns={\"price\": \"distinct_tickers\"})\n",
    "            .pipe(groupby_index_to_series)\n",
    "        )\n",
    "\n",
    "        # print(pd.concat([notional_and_num_trades, distinct_tickers]))\n",
    "        return (\n",
    "            pd.concat([notional_and_num_trades, distinct_tickers])\n",
    "            .reindex(all_index)\n",
    "            .fillna(0)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"neigh must be None or list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def append_features(etf_executions: pd.DataFrame, equity_executions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"Note that this function is not inplace.\"\n",
    "    # infer tolerances from column names\n",
    "    column_names = etf_executions.filter(regex=\"neighbors\").columns.values.tolist()\n",
    "    tolerances = [i.split(\"_\")[1] for i in column_names]\n",
    "    \n",
    "    # TODO: check if its faster to partial compute features with equity executions\n",
    "    \n",
    "    features_dfs = []\n",
    "    for tolerance in tolerances:\n",
    "        # add_neighbors(df, equity_executions, tolerance)\n",
    "        features = etf_executions.apply(\n",
    "            lambda row: compute_features(\n",
    "                row.name, row.direction, row[f\"_{tolerance}_neighbors\"], equity_executions=equity_executions,\n",
    "            ),\n",
    "            axis=1,\n",
    "            result_type=\"expand\",\n",
    "        ).add_prefix(f\"_{tolerance}_\")\n",
    "\n",
    "        features = features.astype({col: col_to_dtype(col) for col in features.columns})\n",
    "\n",
    "        features_dfs += [features]\n",
    "\n",
    "    features_df = pd.concat(features_dfs, axis=1)\n",
    "    return pd.concat([etf_executions, features_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def count_non_null(df, tolerance):\n",
    "    return df[f\"_{tolerance}_neighbors\"].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def marginalise(df: pd.DataFrame, \n",
    "                over: Literal[\"same_sign/opposite_sign\", \"before/after\"], # which feature to marginalise over \n",
    "                drop=False # drop or keep columns that were marginalised over\n",
    "                ) -> pd.DataFrame:\n",
    "    \"\"\"Marginalise over a specific feature split by summing columns together. Compute marginalised features from dataframe by summing over columns with keywords.\"\"\"\n",
    "    \n",
    "    # TODO think whether inplace is better\n",
    "    df = df.copy()\n",
    "\n",
    "    match over:\n",
    "        case \"same_sign/opposite_sign\":\n",
    "            keyword0, keyword1 = \"_ss\", \"_os\"\n",
    "        case \"before/after\":\n",
    "            keyword0, keyword1 = \"_bf\", \"_af\"\n",
    "        case _:\n",
    "            # TODO figure out how to do: f\"over must be keywords must be one of {get_args(over)}\"\n",
    "            raise ValueError(f\"invalid 'over' keyword\")\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "        if keyword0 in col_name:\n",
    "            base_col_name = col_name.replace(keyword0, \"\")\n",
    "            opposite_col_name = col_name.replace(keyword0, keyword1)\n",
    "            df[base_col_name] = df[col_name] + df[opposite_col_name]\n",
    "            \n",
    "        # TODO sanity check this\n",
    "            if drop:\n",
    "                df.drop(columns=[col_name, opposite_col_name], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "aggregate_before_after = partial(marginalise, over=\"before/after\", drop=False)\n",
    "aggregate_same_sign_opposite_sign = partial(marginalise, over=\"same_sign/opposite_sign\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def add_arb_tag(df: pd.DataFrame, tag_functions: list[str]) -> None:\n",
    "    \"\"\"Add arb tags using a list of str eval expressions.\"\"\"\n",
    "    for tag_function in tag_functions:\n",
    "        df.eval(tag_function, inplace=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# manually specify tag function strings\n",
    "# tag_functions = [\n",
    "#     \"_500ms_arb_tag = _500ms_notional_os > 0\",\n",
    "#     \"_300ms_arb_tag = _300ms_notional_os > 0\",\n",
    "# ]\n",
    "\n",
    "# # generate tag function strings\n",
    "# tag_functions = [\n",
    "#     f\"_{tolerance}_arb_tag = _{tolerance}_notional_os > 0\" for tolerance in tolerances\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def drop_features(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Drops all intermediate features, and just leaves the arbitrage tags.\n",
    "    Not the nicest way. Could do better regex.\"\"\"\n",
    "    features_and_arb_tag = set(df.filter(regex=\"^_[0-9]+ms_\").columns)\n",
    "    arb_tag = set(df.filter(regex=\"arb_tag\").columns)\n",
    "    features = features_and_arb_tag - arb_tag\n",
    "    df.drop(columns=features, inplace=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def split_isolated_non_isolated(etf_executions: pd.DataFrame, tolerance) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Returns a tuple of (isolated, non_isolated). For now, use deep copy, although this may not be great.\"\"\"\n",
    "    tolerance_str = f\"_{tolerance}_neighbors\"\n",
    "    isolated_indices = etf_executions[tolerance_str].isna()\n",
    "    return etf_executions[isolated_indices].copy(deep=True), etf_executions[~isolated_indices].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def ofi(df: pd.DataFrame, resample_freq: str = \"5T\", suffix: str | None = None) -> pd.DataFrame:\n",
    "    suffix = \"\" if suffix is None else (\"_\" + suffix)\n",
    "    \n",
    "    ofi_series = []\n",
    "    for ticker in df.ticker.unique().tolist():\n",
    "        ofi = (\n",
    "            df.query(\"ticker == @ticker\")\n",
    "            .eval('signed_size = size.astype(\"int64\") * direction')[\n",
    "                [\"size\", \"signed_size\"]\n",
    "            ]\n",
    "            .resample(resample_freq, label=\"right\")\n",
    "            .sum()\n",
    "            .eval(\"ofi = signed_size / size\")\n",
    "            .ofi.rename(f\"_{ticker}_{resample_freq}_ofi\" + suffix)\n",
    "            .fillna(0)\n",
    "        )\n",
    "        ofi_series += [ofi]\n",
    "\n",
    "    ofi_df = pd.concat(ofi_series, axis=1)\n",
    "    return ofi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def ofis(df: pd.DataFrame, # dataframe that contains only one etf (i.e has already been queried for one etf)\n",
    "        resample_freq: str = \"5T\", suffix: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"DIFF Function\"\"\"\n",
    "    suffix = \"\" if suffix is None else (\"_\" + suffix)\n",
    "    \n",
    "    ofi_series = []\n",
    "    etfs = df.ticker.unique().tolist()\n",
    "    for etf in etfs:\n",
    "        \n",
    "        ofi = (\n",
    "            df.query(\"ticker == @etf\")\n",
    "            .eval('signed_size = size.astype(\"int64\") * direction')[\n",
    "                [\"size\", \"signed_size\"]\n",
    "            ]\n",
    "            .resample(resample_freq, label=\"right\")\n",
    "            .sum()\n",
    "            .eval(\"ofi = signed_size / size\")\n",
    "            .ofi.rename(f\"_{resample_freq}_{etf}_ofi\" + suffix)\n",
    "            .fillna(0)\n",
    "        )\n",
    "        ofi_series += [ofi]\n",
    "\n",
    "    ofi_df = pd.concat(ofi_series, axis=1)\n",
    "    return ofi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def not_ofi(df: pd.DataFrame, resample_freq: str = \"5T\", suffix: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Quick modification where I don't normalize by size.\"\"\"\n",
    "    suffix = \"\" if suffix is None else (\"_\" + suffix)\n",
    "    \n",
    "    ofi_series = []\n",
    "    etfs = df.ticker.unique().tolist()\n",
    "    for etf in etfs:\n",
    "        ofi = (\n",
    "            df.query(\"ticker == @etf\")\n",
    "            .eval('signed_size = size.astype(\"int64\") * direction')[\n",
    "                [\"size\", \"signed_size\"]\n",
    "            ]\n",
    "            .resample(resample_freq, label=\"right\")\n",
    "            .sum()\n",
    "            .signed_size.rename(f\"_{resample_freq}_{etf}_aggregated_size\" + suffix)\n",
    "            .fillna(0)\n",
    "        )\n",
    "        ofi_series += [ofi]\n",
    "\n",
    "    ofi_df = pd.concat(ofi_series, axis=1)\n",
    "    return ofi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# TODO: extend function / create another functino to loop over sampling freq\n",
    "# TODO: etfs is a global variable in this function. compute it within the function or pass it as an argument\n",
    "\n",
    "\n",
    "def compute_ofi(\n",
    "    df: pd.DataFrame, resample_freq: str = \"5T\", arb_filter: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Computes OFI by removing the arbitrage flows and resampling to 5 minutes.\"\"\"\n",
    "    match arb_filter:\n",
    "        case \"only_arb\":\n",
    "            df = df.query(\"arb_tag == True\")\n",
    "            arb_filter_str = \"_only_arb\"\n",
    "        case \"no_arb\":\n",
    "            df = df.query(\"arb_tag == False\")\n",
    "            arb_filter_str = \"_no_arb\"\n",
    "        case None:\n",
    "            arb_filter_str = \"\"\n",
    "\n",
    "    ofi_series = []\n",
    "    for etf in etfs:\n",
    "        ofi = (\n",
    "            df.query(\"ticker == @etf\")\n",
    "            .eval('signed_size = size.astype(\"int64\") * direction')[\n",
    "                [\"size\", \"signed_size\"]\n",
    "            ]\n",
    "            .resample(resample_freq)\n",
    "            .sum()\n",
    "            # .apply(lambda df: df.signed_size / df.size, axis=1)\n",
    "            .eval(\"ofi = signed_size / size\")\n",
    "            .ofi.rename(f\"_{resample_freq}_{etf}_ofi\" + arb_filter_str)\n",
    "            .fillna(0)\n",
    "        )\n",
    "        ofi_series += [ofi]\n",
    "\n",
    "    ofi_df = pd.concat(ofi_series, axis=1)\n",
    "    return ofi_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/petit/Documents/data/lobster/csv\"\n",
    "tolerances = [\"500us\", \"1ms\"]\n",
    "resample_freq = \"5T\"\n",
    "equities = [\"AIG\", \"AAPL\"]\n",
    "etfs = [\"WMT\", \"GE\"]\n",
    "date_range = \"2019-01-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_lobster = partial(load_lobster, directory_path=\"/home/petit/Documents/data/lobster/csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equity_lobster = load_lobster(date_range=date_range, ticker=equities)\n",
    "# etf_executions = load_etf_executions(date_range=date_range, tickers=etfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def resample_mid(df: pd.DataFrame, resample_freq=\"5T\"):\n",
    "    return df.resample(resample_freq, label=\"right\").last().eval(\"mid = bid_price_1 + (ask_price_1 - bid_price_1) / 2\")[\"mid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def restrict_common_index(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Restrict two dataframes to their common index.\"\"\"\n",
    "    common_index = df1.index.intersection(df2.index)\n",
    "    return df1.loc[common_index], df2.loc[common_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def markout_returns(\n",
    "    df,  # dataframe to infer times to markout from\n",
    "    markouts: list[str],  # list of markouts to compute returns for\n",
    ") -> pd.DataFrame:\n",
    "    # TODO: markouts of string literals also?\n",
    "    return pd.DataFrame(index=df.index, data={f\"_{markout}\": df.index + pd.Timedelta(markout) for markout in markouts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "# def clip_for_markout(df, max_markout):\n",
    "#     end = max(df.index) - pd.Timedelta(max_markout)\n",
    "#     end = end.time()\n",
    "#     return clip_times(df, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# TODO fix this\n",
    "# def compute_returns():\n",
    "#     index = clip_for_markout(etf_executions.resample(resample_freq, label=\"right\").last(), max_markout=max_markout).index\n",
    "\n",
    "#     returns = {}\n",
    "#     for ticker in etfs:\n",
    "#         df = pd.DataFrame(index=index)\n",
    "#         for markout in [\"0S\"] + markouts:\n",
    "#             df[f\"_{markout}\"] = mids.loc[df.index + pd.Timedelta(markout), ticker].values\n",
    "\n",
    "#         for markout in markouts:\n",
    "#             df.eval(f\"return_{markout} = (_{markout} / _0S ) - 1\", inplace=True)\n",
    "\n",
    "#         df[\"return_contemp\"] = mids[ticker].resample(\"5T\").first().pct_change()\n",
    "#         df_returns = df.filter(regex=\"return\")\n",
    "#         df_returns.columns = [column.replace(\"return_\", \"\") for column in df_returns.columns]\n",
    "#         df_returns.columns = [(\"_\" + column if column[0].isdigit() else column) for column in df_returns.columns ]\n",
    "#         returns[ticker] = df_returns\n",
    "#     return returns\n",
    "\n",
    "# returns = compute_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ofi_all = ofi(etf_executions, resample_freq=\"5T\", suffix=\"all\")\n",
    "# ofi_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# TODO: TEMP FIX. PUSH TO PYPI AND REINSTALL\n",
    "def clip_df_times(df: pd.DataFrame, start: datetime.time | None = None, end: datetime.time | None = None) -> pd.DataFrame:\n",
    "        \"\"\"Clip a dataframe or lobster object to a time range.\"\"\"\n",
    "        # TODO: improve this function? with the 4 if statements lol, i guess i could clip the index twice and have two if statements\n",
    "        if start and end:\n",
    "            return df.iloc[(df.index.time >= start) & (df.index.time < end)]\n",
    "        elif start:\n",
    "            return df.iloc[df.index.time >= start]\n",
    "        elif end:\n",
    "            return df.iloc[df.index.time < end]\n",
    "        else:\n",
    "            raise ValueError(\"start and end cannot both be None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def clip_for_markout(df, max_markout):\n",
    "    end = (max(df.index) - pd.Timedelta(max_markout)).time()\n",
    "    # end = end.time()\n",
    "    return clip_df_times(df, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
