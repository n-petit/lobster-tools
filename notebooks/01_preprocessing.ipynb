{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> Preprocessing of csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import datetime\n",
    "import enum\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Literal, Optional, Self, Protocol, runtime_checkable, get_args\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "@enum.unique\n",
    "class Event(enum.Enum):\n",
    "    \"A class to represent the event type of a LOBSTER message.\"\n",
    "    UNKNOWN = 0\n",
    "    SUBMISSION = 1\n",
    "    CANCELLATION = 2\n",
    "    DELETION = 3\n",
    "    EXECUTION = 4\n",
    "    HIDDEN_EXECUTION = 5\n",
    "    CROSS_TRADE = 6\n",
    "    ORIGINAL_TRADING_HALT = 7\n",
    "    OTHER = 8\n",
    "    TRADING_HALT = 9\n",
    "    RESUME_QUOTE = 10\n",
    "    TRADING_RESUME = 11\n",
    "\n",
    "\n",
    "@enum.unique\n",
    "class EventGroup(enum.Enum):\n",
    "    EXECUTIONS = [\n",
    "        Event.EXECUTION.value,\n",
    "        Event.HIDDEN_EXECUTION.value,\n",
    "        Event.CROSS_TRADE.value,\n",
    "    ]\n",
    "    HALTS = [\n",
    "        Event.TRADING_HALT.value,\n",
    "        Event.RESUME_QUOTE.value,\n",
    "        Event.TRADING_RESUME.value,\n",
    "    ]\n",
    "    CANCELLATIONS = [Event.CANCELLATION.value, Event.DELETION.value]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event column in the lobster data encodes the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNKNOWN': 0,\n",
       " 'SUBMISSION': 1,\n",
       " 'CANCELLATION': 2,\n",
       " 'DELETION': 3,\n",
       " 'EXECUTION': 4,\n",
       " 'HIDDEN_EXECUTION': 5,\n",
       " 'CROSS_TRADE': 6,\n",
       " 'OTHER': 8,\n",
       " 'TRADING_HALT': 7,\n",
       " 'RESUME_QUOTE': 10,\n",
       " 'TRADING_RESUME': 11}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "{e.name: e.value for e in Event}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "Here, trading halts are split into three categories: trading halt, resume quote and trading resume. To access all three types of trading halt events, use `EventGroup.TRADING_HALTS`.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access common groups of events, an `EventGroup` is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CANCELLATIONS': [2, 3],\n",
      " 'EXECUTIONS': [4, 5, 6],\n",
      " 'HALTS': [7, 10, 11]}\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "pprint({e.name: e.value for e in EventGroup}, width=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "@enum.unique\n",
    "class Direction(enum.Enum):\n",
    "    BUY = 1\n",
    "    SELL = -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv data is assumed to be stored as follows. There should be one folder per ticker, with underscores separating the ticker name, start date, end date and number of levels. Each individual csv filename should also end with the number of levels. All dates should be listed as %YYYY-%MM-%DD convention. It is still possible to work with data that is not stored in this way, but the `Data` class must have the `date_range` and `levels` arguments provided."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "csv_lobster_data\n",
    "├── AAPL_2016-06-21_2016-06-21_10\n",
    "│   ├── AAPL_2012-06-21_34200000_57600000_message_10.csv\n",
    "│   ├── AAPL_2012-06-21_34200000_57600000_orderbook_10.csv\n",
    "│   ├── AAPL_2012-06-22_34200000_57600000_message_10.csv\n",
    "│   └── AAPL_2012-06-22_34200000_57600000_orderbook_10.csv\n",
    "├── GOOG_2016-06-21_2016-06-22_10\n",
    "│   ├── GOOG_2012-06-21_34200000_57600000_message_10.csv\n",
    "│   ├── GOOG_2012-06-21_34200000_57600000_orderbook_10.csv\n",
    "│   ├── GOOG_2012-06-22_34200000_57600000_message_10.csv\n",
    "│   └── GOOG_2012-06-22_34200000_57600000_orderbook_10.csv\n",
    "└── LOBSTER_SampleFiles_ReadMe.txt\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Data` object stores information about the data to be loaded, as well as specifying which preprocessing options are to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "# | exports\n",
    "@dataclass\n",
    "class Data:\n",
    "    directory_path: str | None = None  # path to data\n",
    "    ticker: str | None = None  # ticker name\n",
    "    date_range: Optional[str | tuple[str, str]] = None\n",
    "    levels: Optional[int] = None\n",
    "    nrows: Optional[int] = None\n",
    "    load: Literal[\"both\", \"messages\", \"book\"] = \"both\"\n",
    "    add_ticker_column: bool = False\n",
    "    ticker_type: Literal[None, \"equity\", \"etf\"] = None\n",
    "    clip_trading_hours: bool = True\n",
    "    aggregate_duplicates: bool = True\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.directory_path is None:\n",
    "            self.directory_path = os.getenv(\"LOBSTER_DATA_PATH\", \"../data\")\n",
    "        if self.ticker is None:\n",
    "            self.ticker = os.getenv(\"DEFAULT_TICKER\", \"AMZN\")\n",
    "\n",
    "        # TODO: do this better, maybe pydantic, maybe a decorator, or maybe with\n",
    "        # LoadType = Literal[\"both\", \"messages\", \"book\"]\n",
    "        # TickerTypes = Literal[None, \"equity\", \"etf\"]\n",
    "        # assert self.ticker_type in get_args(TickerTypes)\n",
    "        if self.load not in (\"both\", \"messages\", \"book\"):\n",
    "            raise ValueError(f\"Invalid load type: {self.load}\")\n",
    "        if self.ticker_type not in (None, \"equity\", \"etf\"):\n",
    "            raise ValueError(f\"Invalid ticker type: {self.ticker_type}\")\n",
    "\n",
    "        # ticker path\n",
    "        tickers = glob.glob(f\"{self.directory_path}/*\")\n",
    "        # print(self.ticker)\n",
    "        ticker_path = [t for t in tickers if self.ticker in t]\n",
    "        if len(ticker_path) != 1:\n",
    "            raise ValueError(f\"Expected exactly 1 directory with name {self.ticker}\")\n",
    "        # assert len(ticker_path) == 1\n",
    "        self.ticker_path = ticker_path[0]\n",
    "\n",
    "        # levels\n",
    "        if not self.levels:\n",
    "            self.levels = int(self.ticker_path.split(\"_\")[-1])\n",
    "\n",
    "            if self.levels < 1:\n",
    "                raise ValueError(f\"Invalid number of levels: {self.levels}\")\n",
    "            # assert self.levels >= 1\n",
    "\n",
    "        # infer date range from ticker folder name\n",
    "        if not self.date_range:\n",
    "            self.date_range = tuple(re.findall(pattern=r\"\\d\\d\\d\\d-\\d\\d-\\d\\d\", string=self.ticker_path))\n",
    "            if len(self.date_range) != 2:\n",
    "                raise ValueError(f\"Expected exactly 2 dates in regex match of in {self.ticker_path}\")\n",
    "            # assert len(self.date_range) == 2\n",
    "\n",
    "        # book and message paths\n",
    "        tickers = glob.glob(f\"{self.ticker_path}/*\")\n",
    "        tickers_end = list(map(os.path.basename, tickers))\n",
    "\n",
    "        if isinstance(self.date_range, tuple):\n",
    "            # get all dates in folder\n",
    "            dates = {re.findall(pattern=r\"\\d\\d\\d\\d-\\d\\d-\\d\\d\", string=file)[0] for file in tickers_end}\n",
    "            # dates = set([re.findall(pattern=r\"\\d\\d\\d\\d-\\d\\d-\\d\\d\", string=file)[0] for file in tickers_end])\n",
    "            # filter for dates within specified range\n",
    "            dates = sorted(\n",
    "                list(\n",
    "                    filter(\n",
    "                        lambda date: self.date_range[0] <= date <= self.date_range[1],\n",
    "                        dates,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.dates = dates\n",
    "            self.date_range = (min(self.dates), max(self.dates))\n",
    "\n",
    "        elif isinstance(self.date_range, str):\n",
    "            self.dates, self.date_range = [self.date_range], (\n",
    "                self.date_range,\n",
    "                self.date_range,\n",
    "            )\n",
    "\n",
    "        # messages and book filepath dictionaries\n",
    "        def _create_date_to_path_dict(keyword: str) -> dict:\n",
    "            filter_keyword_tickers = list(filter(lambda x: keyword in x, tickers_end))\n",
    "            date_path_dict = {}\n",
    "            for date in self.dates:\n",
    "                filter_date_tickers = list(filter(lambda x: date in x, filter_keyword_tickers))\n",
    "                if len(filter_date_tickers) != 1:\n",
    "                    raise ValueError(f\"Expected exactly 1 match for {date}\")\n",
    "                # assert len(filter_date_tickers) == 1\n",
    "                date_path_dict[date] = os.path.join(self.ticker_path, filter_date_tickers[0])\n",
    "            return date_path_dict\n",
    "\n",
    "        self.book_paths = _create_date_to_path_dict(\"book\")\n",
    "        self.messages_paths = _create_date_to_path_dict(\"message\")\n",
    "\n",
    "        if self.load in (\"book\", \"both\"):\n",
    "            self.load_book = True\n",
    "        if self.load in (\"messages\", \"both\"):\n",
    "            self.load_messages = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "def _aggregate_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.reset_index(inplace=True)\n",
    "    duplicates = df.duplicated(subset=df.columns.difference([\"size\"]), keep=False)\n",
    "    df.loc[duplicates, \"size\"] = df.loc[duplicates, \"size\"].groupby(df.loc[duplicates].datetime).transform(\"sum\")\n",
    "    df.drop_duplicates(subset=df.columns.difference([\"size\"]), inplace=True)\n",
    "    df.set_index(\"datetime\", inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Lobster` loads the csv data into its `messages` and `book` attributes. The data to be loaded is passed in as a `Data` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def clip_df_times(df: pd.DataFrame, start: datetime.time | None = None, end: datetime.time | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Clip a dataframe or lobster object to a time range.\"\"\"\n",
    "    # TODO: improve this function? with the 4 if statements lol, i guess i could clip the index twice and have two if statements\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"Expected a dataframe with a datetime index\")\n",
    "\n",
    "    if start and end:\n",
    "        return df.iloc[(df.index.time >= start) & (df.index.time < end)]\n",
    "    elif start:\n",
    "        return df.iloc[df.index.time >= start]\n",
    "    elif end:\n",
    "        return df.iloc[df.index.time < end]\n",
    "    else:\n",
    "        raise ValueError(\"start and end cannot both be None\")\n",
    "\n",
    "\n",
    "_clip_df_trading_hours = partial(clip_df_times, start=datetime.time(9, 30), end=datetime.time(16, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@dataclass\n",
    "class Lobster:\n",
    "    \"Lobster data class for a single symbol of Lobster data.\"\n",
    "    data: Data | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.data is None:\n",
    "            self.data = Data()\n",
    "\n",
    "        if self.data.load_messages:\n",
    "            dfs = []\n",
    "            for date, filepath in self.data.messages_paths.items():\n",
    "                # load messages\n",
    "                df = pd.read_csv(\n",
    "                    filepath,\n",
    "                    header=None,\n",
    "                    nrows=self.data.nrows,\n",
    "                    usecols=list(range(6)),\n",
    "                    names=[\"time\", \"event\", \"order_id\", \"size\", \"price\", \"direction\"],\n",
    "                    index_col=False,\n",
    "                    dtype={\n",
    "                        \"time\": \"float64\",\n",
    "                        \"event\": \"uint8\",\n",
    "                        \"price\": \"int64\",\n",
    "                        \"direction\": \"int8\",\n",
    "                        \"order_id\": \"int32\",\n",
    "                        \"size\": \"uint64\",\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # set index as datetime\n",
    "                df[\"datetime\"] = pd.to_datetime(date, format=\"%Y-%m-%d\") + df.time.apply(lambda x: pd.to_timedelta(x, unit=\"s\"))\n",
    "                df.set_index(\"datetime\", drop=True, inplace=True)\n",
    "                df.rename(columns={'time':'seconds_since_midnight'})\n",
    "                # df.drop(columns=\"time\", inplace=True)\n",
    "                dfs.append(df)\n",
    "            df = pd.concat(dfs)\n",
    "\n",
    "            # direction for cross trades is set to zero, and order_id is left unchanged\n",
    "            if not df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"].eq(-1).all():\n",
    "                raise ValueError(\"All cross trades must have direction -1\")\n",
    "            df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"] = 0\n",
    "\n",
    "            if not df.loc[df.event.eq(Event.CROSS_TRADE.value), \"order_id\"].eq(-1).all():\n",
    "                raise ValueError(\"All cross trades must have order_id -1\")\n",
    "\n",
    "            # assert df.loc[df.event.eq(Event.TRADING_HALT.value), \"direction\"].eq(-1).all()\n",
    "            df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"] = 0\n",
    "            if not df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"].eq(-1).all():\n",
    "                raise ValueError(\"All trading halts must have direction -1\")\n",
    "            # assert df.loc[df.event.eq(Event.TRADING_HALT.value), \"direction\"].eq(-1).all()\n",
    "            df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"] = 0\n",
    "\n",
    "            # # process trading halts and map to new trading halts\n",
    "            def _trading_halt_type(price):\n",
    "                return {\n",
    "                    -1: Event.TRADING_HALT.value,\n",
    "                    0: Event.RESUME_QUOTE.value,\n",
    "                    1: Event.TRADING_RESUME.value,\n",
    "                }[price]\n",
    "\n",
    "            # doesn't make much difference\n",
    "            df.loc[df.event.eq(Event.TRADING_HALT.value), \"event\"] = df.loc[df.event.eq(Event.TRADING_HALT.value), \"price\"].apply(_trading_halt_type)\n",
    "\n",
    "            # implentation of above without apply\n",
    "            # trading_halt_mask = (df.event == Event.TRADING_HALT.value)\n",
    "            # halt_type_mapping = {\n",
    "            #     -1: Event.TRADING_HALT.value,\n",
    "            #     0: Event.RESUME_QUOTE.value,\n",
    "            #     1: Event.TRADING_RESUME.value,\n",
    "            # }\n",
    "            # df.loc[trading_halt_mask, \"event\"] = df.loc[trading_halt_mask, \"price\"].map(halt_type_mapping)\n",
    "\n",
    "            # use 0 as NaN for size and direction\n",
    "            df.loc[\n",
    "                df.event.isin(EventGroup.HALTS.value),\n",
    "                [\"order_id\", \"size\", \"price\"],\n",
    "            ] = [0, 0, np.nan]\n",
    "\n",
    "            # set price in dollars\n",
    "            df.price = df.price.apply(lambda x: x / 10_000).astype(\"float64\")\n",
    "\n",
    "            if self.data.ticker_type:\n",
    "                # TODO change to get Literal Values from args?\n",
    "\n",
    "                if self.data.ticker_type not in [\n",
    "                    \"equity\",\n",
    "                    \"etf\",\n",
    "                ]:\n",
    "                    raise ValueError(\"ticker_type must be either `equity` or `etf`\")\n",
    "                # assert self.data.ticker_type in [\n",
    "                #     \"equity\",\n",
    "                #     \"etf\",\n",
    "                # ], \"ticker_type must be either `equity` or `etf`\"\n",
    "                df = df.assign(ticker_type=self.data.ticker_type).astype(\n",
    "                    dtype={\"ticker_type\": pd.CategoricalDtype(categories=[\"equity\", \"etf\"])}\n",
    "                )\n",
    "\n",
    "            self.messages = df\n",
    "\n",
    "        if self.data.load_book:\n",
    "            col_names = []\n",
    "            for level in range(1, self.data.levels + 1):\n",
    "                for col_type in [\"ask_price\", \"ask_size\", \"bid_price\", \"bid_size\"]:\n",
    "                    col_name = f\"{col_type}_{level}\"\n",
    "                    col_names.append(col_name)\n",
    "\n",
    "            # for now just use float64\n",
    "            # col_dtypes = {\n",
    "            #     col_name: pd.Int64Dtype() if (\"size\" in col_name) else \"float\"\n",
    "            #     for col_name in col_names\n",
    "            # }\n",
    "\n",
    "            dfs = []\n",
    "            for filename in self.data.book_paths.values():\n",
    "                df = pd.read_csv(\n",
    "                    filename,\n",
    "                    header=None,\n",
    "                    nrows=self.data.nrows,\n",
    "                    usecols=list(range(4 * self.data.levels)),\n",
    "                    names=col_names,\n",
    "                    dtype=\"float64\",\n",
    "                    na_values=[-9999999999, 9999999999, 0],\n",
    "                )\n",
    "\n",
    "                dfs.append(df)\n",
    "            df = pd.concat(dfs)\n",
    "\n",
    "            df.set_index(self.messages.index, inplace=True, drop=True)\n",
    "\n",
    "            price_cols = df.columns.str.contains(\"price\")\n",
    "            df.loc[:, price_cols] = df.loc[:, price_cols].apply(lambda x: x / 10_000)\n",
    "\n",
    "            self.book = df\n",
    "\n",
    "        # data cleaning on messages done only now, as book infers times from messages file\n",
    "        # TODO: think if leaving bool flags good idea\n",
    "        if self.data.aggregate_duplicates:\n",
    "            self.aggregate_duplicates()\n",
    "        if self.data.clip_trading_hours:\n",
    "            self.clip_trading_hours()\n",
    "        if self.data.add_ticker_column:\n",
    "            self.add_ticker_column()\n",
    "\n",
    "    def clip_trading_hours(self) -> Self:\n",
    "        if hasattr(self, \"book\"):\n",
    "            self.book = _clip_df_trading_hours(self.book)\n",
    "        if hasattr(self, \"messages\"):\n",
    "            self.messages = _clip_df_trading_hours(self.messages)\n",
    "        return self\n",
    "\n",
    "    def aggregate_duplicates(self) -> Self:\n",
    "        self.messages = _aggregate_duplicates(self.messages)\n",
    "        return self\n",
    "\n",
    "    # TODO: write decorator to simplify the \"both\", \"messages\", \"book\" logic that is common to a few methods\n",
    "    def add_ticker_column(self, to: Literal[\"both\", \"messages\", \"book\"] = \"messages\") -> Self:\n",
    "        if to in (\"both\", \"messages\"):\n",
    "            self.messages = self.messages.assign(ticker=self.data.ticker).astype({\"ticker\": \"category\"})\n",
    "        if to in (\"both\", \"book\"):\n",
    "            self.book = self.book.assign(ticker=self.data.ticker).astype({\"ticker\": \"category\"})\n",
    "        return self\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Lobster data for ticker: {self.data.ticker} for date range: {self.data.date_range[0]} to {self.data.date_range[1]}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing the `Data` class into another python notebook or script, it may be convenient to overwrite defaults arguments such as `directory_path`. This way these arguments need only be specified once. This can be achieved by using  `functools.partial` or by using inheritance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(functools.partial(<class '__main__.Data'>, ticker='AIG', date_range=('2019-01-02', '2019-01-02'), directory_path='/home/petit/Documents/data/lobster/csv', load='both'), ticker='GEd', date_range=('2019-01-02', '2019-01-02'), directory_path='/home/petit/Documents/data/lobster/csv', load='both')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | eval: false\n",
    "MyData = partial(Data, \n",
    "                    directory_path=\"/my/local/data/path\",\n",
    "                    ticker=\"AIG\",\n",
    "                    clip_trading_hours=True,\n",
    "                    load=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "class MyData(Data):\n",
    "    def __init__(   self, \n",
    "                    directory_path=\"/my/local/data/path\",\n",
    "                    ticker=\"AIG\", \n",
    "                    clip_trading_hours=True,\n",
    "                    *args, \n",
    "                    **kwargs):\n",
    "        super().__init__(\n",
    "                    directory_path=directory_path,\n",
    "                    ticker=ticker, \n",
    "                    clip_trading_hours=clip_trading_hours,\n",
    "                    *args, \n",
    "                    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make sth like this work / think further on it\n",
    "# # | exports\n",
    "# # | code-fold: true\n",
    "# @dataclass\n",
    "# class Lobsters:\n",
    "#     \"Lobsters data class for multiple tickers of Lobster data.\"\n",
    "#     data: Data | None = None\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         if self.data is None:\n",
    "#             self.data = Data(ticker=[\"AMZN\", \"AAPL\"])\n",
    "        \n",
    "#         tickers = self.data.ticker\n",
    "#         # data_dict = asdict(self.data).copy()\n",
    "#         lobsters_list = []\n",
    "#         data_without_ticker = self.data.__dict__.pop(\"ticker\")\n",
    "#         print(data_without_ticker)\n",
    "#         while tickers:\n",
    "#             ticker = tickers.pop(0)\n",
    "#             print(ticker)\n",
    "#             lobsters_list += [Lobster(Data(**data_without_ticker, ticker=ticker))]\n",
    "\n",
    "\n",
    "#         # for ticker in self.data.ticker:\n",
    "#         #     # self.data.ticker.pop(0)\n",
    "#         #     # lobsters += [Lobster(Data(**self.data.__dict__, ticker=ticker))]\n",
    "#         #     lobsters += [Lobster(Data(**self.data.__dict__, ticker=ticker))]\n",
    "#         # self.lobsters_list = lobsters_list\n",
    "#         #     # setattr(self, ticker, Lobster(Data(**self.data.__dict__, ticker=ticker)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_lobster` is a simple function which returns a `Lobster` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def load_lobster(**kwargs):\n",
    "    \"\"\"Load `Lobster` object from csv data.\"\"\"\n",
    "    # TODO remove this function and turn Lobster into callable class\n",
    "    data = Data(**kwargs)\n",
    "    lobster = Lobster(data)\n",
    "\n",
    "    return lobster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def load_lobsters(**kwargs):\n",
    "    \"\"\"Load multiple `Lobster` objects into list.\"\"\"\n",
    "    if not isinstance(kwargs[\"ticker\"], list):\n",
    "        raise ValueError(\"load lobsters is used for loading multiple tickers\")\n",
    "    # assert isinstance(kwargs[\"ticker\"], list), \"load lobsters is used for loading multiple tickers\"\n",
    "    tickers = kwargs.pop(\"ticker\")\n",
    "\n",
    "    lobsters = []\n",
    "    for ticker in tickers:\n",
    "        data = Data(ticker=ticker, **kwargs)\n",
    "        lobsters += [Lobster(data)]\n",
    "\n",
    "    return lobsters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overwrite defaults `functools.partial` can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "my_load_lobster = partial(load_lobster, directory_path=\"/my/path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "load_lobster = partial(load_lobster, directory_path=\"/home/petit/Documents/data/csv_lobster_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snipet infers the `date_range` and ticker names from the folder names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     ticker_date_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mmatch\u001b[39;00m\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m): (match\u001b[39m.\u001b[39mgroup(\u001b[39m2\u001b[39m), match\u001b[39m.\u001b[39mgroup(\u001b[39m3\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mfor\u001b[39;00m folder \u001b[39min\u001b[39;00m all_folders\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mif\u001b[39;00m (match \u001b[39m:=\u001b[39m ticker_pattern\u001b[39m.\u001b[39msearch(folder))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ticker_date_dict\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m infer_ticker_to_date_range()\n",
      "\u001b[1;32m/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb Cell 33\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer_ticker_to_date_range\u001b[39m(csv_files_path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/nfs/home/nicolasp/home/data/tmp\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Infer from folder structure the ticker to date_range mapping.\"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     csv_files_path \u001b[39m=\u001b[39m Path(csv_files_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     all_folders \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(path) \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m csv_files_path\u001b[39m.\u001b[39mglob(\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/01_preprocessing.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     ticker_pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_([A-Z.]\u001b[39m\u001b[39m{\u001b[39m\u001b[39m2,6})_(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{4}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m)_(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{4}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "def infer_ticker_to_date_range(\n",
    "    files_path: str = \"/nfs/home/nicolasp/home/data/tmp\", /\n",
    ") -> dict:\n",
    "    \"\"\"Infer from folder structure the ticker to date_range mapping.\"\"\"\n",
    "    files_path_ = Path(files_path)\n",
    "    all_folders = [str(path) for path in files_path_.glob(\"*\")]\n",
    "\n",
    "    ticker_pattern = re.compile(\n",
    "        r\"_([A-Z.]{2,6})_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})\"\n",
    "    )\n",
    "    ticker_date_dict = {\n",
    "        match.group(1): (match.group(2), match.group(3))\n",
    "        for folder in all_folders\n",
    "        if (match := ticker_pattern.search(folder))\n",
    "    }\n",
    "    return ticker_date_dict\n",
    "\n",
    "\n",
    "infer_ticker_to_date_range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class FolderInfo:\n",
    "    full: str\n",
    "    ticker: str\n",
    "    ticker_till_end: str\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "\n",
    "\n",
    "def infer_ticker_dict(\n",
    "    files_path: str = \"/nfs/home/nicolasp/home/data/tmp\", /\n",
    ") -> list[FolderInfo]:\n",
    "    \"\"\"Infer from folder structure the ticker to date_range mapping.\"\"\"\n",
    "    files_path_ = Path(files_path)\n",
    "    all_folders = [str(path) for path in files_path_.glob(\"*\")]\n",
    "\n",
    "    ticker_pattern = re.compile(\n",
    "        r\"(.*?)(([A-Z.]{1,7})_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})(.*))\"\n",
    "    )\n",
    "\n",
    "    folder_info = [\n",
    "        FolderInfo(\n",
    "            full=match.group(0),\n",
    "            ticker=match.group(3),\n",
    "            ticker_till_end=match.group(2).rstrip('.7z'),\n",
    "            start_date=match.group(4),\n",
    "            end_date=match.group(5),\n",
    "        )\n",
    "        for folder in all_folders\n",
    "        if (match := ticker_pattern.search(folder))\n",
    "    ]\n",
    "    return folder_info\n",
    "\n",
    "    # ticker_date_dict = {\n",
    "    #     match.group(3): {\n",
    "    #         \"full\": match.group(0),\n",
    "    #         \"ticker_till_end\": match.group(2),\n",
    "    #         \"start_date\": match.group(4),\n",
    "    #         \"end_date\": match.group(5),\n",
    "    #     }\n",
    "    #     for folder in all_folders\n",
    "    #     if (match := ticker_pattern.search(folder))\n",
    "    # }\n",
    "    # return ticker_date_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/PSX_2016-01-01_2016-12-31_10', ticker='PSX', ticker_till_end='PSX_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/SLB_2016-01-01_2016-12-31_10', ticker='SLB', ticker_till_end='SLB_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/HAL_2016-01-01_2016-12-31_10', ticker='HAL', ticker_till_end='HAL_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/COP_2016-01-01_2016-12-31_10', ticker='COP', ticker_till_end='COP_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/OKE_2016-01-01_2016-12-31_10', ticker='OKE', ticker_till_end='OKE_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/DVN_2016-01-01_2016-12-31_10', ticker='DVN', ticker_till_end='DVN_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/MRO_2016-01-01_2016-12-31_10', ticker='MRO', ticker_till_end='MRO_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/FANG_2016-01-01_2016-12-31_10', ticker='FANG', ticker_till_end='FANG_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/CVX_2016-01-01_2016-12-31_10', ticker='CVX', ticker_till_end='CVX_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/XOM_2016-01-01_2016-12-31_10', ticker='XOM', ticker_till_end='XOM_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/KMI_2016-01-01_2016-12-31_10', ticker='KMI', ticker_till_end='KMI_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/PXD_2016-01-01_2016-12-31_10', ticker='PXD', ticker_till_end='PXD_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31'),\n",
       " FolderInfo(full='/nfs/home/nicolasp/home/data/tmp/WMB_2016-01-01_2016-12-31_10', ticker='WMB', ticker_till_end='WMB_2016-01-01_2016-12-31_10', start_date='2016-01-01', end_date='2016-12-31')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = infer_ticker_dict(\"/nfs/lobster_data/lobster_raw/2016\")\n",
    "z = infer_ticker_dict()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PSX': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_PSX_2020-01-01_2021-04-27_10',\n",
       " 'KMI': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_KMI_2020-01-01_2021-04-27_10',\n",
       " 'SLB': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_SLB_2020-01-01_2021-04-27_10',\n",
       " 'FANG': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_FANG_2020-01-01_2021-04-27_10',\n",
       " 'WMB': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_WMB_2020-01-01_2021-04-27_10',\n",
       " 'PXD': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_PXD_2020-01-01_2021-04-27_10',\n",
       " 'XOM': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_XOM_2020-01-01_2021-04-27_10',\n",
       " 'CVX': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_CVX_2020-01-01_2021-04-27_10',\n",
       " 'EOG': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_EOG_2020-01-01_2021-04-27_10',\n",
       " 'VLO': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_VLO_2020-01-01_2021-04-27_10',\n",
       " 'HES': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_HES_2020-01-01_2021-04-27_10',\n",
       " 'DVN': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_DVN_2020-01-01_2021-04-27_10',\n",
       " 'APA': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_APA_2020-01-01_2021-04-27_10',\n",
       " 'COP': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_COP_2020-01-01_2021-04-27_10',\n",
       " 'HAL': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_HAL_2020-01-01_2021-04-27_10',\n",
       " 'MPC': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_MPC_2020-01-01_2021-04-27_10',\n",
       " 'BKR': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_BKR_2020-01-01_2021-04-27_10',\n",
       " 'XLE': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_XLE_2020-01-01_2021-04-27_10',\n",
       " 'OXY': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_OXY_2020-01-01_2021-04-27_10',\n",
       " 'OKE': '/nfs/home/nicolasp/home/data/tmp/_data_dwn_32_302_lob2020_OKE_2020-01-01_2021-04-27_10'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | export\n",
    "def infer_ticker_to_ticker_path(\n",
    "    files_path: str = \"/nfs/home/nicolasp/home/data/tmp\", /\n",
    ") -> dict:\n",
    "    \"\"\"Same as above but just return file path.\"\"\"\n",
    "    files_path_ = Path(files_path)\n",
    "    all_folders = [str(path) for path in files_path_.glob(\"*\")]\n",
    "\n",
    "    ticker_pattern = re.compile(\n",
    "        r\".*_([A-Z.]{2,6})_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2}).*\"\n",
    "    )\n",
    "    ticker_ticker_path_dict = {\n",
    "        match.group(1): match.group(0)\n",
    "        for folder in all_folders\n",
    "        if (match := ticker_pattern.search(folder))\n",
    "    }\n",
    "    return ticker_ticker_path_dict\n",
    "\n",
    "\n",
    "infer_ticker_to_ticker_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
