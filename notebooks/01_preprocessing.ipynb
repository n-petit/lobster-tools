{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> Preprocessing of csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import datetime\n",
    "import enum\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import re\n",
    "import typing as t\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "@enum.unique\n",
    "class Event(enum.Enum):\n",
    "    \"A class to represent the event type of a LOBSTER message.\"\n",
    "    UNKNOWN = 0\n",
    "    SUBMISSION = 1\n",
    "    CANCELLATION = 2\n",
    "    DELETION = 3\n",
    "    EXECUTION = 4\n",
    "    HIDDEN_EXECUTION = 5\n",
    "    CROSS_TRADE = 6\n",
    "    ORIGINAL_TRADING_HALT = 7\n",
    "    OTHER = 8\n",
    "    TRADING_HALT = 9\n",
    "    RESUME_QUOTE = 10\n",
    "    TRADING_RESUME = 11\n",
    "\n",
    "\n",
    "@enum.unique\n",
    "class EventGroup(enum.Enum):\n",
    "    \"\"\"Note that the `EXECUTIONS` group does not contain `Event.CROSS_TRADE`.\"\"\"\n",
    "\n",
    "    EXECUTIONS = [\n",
    "        Event.EXECUTION.value,\n",
    "        Event.HIDDEN_EXECUTION.value,\n",
    "        # Event.CROSS_TRADE.value,\n",
    "    ]\n",
    "    HALTS = [\n",
    "        Event.TRADING_HALT.value,\n",
    "        Event.RESUME_QUOTE.value,\n",
    "        Event.TRADING_RESUME.value,\n",
    "    ]\n",
    "    CANCELLATIONS = [Event.CANCELLATION.value, Event.DELETION.value]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event column in the LOBSTER data encodes the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNKNOWN': 0,\n",
       " 'SUBMISSION': 1,\n",
       " 'CANCELLATION': 2,\n",
       " 'DELETION': 3,\n",
       " 'EXECUTION': 4,\n",
       " 'HIDDEN_EXECUTION': 5,\n",
       " 'CROSS_TRADE': 6,\n",
       " 'ORIGINAL_TRADING_HALT': 7,\n",
       " 'OTHER': 8,\n",
       " 'TRADING_HALT': 9,\n",
       " 'RESUME_QUOTE': 10,\n",
       " 'TRADING_RESUME': 11}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "{e.name: e.value for e in Event}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "Here, trading halts are split into three categories: trading halt, resume quote and trading resume. To access all three types of trading halt events, use `EventGroup.TRADING_HALTS`.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `EventGroup` is used to access the following common groups of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CANCELLATIONS': [2, 3],\n",
      " 'EXECUTIONS': [4, 5],\n",
      " 'HALTS': [9, 10, 11]}\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "pprint({e.name: e.value for e in EventGroup}, width=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "@enum.unique\n",
    "class Direction(enum.Enum):\n",
    "    BUY = 1\n",
    "    SELL = -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv data is assumed to be stored as follows. There should be one folder per ticker, with underscores separating the ticker name, start date, end date and number of levels. Each individual csv filename should also end with the number of levels. All dates should be listed as %YYYY-%MM-%DD convention. It is still possible to work with data that is not stored in this way, but the `Data` class must have the `date_range` and `levels` arguments provided."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "csv_lobster_data\n",
    "├── AAPL_2016-06-21_2016-06-21_10\n",
    "│   ├── AAPL_2012-06-21_34200000_57600000_message_10.csv\n",
    "│   ├── AAPL_2012-06-21_34200000_57600000_orderbook_10.csv\n",
    "│   ├── AAPL_2012-06-22_34200000_57600000_message_10.csv\n",
    "│   └── AAPL_2012-06-22_34200000_57600000_orderbook_10.csv\n",
    "├── GOOG_2016-06-21_2016-06-22_10\n",
    "│   ├── GOOG_2012-06-21_34200000_57600000_message_10.csv\n",
    "│   ├── GOOG_2012-06-21_34200000_57600000_orderbook_10.csv\n",
    "│   ├── GOOG_2012-06-22_34200000_57600000_message_10.csv\n",
    "│   └── GOOG_2012-06-22_34200000_57600000_orderbook_10.csv\n",
    "└── LOBSTER_SampleFiles_ReadMe.txt\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Data` object stores information about the data to be loaded, as well as specifying which preprocessing options are to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "# | exports\n",
    "@dataclass\n",
    "class Data:\n",
    "    directory_path: str | None = None  # path to data\n",
    "    ticker: str | None = None  # ticker name\n",
    "    date_range: t.Optional[str | tuple[str, str]] = None\n",
    "    levels: t.Optional[int] = None\n",
    "    nrows: t.Optional[int] = None\n",
    "    load: t.Literal[\"both\", \"messages\", \"book\"] = \"both\"\n",
    "    add_ticker_column: bool = False\n",
    "    ticker_type: t.Literal[None, \"equity\", \"etf\"] = None\n",
    "    clip_trading_hours: bool = True\n",
    "    aggregate_duplicates: bool = True\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.directory_path is None:\n",
    "            self.directory_path = os.getenv(\"LOBSTER_DATA_PATH\", \"../data\")\n",
    "        if self.ticker is None:\n",
    "            self.ticker = os.getenv(\"DEFAULT_TICKER\", \"AMZN\")\n",
    "\n",
    "        # TODO: do this better, maybe pydantic, maybe a decorator, or maybe with\n",
    "        # LoadType = t.Literal[\"both\", \"messages\", \"book\"]\n",
    "        # TickerTypes = t.Literal[None, \"equity\", \"etf\"]\n",
    "        # assert self.ticker_type in get_args(TickerTypes)\n",
    "        if self.load not in (\"both\", \"messages\", \"book\"):\n",
    "            raise ValueError(f\"Invalid load type: {self.load}\")\n",
    "        if self.ticker_type not in (None, \"equity\", \"etf\"):\n",
    "            raise ValueError(f\"Invalid ticker type: {self.ticker_type}\")\n",
    "\n",
    "        # ticker path\n",
    "        tickers = glob.glob(f\"{self.directory_path}/*\")\n",
    "        # ticker_path = [t for t in tickers if self.ticker in t]\n",
    "        ticker_path = [\n",
    "            t for t in tickers if os.path.basename(t).startswith(f\"{self.ticker}_\")\n",
    "        ]\n",
    "\n",
    "        if len(ticker_path) != 1:\n",
    "            raise ValueError(f\"Expected exactly 1 directory with name {self.ticker}\")\n",
    "        self.ticker_path = ticker_path[0]\n",
    "\n",
    "        # levels\n",
    "        if self.levels is None:\n",
    "            self.levels = int(self.ticker_path.split(\"_\")[-1])\n",
    "\n",
    "            if self.levels < 1:\n",
    "                raise ValueError(f\"Invalid number of levels: {self.levels}\")\n",
    "        if self.levels is None:\n",
    "            raise ValueError(\"Unable to infer levels from folder structure.\")\n",
    "        self.levels = t.cast(int, self.levels)\n",
    "\n",
    "        # infer date range from ticker folder name\n",
    "        if not self.date_range:\n",
    "            self.date_range = tuple(\n",
    "                re.findall(pattern=r\"\\d\\d\\d\\d-\\d\\d-\\d\\d\", string=self.ticker_path)\n",
    "            )\n",
    "            if len(self.date_range) != 2:\n",
    "                raise ValueError(\n",
    "                    f\"Expected exactly 2 dates in regex match of in {self.ticker_path}\"\n",
    "                )\n",
    "\n",
    "        # book and message paths\n",
    "        tickers = glob.glob(f\"{self.ticker_path}/*\")\n",
    "        tickers_end = list(map(os.path.basename, tickers))\n",
    "\n",
    "        if isinstance(self.date_range, tuple):\n",
    "            # get all dates in folder\n",
    "            dates = {\n",
    "                re.findall(pattern=r\"\\d\\d\\d\\d-\\d\\d-\\d\\d\", string=file)[0]\n",
    "                for file in tickers_end\n",
    "            }\n",
    "            # filter for dates within specified range\n",
    "            dates = sorted(\n",
    "                list(\n",
    "                    filter(\n",
    "                        lambda date: self.date_range[0] <= date <= self.date_range[1],\n",
    "                        dates,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.dates = dates\n",
    "            self.date_range = (min(self.dates), max(self.dates))\n",
    "\n",
    "        elif isinstance(self.date_range, str):\n",
    "            self.dates, self.date_range = [self.date_range], (\n",
    "                self.date_range,\n",
    "                self.date_range,\n",
    "            )\n",
    "\n",
    "        # messages and book filepath dictionaries\n",
    "        def _create_date_to_path_dict(keyword: str) -> dict:\n",
    "            filter_keyword_tickers = list(filter(lambda x: keyword in x, tickers_end))\n",
    "            date_path_dict = {}\n",
    "            for date in self.dates:\n",
    "                filter_date_tickers = list(\n",
    "                    filter(lambda x: date in x, filter_keyword_tickers)\n",
    "                )\n",
    "                if len(filter_date_tickers) != 1:\n",
    "                    raise ValueError(f\"Expected exactly 1 match for {date}\")\n",
    "                date_path_dict[date] = os.path.join(\n",
    "                    self.ticker_path, filter_date_tickers[0]\n",
    "                )\n",
    "            return date_path_dict\n",
    "\n",
    "        self.book_paths = _create_date_to_path_dict(\"book\")\n",
    "        self.messages_paths = _create_date_to_path_dict(\"message\")\n",
    "\n",
    "        self.load_book = False\n",
    "        self.load_messages = False\n",
    "        if self.load in (\"book\", \"both\"):\n",
    "            self.load_book = True\n",
    "        if self.load in (\"messages\", \"both\"):\n",
    "            self.load_messages = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "def _aggregate_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.reset_index(inplace=True)\n",
    "    duplicates = df.duplicated(subset=df.columns.difference([\"size\"]), keep=False)\n",
    "    df.loc[duplicates, \"size\"] = (\n",
    "        df.loc[duplicates, \"size\"].groupby(df.loc[duplicates].datetime).transform(\"sum\")\n",
    "    )\n",
    "    df.drop_duplicates(subset=df.columns.difference([\"size\"]), inplace=True)\n",
    "    df.set_index(\"datetime\", inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Lobster` loads the csv data into its `messages` and `book` attributes. The data to be loaded is passed in as a `Data` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def clip_times(\n",
    "    df: pd.DataFrame,\n",
    "    start: datetime.time | None = None,\n",
    "    end: datetime.time | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Clip a dataframe or lobster object to a time range.\"\"\"\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"Expected a dataframe with a datetime index\")\n",
    "\n",
    "    if start and end:\n",
    "        return df.iloc[(df.index.time >= start) & (df.index.time < end)]\n",
    "    elif start:\n",
    "        return df.iloc[df.index.time >= start]\n",
    "    elif end:\n",
    "        return df.iloc[df.index.time < end]\n",
    "    else:\n",
    "        raise ValueError(\"start and end cannot both be None\")\n",
    "\n",
    "\n",
    "_clip_to_trading_hours = partial(\n",
    "    clip_times, start=datetime.time(9, 30), end=datetime.time(16, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@dataclass\n",
    "class Lobster:\n",
    "    \"Lobster data class for a single symbol of Lobster data.\"\n",
    "    data: Data | None = None\n",
    "\n",
    "    def process_messages(self, date, filepath):\n",
    "        df = pd.read_csv(\n",
    "            filepath,\n",
    "            header=None,\n",
    "            nrows=self.data.nrows,\n",
    "            usecols=list(range(6)),\n",
    "            names=[\"time\", \"event\", \"order_id\", \"size\", \"price\", \"direction\"],\n",
    "            index_col=False,\n",
    "            dtype={\n",
    "                \"time\": \"float64\",\n",
    "                \"event\": \"int8\",\n",
    "                \"price\": \"int64\",\n",
    "                \"direction\": \"int8\",\n",
    "                \"order_id\": \"int32\",\n",
    "                \"size\": \"int64\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # set index as datetime\n",
    "        # df.rename(columns={'time':'seconds_since_midnight'})\n",
    "        df[\"datetime\"] = pd.to_datetime(date, format=\"%Y-%m-%d\") + df.time.apply(\n",
    "            lambda x: pd.to_timedelta(x, unit=\"s\")\n",
    "        )\n",
    "        df.set_index(\"datetime\", drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.data is None:\n",
    "            self.data = Data()\n",
    "\n",
    "        if self.data.load_messages:\n",
    "            # dfs = []\n",
    "            # for date, filepath in self.data.messages_paths.items():\n",
    "            #     # load messages\n",
    "            #     df = pd.read_csv(\n",
    "            #         filepath,\n",
    "            #         header=None,\n",
    "            #         nrows=self.data.nrows,\n",
    "            #         usecols=list(range(6)),\n",
    "            #         names=[\"time\", \"event\", \"order_id\", \"size\", \"price\", \"direction\"],\n",
    "            #         index_col=False,\n",
    "            #         dtype={\n",
    "            #             \"time\": \"float64\",\n",
    "            #             \"event\": \"int8\",\n",
    "            #             \"price\": \"int64\",\n",
    "            #             \"direction\": \"int8\",\n",
    "            #             \"order_id\": \"int32\",\n",
    "            #             \"size\": \"int64\",\n",
    "            #         },\n",
    "            #     )\n",
    "\n",
    "            #     # set index as datetime\n",
    "            #     # df.rename(columns={'time':'seconds_since_midnight'})\n",
    "            #     df[\"datetime\"] = pd.to_datetime(date, format=\"%Y-%m-%d\") + df.time.apply(lambda x: pd.to_timedelta(x, unit=\"s\"))\n",
    "            #     df.set_index(\"datetime\", drop=True, inplace=True)\n",
    "            #     dfs.append(df)\n",
    "            # df = pd.concat(dfs)\n",
    "\n",
    "            # refactor for memory use\n",
    "            dfs = (\n",
    "                self.process_messages(date=date, filepath=filepath)\n",
    "                for date, filepath in self.data.messages_paths.items()\n",
    "            )\n",
    "            df = pd.concat(dfs)\n",
    "\n",
    "            # direction for cross trades is set to zero, and order_id is left unchanged\n",
    "            if (\n",
    "                not df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"]\n",
    "                .eq(-1)\n",
    "                .all()\n",
    "            ):\n",
    "                raise ValueError(\"All cross trades must have direction -1\")\n",
    "            df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"] = 0\n",
    "\n",
    "            # seems as though this is not true?\n",
    "            # if not df.loc[df.event.eq(Event.CROSS_TRADE.value), \"order_id\"].eq(-1).all():\n",
    "            #     raise ValueError(\"All cross trades must have order_id -1\")\n",
    "\n",
    "            if (\n",
    "                not df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"]\n",
    "                .eq(-1)\n",
    "                .all()\n",
    "            ):\n",
    "                raise ValueError(\"All trading halts must have direction -1\")\n",
    "            df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"] = 0\n",
    "\n",
    "            # # process trading halts and map to new trading halts\n",
    "            def _trading_halt_type(price):\n",
    "                return {\n",
    "                    -1: Event.TRADING_HALT.value,\n",
    "                    0: Event.RESUME_QUOTE.value,\n",
    "                    1: Event.TRADING_RESUME.value,\n",
    "                }[price]\n",
    "\n",
    "            # doesn't make much difference\n",
    "            df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"event\"] = df.loc[\n",
    "                df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"price\"\n",
    "            ].apply(_trading_halt_type)\n",
    "\n",
    "            # implentation of above without apply\n",
    "            # trading_halt_mask = (df.event == Event.TRADING_HALT.value)\n",
    "            # halt_type_mapping = {\n",
    "            #     -1: Event.TRADING_HALT.value,\n",
    "            #     0: Event.RESUME_QUOTE.value,\n",
    "            #     1: Event.TRADING_RESUME.value,\n",
    "            # }\n",
    "            # df.loc[trading_halt_mask, \"event\"] = df.loc[trading_halt_mask, \"price\"].map(halt_type_mapping)\n",
    "\n",
    "            # use 0 as NaN for size and direction\n",
    "            df.loc[\n",
    "                df.event.isin(EventGroup.HALTS.value),\n",
    "                [\"order_id\", \"size\", \"price\"],\n",
    "            ] = [0, 0, np.nan]\n",
    "\n",
    "            # set price in dollars\n",
    "            df.price = df.price.apply(lambda x: x / 10_000).astype(\"float64\")\n",
    "\n",
    "            if self.data.ticker_type:\n",
    "                # TODO change to get Literal Values from args?\n",
    "\n",
    "                if self.data.ticker_type not in [\n",
    "                    \"equity\",\n",
    "                    \"etf\",\n",
    "                ]:\n",
    "                    raise ValueError(\"ticker_type must be either `equity` or `etf`\")\n",
    "                # assert self.data.ticker_type in [\n",
    "                #     \"equity\",\n",
    "                #     \"etf\",\n",
    "                # ], \"ticker_type must be either `equity` or `etf`\"\n",
    "                df = df.assign(ticker_type=self.data.ticker_type).astype(\n",
    "                    dtype={\n",
    "                        \"ticker_type\": pd.CategoricalDtype(categories=[\"equity\", \"etf\"])\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            self.messages = df\n",
    "\n",
    "        if self.data.load_book:\n",
    "            col_names = []\n",
    "            for level in range(1, self.data.levels + 1):\n",
    "                for col_type in [\"ask_price\", \"ask_size\", \"bid_price\", \"bid_size\"]:\n",
    "                    col_name = f\"{col_type}_{level}\"\n",
    "                    col_names.append(col_name)\n",
    "\n",
    "            # for now just use float64\n",
    "            # col_dtypes = {\n",
    "            #     col_name: pd.Int64Dtype() if (\"size\" in col_name) else \"float\"\n",
    "            #     for col_name in col_names\n",
    "            # }\n",
    "\n",
    "            dfs = []\n",
    "            for filename in self.data.book_paths.values():\n",
    "                df = pd.read_csv(\n",
    "                    filename,\n",
    "                    header=None,\n",
    "                    nrows=self.data.nrows,\n",
    "                    usecols=list(range(4 * self.data.levels)),\n",
    "                    names=col_names,\n",
    "                    dtype=\"float64\",\n",
    "                    na_values=[\"-9999999999\", \"9999999999\", \"0\"],\n",
    "                )\n",
    "\n",
    "                dfs.append(df)\n",
    "            df = pd.concat(dfs)\n",
    "\n",
    "            df.set_index(self.messages.index, inplace=True, drop=True)\n",
    "\n",
    "            price_cols = df.columns.str.contains(\"price\")\n",
    "            df.loc[:, price_cols] = df.loc[:, price_cols] / 10_000\n",
    "\n",
    "            self.book = df\n",
    "\n",
    "        # data cleaning on messages done only now, as book infers times from messages file\n",
    "        # TODO: think if leaving bool flags good idea\n",
    "        if self.data.aggregate_duplicates:\n",
    "            self.aggregate_duplicates()\n",
    "        if self.data.clip_trading_hours:\n",
    "            self.clip_trading_hours()\n",
    "        if self.data.add_ticker_column:\n",
    "            self.add_ticker_column()\n",
    "\n",
    "    def clip_trading_hours(self) -> t.Self:\n",
    "        if hasattr(self, \"book\"):\n",
    "            self.book = _clip_to_trading_hours(self.book)\n",
    "        if hasattr(self, \"messages\"):\n",
    "            self.messages = _clip_to_trading_hours(self.messages)\n",
    "        return self\n",
    "\n",
    "    def aggregate_duplicates(self) -> t.Self:\n",
    "        self.messages = _aggregate_duplicates(self.messages)\n",
    "        return self\n",
    "\n",
    "    # TODO: write decorator to simplify the \"both\", \"messages\", \"book\" logic that is common to a few methods\n",
    "    def add_ticker_column(\n",
    "        self, to: t.Literal[\"both\", \"messages\", \"book\"] = \"messages\"\n",
    "    ) -> t.Self:\n",
    "        if to in (\"both\", \"messages\"):\n",
    "            self.messages = self.messages.assign(ticker=self.data.ticker).astype(\n",
    "                {\"ticker\": \"category\"}\n",
    "            )\n",
    "        if to in (\"both\", \"book\"):\n",
    "            self.book = self.book.assign(ticker=self.data.ticker).astype(\n",
    "                {\"ticker\": \"category\"}\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Lobster data for ticker: {self.data.ticker} for date range: {self.data.date_range[0]} to {self.data.date_range[1]}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "class MPLobster:\n",
    "    \"Lobster data class for a single symbol of Lobster data.\"\n",
    "    MAX_WORKERS: int = 70\n",
    "\n",
    "    @staticmethod\n",
    "    def process_messages(date, filepath):\n",
    "        print(f\"start {date}\")\n",
    "        df = pd.read_csv(\n",
    "            filepath,\n",
    "            header=None,\n",
    "            nrows=None,\n",
    "            usecols=list(range(6)),\n",
    "            names=[\"time\", \"event\", \"order_id\", \"size\", \"price\", \"direction\"],\n",
    "            index_col=False,\n",
    "            dtype={\n",
    "                \"time\": \"float64\",\n",
    "                \"event\": \"int8\",\n",
    "                \"price\": \"int64\",\n",
    "                \"direction\": \"int8\",\n",
    "                \"order_id\": \"int32\",\n",
    "                \"size\": \"int64\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # set index as datetime\n",
    "        df[\"datetime\"] = pd.to_datetime(date, format=\"%Y-%m-%d\") + df.time.apply(\n",
    "            lambda x: pd.to_timedelta(x, unit=\"s\")\n",
    "        )\n",
    "        df.set_index(\"datetime\", drop=True, inplace=True)\n",
    "        print(f\"finished {date}\")\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def process_all_messages(messages_paths, max_workers=70):\n",
    "        print(max_workers)\n",
    "        mp.set_start_method(\"spawn\")\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            dfs = list(\n",
    "                executor.map(\n",
    "                    MPLobster.process_messages,\n",
    "                    messages_paths.keys(),\n",
    "                    messages_paths.values(),\n",
    "                )\n",
    "            )\n",
    "            # dfs = (self.process_messages(date=date, filepath=filepath) for date, filepath in self.data.messages_paths.items())\n",
    "        df = pd.concat(dfs)\n",
    "        del dfs\n",
    "        gc.collect()\n",
    "\n",
    "        # direction for cross trades is set to zero, and order_id is left unchanged\n",
    "        if not df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"].eq(-1).all():\n",
    "            raise ValueError(\"All cross trades must have direction -1\")\n",
    "        df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"] = 0\n",
    "\n",
    "        if (\n",
    "            not df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"]\n",
    "            .eq(-1)\n",
    "            .all()\n",
    "        ):\n",
    "            raise ValueError(\"All trading halts must have direction -1\")\n",
    "        df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"] = 0\n",
    "\n",
    "        # process trading halts and map to new trading halts\n",
    "        def _trading_halt_type(price):\n",
    "            return {\n",
    "                -1: Event.TRADING_HALT.value,\n",
    "                0: Event.RESUME_QUOTE.value,\n",
    "                1: Event.TRADING_RESUME.value,\n",
    "            }[price]\n",
    "\n",
    "        df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"event\"] = df.loc[\n",
    "            df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"price\"\n",
    "        ].apply(_trading_halt_type)\n",
    "\n",
    "        # use 0 as NaN for size and direction\n",
    "        df.loc[\n",
    "            df.event.isin(EventGroup.HALTS.value),\n",
    "            [\"order_id\", \"size\", \"price\"],\n",
    "        ] = [0, 0, np.nan]\n",
    "\n",
    "        # set price in dollars\n",
    "        df[\"price\"] = (df.price / 10_000).astype(\"float\")\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def process_books(filename):\n",
    "        print(f\"start {filename}\")\n",
    "        col_names = []\n",
    "        for level in range(1, 10 + 1):\n",
    "            for col_type in [\"ask_price\", \"ask_size\", \"bid_price\", \"bid_size\"]:\n",
    "                col_name = f\"{col_type}_{level}\"\n",
    "                col_names.append(col_name)\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            filename,\n",
    "            header=None,\n",
    "            nrows=None,\n",
    "            usecols=None,\n",
    "            names=col_names,\n",
    "            dtype=\"float64\",\n",
    "            na_values=[\"-9999999999\", \"9999999999\", \"0\"],\n",
    "        )\n",
    "        print(f\"end {filename}\")\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def process_all_books(book_paths: dict, max_workers=70):\n",
    "        # check if set\n",
    "        if mp.get_start_method(allow_none=True) is None:\n",
    "            mp.set_start_method(\"spawn\")\n",
    "        print(max_workers)\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            dfs = list(executor.map(MPLobster.process_books, book_paths.values()))\n",
    "        df = pd.concat(dfs)\n",
    "        del dfs\n",
    "\n",
    "        # for now just don't grab message index..\n",
    "        # df.set_index(self.messages.index, inplace=True, drop=True)\n",
    "\n",
    "        price_cols = df.columns.str.contains(\"price\")\n",
    "        df.loc[:, price_cols] = df.loc[:, price_cols] / 10_000\n",
    "        return df\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        if self.data.load_messages:\n",
    "            self.messages = MPLobster.process_all_messages(\n",
    "                messages_paths=self.data.messages_paths\n",
    "            )\n",
    "            gc.collect()\n",
    "\n",
    "        if self.data.load_book:\n",
    "            book = MPLobster.process_all_books(book_paths=self.data.book_paths)\n",
    "            # set index here so that process_all_books can be a static_method\n",
    "            book.set_index(self.messages.index, inplace=True, drop=True)\n",
    "            self.book = book\n",
    "\n",
    "        # everything below here not that important\n",
    "        # TODO: think if leaving bool flags good idea\n",
    "        if self.data.aggregate_duplicates:\n",
    "            self.aggregate_duplicates()\n",
    "        if self.data.clip_trading_hours:\n",
    "            self.clip_trading_hours()\n",
    "        if self.data.add_ticker_column:\n",
    "            self.add_ticker_column()\n",
    "\n",
    "    def clip_trading_hours(self) -> t.Self:\n",
    "        if hasattr(self, \"book\"):\n",
    "            self.book = _clip_to_trading_hours(self.book)\n",
    "        if hasattr(self, \"messages\"):\n",
    "            self.messages = _clip_to_trading_hours(self.messages)\n",
    "        return self\n",
    "\n",
    "    def aggregate_duplicates(self) -> t.Self:\n",
    "        self.messages = _aggregate_duplicates(self.messages)\n",
    "        return self\n",
    "\n",
    "    # TODO: write decorator to simplify the \"both\", \"messages\", \"book\" logic that is common to a few methods\n",
    "    def add_ticker_column(\n",
    "        self, to: t.Literal[\"both\", \"messages\", \"book\"] = \"messages\"\n",
    "    ) -> t.Self:\n",
    "        if to in (\"both\", \"messages\"):\n",
    "            self.messages = self.messages.assign(ticker=self.data.ticker).astype(\n",
    "                {\"ticker\": \"category\"}\n",
    "            )\n",
    "        if to in (\"both\", \"book\"):\n",
    "            self.book = self.book.assign(ticker=self.data.ticker).astype(\n",
    "                {\"ticker\": \"category\"}\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Lobster data for ticker: {self.data.ticker} for date range: {self.data.date_range[0]} to {self.data.date_range[1]}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing the `Data` class into another python notebook or script, it may be convenient to overwrite defaults arguments such as `directory_path`. This way these arguments need only be specified once. This can be achieved by using  `functools.partial` or by using inheritance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "MyData = partial(\n",
    "    Data,\n",
    "    directory_path=\"/my/local/data/path\",\n",
    "    ticker=\"AIG\",\n",
    "    clip_trading_hours=True,\n",
    "    load=\"both\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "class MyData(Data):\n",
    "    def __init__(\n",
    "        self,\n",
    "        directory_path=\"/my/local/data/path\",\n",
    "        ticker=\"AIG\",\n",
    "        clip_trading_hours=True,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            directory_path=directory_path,\n",
    "            ticker=ticker,\n",
    "            clip_trading_hours=clip_trading_hours,\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(directory_path='../data', ticker='AMZN', date_range=('2012-06-21', '2012-06-21'), levels=5, nrows=None, load='both', add_ticker_column=False, ticker_type=None, clip_trading_hours=True, aggregate_duplicates=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class FolderInfo:\n",
    "    full: str\n",
    "    ticker: str\n",
    "    ticker_till_end: str\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    date_range: tuple[str, str] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.date_range = (self.start_date, self.end_date)\n",
    "\n",
    "\n",
    "def infer_ticker_dict(\n",
    "    files_path: str = \"/nfs/home/nicolasp/home/data/tmp\", /\n",
    ") -> list[FolderInfo]:\n",
    "    \"\"\"Infer from folder structure the ticker to date_range mapping.\"\"\"\n",
    "    files_path_ = Path(files_path)\n",
    "    all_folders = [str(path) for path in files_path_.glob(\"*\")]\n",
    "\n",
    "    ticker_pattern = re.compile(\n",
    "        r\"(.*?)(([A-Z.]{1,7})_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})(.*))\"\n",
    "    )\n",
    "\n",
    "    folder_info = [\n",
    "        FolderInfo(\n",
    "            full=match.group(0),\n",
    "            ticker=match.group(3),\n",
    "            ticker_till_end=match.group(2).rstrip(\".7z\"),\n",
    "            start_date=match.group(4),\n",
    "            end_date=match.group(5),\n",
    "        )\n",
    "        for folder in all_folders\n",
    "        if (match := ticker_pattern.search(folder))\n",
    "    ]\n",
    "    return folder_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
