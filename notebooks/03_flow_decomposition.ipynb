{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETF flow decomposition\n",
    "\n",
    "> Nearest neighbor functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp flow_decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from functools import partial\n",
    "import itertools as it\n",
    "import typing as t\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "from lobster_tools.preprocessing import *\n",
    "from lobster_tools.querying import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_times(df: pd.DataFrame) -> NDArray[np.datetime64]:\n",
    "    \"Return numpy array of times from the index of the DataFrame.\"\n",
    "    if df.index.values.dtype != \"datetime64[ns]\":\n",
    "        raise TypeError(\"DataFrame index must be of type datetime64[ns]\")\n",
    "    return df.index.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def add_neighbors(\n",
    "    etf_executions: pd.DataFrame,\n",
    "    equity_executions: pd.DataFrame,\n",
    "    tolerances: list[str],\n",
    "):\n",
    "    \"\"\"Annotate the etf execution dataframe with the indices of the neighbouring equity executions.\n",
    "    Note: Building the KDTree on the equity dataframe. Blah\n",
    "    \"\"\"\n",
    "    etf_executions = etf_executions.copy()\n",
    "\n",
    "    etf_executions_times = get_times(etf_executions)\n",
    "    equity_executions_times = get_times(equity_executions)\n",
    "    equity_tree = KDTree(equity_executions_times, metric=\"l1\")\n",
    "\n",
    "    def _add_neighbors_col(etf_executions, tolerance_str):\n",
    "        tolerance_in_nanoseconds = str_to_nanoseconds(tolerance_str)\n",
    "        etf_executions[f\"_{tolerance_str}_neighbors\"] = equity_tree.query_radius(\n",
    "            etf_executions_times, r=tolerance_in_nanoseconds\n",
    "        )\n",
    "        etf_executions[f\"_{tolerance_str}_non-iso\"] = etf_executions[\n",
    "            f\"_{tolerance_str}_neighbors\"\n",
    "        ].apply(lambda x: x.size > 0)\n",
    "\n",
    "    for tolerance in tolerances:\n",
    "        _add_neighbors_col(etf_executions, tolerance)\n",
    "\n",
    "    return etf_executions\n",
    "\n",
    "\n",
    "def drop_all_neighbor_cols(df: pd.DataFrame):\n",
    "    \"Drop neighbor columns inplace.\"\n",
    "    neighbor_column_names = df.filter(regex=\"neighbors\").columns\n",
    "    df.drop(columns=neighbor_column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def col_to_dtype_inputing_mapping(col, col_to_dtype_dict):\n",
    "    for k, v in col_to_dtype_dict.items():\n",
    "        if k in col:\n",
    "            return v\n",
    "\n",
    "\n",
    "col_to_dtype = partial(\n",
    "    col_to_dtype_inputing_mapping,\n",
    "    col_to_dtype_dict={\n",
    "        \"notional\": pd.SparseDtype(float, 0),\n",
    "        \"num_trades\": pd.SparseDtype(int, 0),\n",
    "        \"distinct_tickers\": pd.SparseDtype(int, 0),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "features = [\"distinct_tickers\", \"notional\", \"num_trades\"]\n",
    "all_index = [\"_\".join(t) for t in it.product(features, [\"ss\", \"os\"], [\"bf\", \"af\"])]\n",
    "\n",
    "empty_series = pd.Series(index=all_index, dtype=\"Sparse[float]\").fillna(0)\n",
    "empty_series = pd.Series(index=all_index, dtype=\"float\").fillna(0)\n",
    "\n",
    "\n",
    "def multi_index_to_single_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.index = [\"_\".join(index_tuple) for index_tuple in df.index]\n",
    "    return df\n",
    "\n",
    "\n",
    "def groupby_index_to_series(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Hierachical groupby index with one column to flattened series. Prepending the column name to the index.\"\"\"\n",
    "    return df.stack().reorder_levels([-1, 0, 1]).pipe(multi_index_to_single_index)\n",
    "\n",
    "\n",
    "def compute_features(\n",
    "    etf_trade_time, etf_trade_direction, neigh: t.Optional[np.ndarray], equity_executions: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    if neigh is None:\n",
    "        return empty_series\n",
    "    elif isinstance(neigh, np.ndarray):\n",
    "        df = equity_executions.iloc[neigh].assign(\n",
    "            bf_af=lambda df: df.index < etf_trade_time,\n",
    "            ss_os=lambda df: df.direction == etf_trade_direction,\n",
    "        )\n",
    "        df[\"ss_os\"] = (\n",
    "            df[\"ss_os\"].apply(lambda x: \"ss\" if x else \"os\").astype(\"category\")\n",
    "        )\n",
    "        df[\"bf_af\"] = (\n",
    "            df[\"bf_af\"].apply(lambda x: \"bf\" if x else \"af\").astype(\"category\")\n",
    "        )\n",
    "\n",
    "        df_subset = df[[\"ticker\", \"ss_os\", \"bf_af\", \"price\", \"size\"]]\n",
    "\n",
    "        # notional value and num trades\n",
    "        notional_and_num_trades = (\n",
    "            df_subset.eval('notional = price * size.astype(\"int64\")')\n",
    "            .groupby([\"ss_os\", \"bf_af\"])\n",
    "            .agg(notional=(\"notional\", \"sum\"), num_trades=(\"size\", \"count\"))\n",
    "            .pipe(groupby_index_to_series)\n",
    "        )\n",
    "\n",
    "        # distinct tickers\n",
    "        distinct_tickers = (\n",
    "            df_subset.drop(columns=\"size\")\n",
    "            .groupby([\"ticker\", \"ss_os\", \"bf_af\"])\n",
    "            .count()\n",
    "            .applymap(lambda x: x > 0)\n",
    "            .groupby([\"ss_os\", \"bf_af\"])\n",
    "            .sum()\n",
    "            .rename(columns={\"price\": \"distinct_tickers\"})\n",
    "            .pipe(groupby_index_to_series)\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            pd.concat([notional_and_num_trades, distinct_tickers])\n",
    "            .reindex(all_index)\n",
    "            .fillna(0)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"neigh must be None or list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def append_features(\n",
    "    etf_executions: pd.DataFrame, equity_executions: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"Note that this function is not inplace.\"\n",
    "    # infer tolerances from column names\n",
    "    column_names = etf_executions.filter(regex=\"neighbors\").columns.values.tolist()\n",
    "    tolerances = [i.split(\"_\")[1] for i in column_names]\n",
    "\n",
    "    # TODO: check if its faster to partial compute features with equity executions\n",
    "\n",
    "    features_dfs = []\n",
    "    for tolerance in tolerances:\n",
    "        # add_neighbors(df, equity_executions, tolerance)\n",
    "        features = etf_executions.apply(\n",
    "            lambda row: compute_features(\n",
    "                row.name,\n",
    "                row.direction,\n",
    "                row[f\"_{tolerance}_neighbors\"],\n",
    "                equity_executions=equity_executions,\n",
    "            ),\n",
    "            axis=1,\n",
    "            result_type=\"expand\",\n",
    "        ).add_prefix(f\"_{tolerance}_\")\n",
    "\n",
    "        features = features.astype({col: col_to_dtype(col) for col in features.columns})\n",
    "\n",
    "        features_dfs += [features]\n",
    "\n",
    "    features_df = pd.concat(features_dfs, axis=1)\n",
    "    return pd.concat([etf_executions, features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def count_non_null(df, tolerance):\n",
    "    return df[f\"_{tolerance}_neighbors\"].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def drop_features(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Drops all intermediate features, and just leaves the arbitrage tags.\n",
    "    Not the nicest way. Could do better regex.\"\"\"\n",
    "    features_and_arb_tag = set(df.filter(regex=\"^_[0-9]+ms_\").columns)\n",
    "    arb_tag = set(df.filter(regex=\"arb_tag\").columns)\n",
    "    features = features_and_arb_tag - arb_tag\n",
    "    df.drop(columns=features, inplace=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def split_isolated_non_isolated(etf_executions: pd.DataFrame, tolerance) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Returns a tuple of (isolated, non_isolated). For now, use deep copy, although this may not be great.\"\"\"\n",
    "    tolerance_str = f\"_{tolerance}_neighbors\"\n",
    "    isolated_indices = etf_executions[tolerance_str].isna()\n",
    "    return etf_executions[isolated_indices].copy(deep=True), etf_executions[~isolated_indices].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def resample_mid(df: pd.DataFrame, resample_freq=\"5T\"):\n",
    "    return df.resample(resample_freq, label=\"right\").last().eval(\"mid = bid_price_1 + (ask_price_1 - bid_price_1) / 2\")[\"mid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def restrict_common_index(df1: pd.DataFrame, df2: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Restrict two dataframes to their common index.\"\"\"\n",
    "    common_index = df1.index.intersection(df2.index)\n",
    "    return df1.loc[common_index], df2.loc[common_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def markout_returns(\n",
    "    df,  # dataframe to infer times to markout from\n",
    "    markouts: list[str],  # list of markouts to compute returns for\n",
    ") -> pd.DataFrame:\n",
    "    return pd.DataFrame(index=df.index, data={f\"_{markout}\": df.index + pd.Timedelta(markout) for markout in markouts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def clip_for_markout(df, max_markout):\n",
    "    end = (max(df.index) - pd.Timedelta(max_markout)).time()\n",
    "    return clip_df_times(df, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
