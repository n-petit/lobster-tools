{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arctic\n",
    "\n",
    "> Arctic helper scripts and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp arctic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "from string import Template\n",
    "import click\n",
    "from click.testing import CliRunner\n",
    "from arcticdb import Arctic, QueryBuilder\n",
    "from arcticdb.version_store.library import Library\n",
    "from arcticdb.exceptions import LibraryNotFound\n",
    "import hydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "import textwrap\n",
    "from lobster_tools.config import (\n",
    "    MainConfig,\n",
    "    Overrides,\n",
    "    NASDAQExchange,\n",
    "    ETFMembers,\n",
    "    etf_to_equities,\n",
    "    register_configs,\n",
    "    get_config,\n",
    ")\n",
    "from lobster_tools.preprocessing import Data, Lobster, MPLobster, Event, infer_ticker_to_date_range, infer_ticker_to_ticker_path, infer_ticker_dict, EhMPLobster\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from logging import Logger\n",
    "from datetime import date\n",
    "from typing import Callable, TypedDict, Protocol, NotRequired, Required, cast\n",
    "from dataclasses import dataclass, asdict\n",
    "import time\n",
    "from inspect import signature\n",
    "from functools import wraps\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, wait, as_completed\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `@hydra.main` decorator, `register_configs` must be called. If simply using a notebook or writing the CLIs with `click`, it is enough to use `get_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "register_configs()\n",
    "cfg = get_config(overrides=Overrides.full_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "CONTEXT_SETTINGS = dict(\n",
    "    help_option_names=[\"-h\", \"--help\"],\n",
    "    token_normalize_func=lambda x: x.lower() if isinstance(x, str) else x,\n",
    "    show_default=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @click.group()\n",
    "# @click.option('--debug/--no-debug', default=False)\n",
    "# @click.pass_context\n",
    "# def cli(ctx, debug):\n",
    "#     # ensure that ctx.obj exists and is a dict (in case `cli()` is called\n",
    "#     # by means other than the `if` block below)\n",
    "#     ctx.ensure_object(dict)\n",
    "\n",
    "#     ctx.obj['DEBUG'] = debug\n",
    "\n",
    "# @cli.command()\n",
    "# @click.pass_context\n",
    "# def sync(ctx):\n",
    "#     click.echo(f\"Debug is {'on' if ctx.obj['DEBUG'] else 'off'}\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     cli(obj={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# option_dict = {\n",
    "#     'db_path': click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\"),\n",
    "#     'library': click.option(\"-l\", \"--library\", default=cfg.db.db_path, help=\"Library name\"),\n",
    "# }\n",
    "\n",
    "# # Custom decorator to apply options based on a list of names\n",
    "# def apply_options(option_names):\n",
    "#     def decorator(f):\n",
    "#         for option_name in reversed(option_names):\n",
    "#             f = option_dict[option_name](f)\n",
    "#         return f\n",
    "#     return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Options:\n",
    "    def __init__(self) -> None:\n",
    "        self.db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\")\n",
    "        self.library = click.option(\"-l\", \"--library\", default=cfg.db.db_path, help=\"Library name\")\n",
    "\n",
    "def apply_options(options: list):\n",
    "    def decorator(f):\n",
    "        for option in reversed(options):\n",
    "            f = option(f)\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "@click.group()\n",
    "def arctic():\n",
    "    pass\n",
    "\n",
    "O = Options()\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path])\n",
    "def initdb(db_path):\n",
    "    print(f'Initialized the database {db_path}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library])\n",
    "def use_both(db_path, library):\n",
    "    print(f'Initialized the database {db_path} {library}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@arctic.command()\n",
    "def dropdb():\n",
    "    click.echo('Dropped the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "click_db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "click_library = click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "\n",
    "@click.group()\n",
    "def cool():\n",
    "    pass\n",
    "\n",
    "@cool.command()\n",
    "@click_db_path\n",
    "def initdb(db_path):\n",
    "    print(f'Initialized the database {db_path}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@cool.command()\n",
    "def dropdb():\n",
    "    click.echo('Dropped the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@click.group()\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.pass_context\n",
    "def nic(ctx, db_path, library):\n",
    "    ctx.ensure_object(dict)\n",
    "    ctx.obj['library'] = library\n",
    "\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    ctx.obj['arctic'] = arctic\n",
    "\n",
    "@nic.command()\n",
    "@click.option(\"-t\", \"--ticker\", default=\"AMZN\", help=\"ticker\")\n",
    "@click.pass_context\n",
    "def read(ctx, ticker):\n",
    "    arctic = ctx.obj[\"arctic\"]\n",
    "    library = ctx.obj[\"library\"]\n",
    "    df = arctic[library].read(ticker).data\n",
    "    print(f'df.head() {df.head()}')\n",
    "\n",
    "@nic.command()\n",
    "@click.pass_context\n",
    "def dropdb(ctx):\n",
    "    click.echo('Dropped the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_arctic_library(db_path, library):\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic_library = arctic[library]\n",
    "    return arctic_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code had library passed to all. maybe nicer to use the context thing in the end and do sth like\n",
    "arctic --library=testa --db_path=sth NEXT_COMMAND. This maybe makes some of the decorators i had less relevant as i won't be specifying libray and db_path all over the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "# # TODO: csv_path vs csv_files_path. think if this is a problem...maybe unify?\n",
    "# class Options:\n",
    "#     def __init__(self) -> None:\n",
    "#         self.db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\")\n",
    "#         self.library = click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"Library name\")\n",
    "#         self.ticker = click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to print\")\n",
    "#         self.start_date = click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "#         self.end_date = click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "#         self.csv_path = click.option(\"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\")\n",
    "#         self.etf = click.option(\"--etf\", default=None, help=\"restrict to subset specified by ETF members\")\n",
    "#         self.zip_path = click.option(\"-z\", \"--zip_path\", default=\"/nfs/lobster_data/lobster_raw/2016\", help=\"zip files path\")\n",
    "#         self.tickers = click.option(\"--tickers\", default=None, multiple=True, type=str, help=\"tickers to dump\")\n",
    "#         self.max_workers = click.option(\"-m\", \"--max_workers\", default=20, help=\"max workers for parallelisation\")\n",
    "# O = Options()\n",
    "\n",
    "# def apply_options(options: list):\n",
    "#     def decorator(f):\n",
    "#         for option in reversed(options):\n",
    "#             f = option(f)\n",
    "#         return f\n",
    "#     return decorator\n",
    "\n",
    "# def inherit_docstring_from(source_fn):\n",
    "#     def decorator(target_fn):\n",
    "#         target_fn.__doc__ = source_fn.__doc__\n",
    "#         return target_fn\n",
    "#     return decorator\n",
    "\n",
    "# def infer_options(func) -> list[Callable]:\n",
    "#     \"\"\"Works together with the `auto_apply` to automatically infer arguments.\n",
    "    \n",
    "#     Used together this looks like:\n",
    "#     @auto_apply(infer_options)\n",
    "#     \"\"\"\n",
    "#     sig = signature(func)\n",
    "#     param_names = [\n",
    "#         param.name\n",
    "#         for param in sig.parameters.values()\n",
    "#         if param.kind == param.POSITIONAL_OR_KEYWORD\n",
    "#     ]\n",
    "#     options_list = [getattr(O, name) for name in param_names]\n",
    "#     return options_list\n",
    "\n",
    "# def inherits_from(func):\n",
    "#     \"\"\"Inherit docstring and options from `func`. Actually this wasn't the best idea. Keep separate\"\"\"\n",
    "#     options_list = infer_options(func)\n",
    "\n",
    "#     # still stlightly confused about the order of the decorators, oh well\n",
    "#     def decorator(target_fn):\n",
    "#         @inherit_docstring_from(func)\n",
    "#         @apply_options(options_list)\n",
    "#         @wraps(target_fn)\n",
    "#         def wrapper(*args, **kwargs):\n",
    "#             return target_fn(*args, **kwargs)\n",
    "#         return wrapper\n",
    "#     return decorator\n",
    "\n",
    "# # def simple_inherits_from(func):\n",
    "# #     \"\"\"Simple without using functools.wraps\"\"\"\n",
    "# #     options_list = infer_options(func)\n",
    "# #     def decorator(target_fn):\n",
    "# #         decorated = apply_options(options_list)(target_fn)\n",
    "# #         decorated.__doc__ = func.__doc__\n",
    "# #         return decorated\n",
    "# #     return decorator\n",
    "\n",
    "# @click.group(context_settings=CONTEXT_SETTINGS)\n",
    "# def arctic():\n",
    "#     pass\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path])\n",
    "# def list_libraries(db_path) -> None:\n",
    "#     \"\"\"List arcticdb libraries\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic.list_libraries())\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def list_symbols(db_path, library) -> None:\n",
    "#     \"\"\"List symbols in the arcticdb library.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_symbols())\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def create_library(db_path, library) -> None:\n",
    "#     \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     arctic.create_library(library) \n",
    "#     print(arctic[library])\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# @click.confirmation_option(prompt='Are you sure you want to delete the entire library?')\n",
    "# def delete_library(db_path, library) -> None:\n",
    "#     \"\"\"Delete entire arcticdb library\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     arctic.delete_library(library) \n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.ticker, O.start_date, O.end_date])\n",
    "# def read(db_path, library, ticker, start_date, end_date,\n",
    "# ):\n",
    "#     \"\"\"Read ticker and print head and tail.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     if start_date and end_date:\n",
    "#         start_datetime = pd.Timestamp(f\"{start_date}T{NASDAQExchange.exchange_open}\")\n",
    "#         end_datetime = pd.Timestamp(f\"{end_date}T{NASDAQExchange.exchange_close}\")\n",
    "#         date_range = (start_datetime, end_datetime)\n",
    "#         df = arctic[library].read(ticker, date_range=date_range).data\n",
    "#     else:\n",
    "#         print(\"not using start or end dates\")\n",
    "#         df = arctic[library].read(ticker).data\n",
    "    \n",
    "#     print(f\"Printing df.head() and df.tail() for ticker {ticker}\")\n",
    "#     print(df.head())\n",
    "#     print(df.tail())\n",
    "\n",
    "# def _write(\n",
    "#     db_path,\n",
    "#     library,\n",
    "#     csv_path,\n",
    "#     ticker,\n",
    "#     start_date,\n",
    "#     end_date,\n",
    "# ):\n",
    "#     \"\"\"Preprocess and write ticker to database.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     date_range = (start_date, end_date)\n",
    "#     data = Data(\n",
    "#         directory_path=csv_path,\n",
    "#         ticker=ticker,\n",
    "#         date_range=date_range,\n",
    "#         aggregate_duplicates=False,\n",
    "#     )\n",
    "#     lobster = Lobster(data=data)\n",
    "#     df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "#     print(f\"head of ticker {ticker}\")\n",
    "#     print(df.head())\n",
    "\n",
    "#     arctic[library].write(symbol=ticker, data=df)\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.ticker, O.start_date, O.end_date])\n",
    "# def write(**kwargs):\n",
    "#     _write(**kwargs)\n",
    "\n",
    "\n",
    "# # if want to also access _say from other functions then need to do this.\n",
    "# def _say(\n",
    "#     db_path,\n",
    "#     library,\n",
    "# ):\n",
    "#     \"\"\"Print some really important information\"\"\"\n",
    "#     print(db_path, library)\n",
    "\n",
    "\n",
    "# # @arctic.command()\n",
    "# # @apply_options(infer_options(_say))\n",
    "# # @inherit_docstring_from(_say)\n",
    "# # def say(**kwargs):\n",
    "# #     _say(**kwargs)\n",
    "\n",
    "# @arctic.command()\n",
    "# @inherits_from(_say)\n",
    "# def say(**kwargs):\n",
    "#     _say(**kwargs)\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.start_date, O.end_date])\n",
    "# def generate_jobs(db_path, library, csv_path, start_date, end_date):\n",
    "#     ticker_date_dict = infer_ticker_to_date_range(csv_path)\n",
    "#     with open('arctic_commands.txt', 'w') as f:\n",
    "#         for ticker, (inferred_start_date, inferred_end_date) in ticker_date_dict.items():\n",
    "#             # if date is None use the inferred date, otherwise use the CLI argument\n",
    "#             start_date = start_date or inferred_start_date\n",
    "#             end_date = end_date or inferred_end_date\n",
    "#             f.write(f\"arctic write --csv_path={csv_path} --db_path={db_path} --library={library} --ticker={ticker} --start_date={start_date} --end_date={end_date} \\n\")\n",
    "\n",
    "# def sleepy(csv_path, folder_info):\n",
    "#     time.sleep(5)\n",
    "#     print(csv_path, folder_info.full)\n",
    "\n",
    "# def extract_7z(input_path, output_path):\n",
    "#     try:\n",
    "#         subprocess.run([\"7z\", \"x\", input_path, f\"-o{output_path}\"], check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.zip_path, O.csv_path, O.etf, O.max_workers])\n",
    "# def zip(zip_path, csv_path, etf, max_workers):\n",
    "#     folder_infos = infer_ticker_dict(zip_path)\n",
    "\n",
    "#     # filter first\n",
    "#     if etf:\n",
    "#         def in_etf(folder_info):\n",
    "#             return folder_info.ticker in ETFMembers().mapping[etf] + [etf]\n",
    "#         folder_infos = list(filter(in_etf, folder_infos))\n",
    "\n",
    "#     # commands = [f\"mkdir -p {csv_path}/{folder_info.ticker_till_end}\\n\"\n",
    "#     #             for folder_info in folder_infos]\n",
    "\n",
    "#     # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "#     #     [f.write(command) for command in commands]\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # outputs_dirs = [folder_info.ticker_till_end for folder_info in folder_infos]\n",
    "#         futures = [\n",
    "#             executor.submit(os.mkdir, path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "#         wait(futures)\n",
    "#         futures = [\n",
    "#             executor.submit(extract_7z, input_path=folder_info.full, output_path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "\n",
    "\n",
    "#         # for folder_info in folder_infos:\n",
    "#         #     # print(folder_info.ticker)\n",
    "#         #     f.write(f\"examle: mkdir {csv_path}/{folder_info.ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "#     # ticker_date_dict = infer_ticker_to_ticker_path(zip_path)\n",
    "#     # print(ticker_date_dict)\n",
    "#     # if etf:\n",
    "#     #     print(ETFMembers().mapping[etf])\n",
    "#     #     ticker_date_dict = {\n",
    "#     #         ticker: ticker_path\n",
    "#     #         for ticker, ticker_path in ticker_date_dict.items()\n",
    "#     #         if ticker in ETFMembers().mapping[etf] + [etf]\n",
    "#     #     }\n",
    "#     # print(ticker_date_dict)\n",
    "#     # ticker_dict = infer_ticker_dict(zip_path)\n",
    "#     # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "#     #     for ticker, dict_ in ticker_dict.items():\n",
    "#     #         full = dict_[\"full\"]\n",
    "#     #         ticker_till_end = dict_[\"ticker_till_end\"]\n",
    "#     #         f.write(f\"mkdir {csv_path}/{ticker_till_end}\\n\")\n",
    "#     #         f.write(f\"/nfs/home/nicolasp/usr/bin/7z x {full} -o{ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.tickers, O.max_workers])\n",
    "# def dump(\n",
    "#     db_path,\n",
    "#     library,\n",
    "#     csv_path,\n",
    "#     tickers,\n",
    "#     max_workers,\n",
    "# ):\n",
    "#     \"\"\"Dump all csv to arctic_db inferring start and end date from folder.\"\"\"\n",
    "#     folder_infos = infer_ticker_dict(csv_path)\n",
    "#     print(\"inferred from folder\")\n",
    "#     print(folder_infos)\n",
    "\n",
    "#     if tickers:\n",
    "#         folder_infos = [folder_info for folder_info in folder_infos if folder_info.ticker in tickers]\n",
    "\n",
    "#     print(\"filtered folder_info after filtering for tickers.\")\n",
    "#     print(folder_infos)\n",
    "\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # small job with only a few dates\n",
    "#         # futures = [\n",
    "#         #     executor.submit(write_, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=\"2016-01-01\", end_date=\"2016-01-04\")\n",
    "#         #     for folder_info in folder_infos\n",
    "#         # ]\n",
    "#         # full job with whole year\n",
    "#         futures = [\n",
    "#             executor.submit(_write, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=folder_info.start_date, end_date=folder_info.end_date)\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "#     print('done')\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class ArcticDBInfo:\n",
    "#     ticker: str\n",
    "#     dates_ndarray: np.ndarray\n",
    "#     dates_series: pd.Series\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         self.dates_list: list[str] = list(self.dates_ndarray)\n",
    "#         self.start_date = min(self.dates_ndarray)\n",
    "#         self.end_date = max(self.dates_ndarray)\n",
    "\n",
    "# def _info(db_path, library) -> list[ArcticDBInfo]:\n",
    "#     \"\"\"Return information about ticker info in database.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     arcticdb_infos: list[ArcticDBInfo] = []\n",
    "#     for ticker in arctic[library].list_symbols():\n",
    "#         q = QueryBuilder()\n",
    "#         # there is one auction each morning\n",
    "#         q = q[q.event == Event.CROSS_TRADE.value]\n",
    "#         df = arctic[library].read(symbol=ticker, query_builder=q).data\n",
    "\n",
    "#         dates_series: pd.Series = df.index.date\n",
    "#         dates_ndarray: np.ndarray = df.index.to_series().dt.strftime(\"%Y-%m-%d\").values\n",
    "#         arcticdb_infos.append(\n",
    "#             ArcticDBInfo(ticker=ticker, dates_ndarray=dates_ndarray, dates_series=dates_series)\n",
    "#         )\n",
    "#     return arcticdb_infos\n",
    "\n",
    "\n",
    "# @arctic.group()\n",
    "# @inherits_from(_info)\n",
    "# def info(**kwargs):\n",
    "#     pass\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options(infer_options(_info))\n",
    "# def tickers(db_path, library):\n",
    "#     \"\"\"Print tickers in db.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_symbols())\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def versions(db_path, library):\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_versions())\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options(infer_options(_info))\n",
    "# def dates(**kwargs):\n",
    "#     \"\"\"Print ticker information\"\"\"\n",
    "#     arcticdb_infos = _info(**kwargs)\n",
    "#     print({x.ticker: (x.start_date, x.end_date) for x in arcticdb_infos})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "# # TODO: csv_path vs csv_files_path. think if this is a problem...maybe unify?\n",
    "# class Options:\n",
    "#     def __init__(self) -> None:\n",
    "#         self.db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\")\n",
    "#         self.library = click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"Library name\")\n",
    "#         self.ticker = click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to print\")\n",
    "#         self.start_date = click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "#         self.end_date = click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "#         self.csv_path = click.option(\"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\")\n",
    "#         self.etf = click.option(\"--etf\", default=None, help=\"restrict to subset specified by ETF members\")\n",
    "#         self.zip_path = click.option(\"-z\", \"--zip_path\", default=\"/nfs/lobster_data/lobster_raw/2016\", help=\"zip files path\")\n",
    "#         self.tickers = click.option(\"--tickers\", default=None, multiple=True, type=str, help=\"tickers to dump\")\n",
    "#         self.max_workers = click.option(\"-m\", \"--max_workers\", default=20, help=\"max workers for parallelisation\")\n",
    "# O = Options()\n",
    "\n",
    "# def apply_options(options: list):\n",
    "#     def decorator(f):\n",
    "#         for option in reversed(options):\n",
    "#             f = option(f)\n",
    "#         return f\n",
    "#     return decorator\n",
    "\n",
    "# def inherit_docstring_from(source_fn):\n",
    "#     def decorator(target_fn):\n",
    "#         target_fn.__doc__ = source_fn.__doc__\n",
    "#         return target_fn\n",
    "#     return decorator\n",
    "\n",
    "# def infer_options(func) -> list[Callable]:\n",
    "#     \"\"\"Works together with the `auto_apply` to automatically infer arguments.\n",
    "    \n",
    "#     Used together this looks like:\n",
    "#     @auto_apply(infer_options)\n",
    "#     \"\"\"\n",
    "#     sig = signature(func)\n",
    "#     param_names = [\n",
    "#         param.name\n",
    "#         for param in sig.parameters.values()\n",
    "#         if param.kind == param.POSITIONAL_OR_KEYWORD\n",
    "#     ]\n",
    "#     options_list = [getattr(O, name) for name in param_names]\n",
    "#     return options_list\n",
    "\n",
    "# def inherits_from(func):\n",
    "#     \"\"\"Inherit docstring and options from `func`. Actually this wasn't the best idea. Keep separate\"\"\"\n",
    "#     options_list = infer_options(func)\n",
    "\n",
    "#     # still stlightly confused about the order of the decorators, oh well\n",
    "#     def decorator(target_fn):\n",
    "#         @inherit_docstring_from(func)\n",
    "#         @apply_options(options_list)\n",
    "#         @wraps(target_fn)\n",
    "#         def wrapper(*args, **kwargs):\n",
    "#             return target_fn(*args, **kwargs)\n",
    "#         return wrapper\n",
    "#     return decorator\n",
    "\n",
    "# # def simple_inherits_from(func):\n",
    "# #     \"\"\"Simple without using functools.wraps\"\"\"\n",
    "# #     options_list = infer_options(func)\n",
    "# #     def decorator(target_fn):\n",
    "# #         decorated = apply_options(options_list)(target_fn)\n",
    "# #         decorated.__doc__ = func.__doc__\n",
    "# #         return decorated\n",
    "# #     return decorator\n",
    "\n",
    "# @click.group(context_settings=CONTEXT_SETTINGS)\n",
    "# def arctic():\n",
    "#     pass\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path])\n",
    "# def list_libraries(db_path) -> None:\n",
    "#     \"\"\"List arcticdb libraries\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic.list_libraries())\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def list_symbols(db_path, library) -> None:\n",
    "#     \"\"\"List symbols in the arcticdb library.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_symbols())\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def create_library(db_path, library) -> None:\n",
    "#     \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     arctic.create_library(library) \n",
    "#     print(arctic[library])\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# @click.confirmation_option(prompt='Are you sure you want to delete the entire library?')\n",
    "# def delete_library(db_path, library) -> None:\n",
    "#     \"\"\"Delete entire arcticdb library\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     arctic.delete_library(library) \n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.ticker, O.start_date, O.end_date])\n",
    "# def read(db_path, library, ticker, start_date, end_date,\n",
    "# ):\n",
    "#     \"\"\"Read ticker and print head and tail.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     if start_date and end_date:\n",
    "#         start_datetime = pd.Timestamp(f\"{start_date}T{NASDAQExchange.exchange_open}\")\n",
    "#         end_datetime = pd.Timestamp(f\"{end_date}T{NASDAQExchange.exchange_close}\")\n",
    "#         date_range = (start_datetime, end_datetime)\n",
    "#         df = arctic[library].read(ticker, date_range=date_range).data\n",
    "#     else:\n",
    "#         print(\"not using start or end dates\")\n",
    "#         df = arctic[library].read(ticker).data\n",
    "    \n",
    "#     print(f\"Printing df.head() and df.tail() for ticker {ticker}\")\n",
    "#     print(df.head())\n",
    "#     print(df.tail())\n",
    "\n",
    "# def _write(\n",
    "#     db_path,\n",
    "#     library,\n",
    "#     csv_path,\n",
    "#     ticker,\n",
    "#     start_date,\n",
    "#     end_date,\n",
    "# ):\n",
    "#     \"\"\"Preprocess and write ticker to database.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     date_range = (start_date, end_date)\n",
    "#     data = Data(\n",
    "#         directory_path=csv_path,\n",
    "#         ticker=ticker,\n",
    "#         date_range=date_range,\n",
    "#         aggregate_duplicates=False,\n",
    "#     )\n",
    "#     lobster = Lobster(data=data)\n",
    "#     df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "#     print(f\"head of ticker {ticker}\")\n",
    "#     print(df.head())\n",
    "\n",
    "#     arctic[library].write(symbol=ticker, data=df)\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.ticker, O.start_date, O.end_date])\n",
    "# def write(**kwargs):\n",
    "#     _write(**kwargs)\n",
    "\n",
    "\n",
    "# # if want to also access _say from other functions then need to do this.\n",
    "# def _say(\n",
    "#     db_path,\n",
    "#     library,\n",
    "# ):\n",
    "#     \"\"\"Print some really important information\"\"\"\n",
    "#     print(db_path, library)\n",
    "\n",
    "\n",
    "# # @arctic.command()\n",
    "# # @apply_options(infer_options(_say))\n",
    "# # @inherit_docstring_from(_say)\n",
    "# # def say(**kwargs):\n",
    "# #     _say(**kwargs)\n",
    "\n",
    "# @arctic.command()\n",
    "# @inherits_from(_say)\n",
    "# def say(**kwargs):\n",
    "#     _say(**kwargs)\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.start_date, O.end_date])\n",
    "# def generate_jobs(db_path, library, csv_path, start_date, end_date):\n",
    "#     ticker_date_dict = infer_ticker_to_date_range(csv_path)\n",
    "#     with open('arctic_commands.txt', 'w') as f:\n",
    "#         for ticker, (inferred_start_date, inferred_end_date) in ticker_date_dict.items():\n",
    "#             # if date is None use the inferred date, otherwise use the CLI argument\n",
    "#             start_date = start_date or inferred_start_date\n",
    "#             end_date = end_date or inferred_end_date\n",
    "#             f.write(f\"arctic write --csv_path={csv_path} --db_path={db_path} --library={library} --ticker={ticker} --start_date={start_date} --end_date={end_date} \\n\")\n",
    "\n",
    "# def sleepy(csv_path, folder_info):\n",
    "#     time.sleep(5)\n",
    "#     print(csv_path, folder_info.full)\n",
    "\n",
    "# def extract_7z(input_path, output_path):\n",
    "#     try:\n",
    "#         subprocess.run([\"7z\", \"x\", input_path, f\"-o{output_path}\"], check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.zip_path, O.csv_path, O.etf, O.max_workers])\n",
    "# def zip(zip_path, csv_path, etf, max_workers):\n",
    "#     folder_infos = infer_ticker_dict(zip_path)\n",
    "\n",
    "#     # filter first\n",
    "#     if etf:\n",
    "#         def in_etf(folder_info):\n",
    "#             return folder_info.ticker in ETFMembers().mapping[etf] + [etf]\n",
    "#         folder_infos = list(filter(in_etf, folder_infos))\n",
    "\n",
    "#     # commands = [f\"mkdir -p {csv_path}/{folder_info.ticker_till_end}\\n\"\n",
    "#     #             for folder_info in folder_infos]\n",
    "\n",
    "#     # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "#     #     [f.write(command) for command in commands]\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # outputs_dirs = [folder_info.ticker_till_end for folder_info in folder_infos]\n",
    "#         futures = [\n",
    "#             executor.submit(os.mkdir, path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "#         wait(futures)\n",
    "#         futures = [\n",
    "#             executor.submit(extract_7z, input_path=folder_info.full, output_path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "\n",
    "\n",
    "#         # for folder_info in folder_infos:\n",
    "#         #     # print(folder_info.ticker)\n",
    "#         #     f.write(f\"examle: mkdir {csv_path}/{folder_info.ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "#     # ticker_date_dict = infer_ticker_to_ticker_path(zip_path)\n",
    "#     # print(ticker_date_dict)\n",
    "#     # if etf:\n",
    "#     #     print(ETFMembers().mapping[etf])\n",
    "#     #     ticker_date_dict = {\n",
    "#     #         ticker: ticker_path\n",
    "#     #         for ticker, ticker_path in ticker_date_dict.items()\n",
    "#     #         if ticker in ETFMembers().mapping[etf] + [etf]\n",
    "#     #     }\n",
    "#     # print(ticker_date_dict)\n",
    "#     # ticker_dict = infer_ticker_dict(zip_path)\n",
    "#     # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "#     #     for ticker, dict_ in ticker_dict.items():\n",
    "#     #         full = dict_[\"full\"]\n",
    "#     #         ticker_till_end = dict_[\"ticker_till_end\"]\n",
    "#     #         f.write(f\"mkdir {csv_path}/{ticker_till_end}\\n\")\n",
    "#     #         f.write(f\"/nfs/home/nicolasp/usr/bin/7z x {full} -o{ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.tickers, O.max_workers])\n",
    "# def dump(\n",
    "#     db_path,\n",
    "#     library,\n",
    "#     csv_path,\n",
    "#     tickers,\n",
    "#     max_workers,\n",
    "# ):\n",
    "#     \"\"\"Dump all csv to arctic_db inferring start and end date from folder.\"\"\"\n",
    "#     folder_infos = infer_ticker_dict(csv_path)\n",
    "#     print(\"inferred from folder\")\n",
    "#     print(folder_infos)\n",
    "\n",
    "#     if tickers:\n",
    "#         folder_infos = [folder_info for folder_info in folder_infos if folder_info.ticker in tickers]\n",
    "\n",
    "#     print(\"filtered folder_info after filtering for tickers.\")\n",
    "#     print(folder_infos)\n",
    "\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # small job with only a few dates\n",
    "#         # futures = [\n",
    "#         #     executor.submit(write_, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=\"2016-01-01\", end_date=\"2016-01-04\")\n",
    "#         #     for folder_info in folder_infos\n",
    "#         # ]\n",
    "#         # full job with whole year\n",
    "#         futures = [\n",
    "#             executor.submit(_write, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=folder_info.start_date, end_date=folder_info.end_date)\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "#     print('done')\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class ArcticLibraryInfo:\n",
    "#     ticker: str\n",
    "#     dates_ndarray: np.ndarray\n",
    "#     dates_series: pd.Series\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         self.dates_list: list[str] = list(self.dates_ndarray)\n",
    "#         self.start_date = min(self.dates_ndarray)\n",
    "#         self.end_date = max(self.dates_ndarray)\n",
    "\n",
    "# def _info(db_path, library) -> list[ArcticLibraryInfo]:\n",
    "#     \"\"\"Return information about ticker info in database.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     arcticdb_infos: list[ArcticLibraryInfo] = []\n",
    "#     for ticker in arctic[library].list_symbols():\n",
    "#         q = QueryBuilder()\n",
    "#         # there is one auction each morning\n",
    "#         q = q[q.event == Event.CROSS_TRADE.value]\n",
    "#         df = arctic[library].read(symbol=ticker, query_builder=q).data\n",
    "\n",
    "#         dates_series: pd.Series = df.index.date\n",
    "#         dates_ndarray: np.ndarray = df.index.to_series().dt.strftime(\"%Y-%m-%d\").values\n",
    "#         arcticdb_infos.append(\n",
    "#             ArcticLibraryInfo(ticker=ticker, dates_ndarray=dates_ndarray, dates_series=dates_series)\n",
    "#         )\n",
    "#     return arcticdb_infos\n",
    "\n",
    "\n",
    "# @arctic.group()\n",
    "# @inherits_from(_info)\n",
    "# def info(**kwargs):\n",
    "#     pass\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options(infer_options(_info))\n",
    "# def tickers(db_path, library):\n",
    "#     \"\"\"Print tickers in db.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_symbols())\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def versions(db_path, library):\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_versions())\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options(infer_options(_info))\n",
    "# def dates(**kwargs):\n",
    "#     \"\"\"Print ticker information\"\"\"\n",
    "#     arcticdb_infos = _info(**kwargs)\n",
    "#     print({x.ticker: (x.start_date, x.end_date) for x in arcticdb_infos})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor so that arctic gets Arctci() and passes that down to subcommands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ArcticLibraryInfo:\n",
    "    ticker: str\n",
    "    dates_ndarray: np.ndarray\n",
    "    dates_series: pd.Series\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.dates_list: list[str] = list(self.dates_ndarray)\n",
    "        self.start_date = min(self.dates_ndarray)\n",
    "        self.end_date = max(self.dates_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "CONTEXT_SETTINGS = dict(\n",
    "    help_option_names=[\"-h\", \"--help\"],\n",
    "    token_normalize_func=lambda x: x.lower() if isinstance(x, str) else x,\n",
    "    show_default=True,\n",
    "    auto_envvar_prefix=\"ARCTIC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def demo(**kwargs):\n",
    "    click.echo(kwargs)\n",
    "    print(kwargs, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _single_write_within_multi_write(arctic_library, data):\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "    print(df.head())\n",
    "\n",
    "    arctic_library.write(symbol=data.ticker, data=df)\n",
    "    print(f\"finished writing ticker {data.ticker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Library' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb Cell 24\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# | export\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# REFACTORINOOOOO\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_library_info\u001b[39m(\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     arctic_library: Library,  \u001b[39m# arcticdb library\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     tickers: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,  \u001b[39m# tickers to filter on\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[ArcticLibraryInfo]:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return information about ticker info in database.\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     arctic_symbols \u001b[39m=\u001b[39m arctic_library\u001b[39m.\u001b[39mlist_symbols()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Library' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "# REFACTORINOOOOO\n",
    "\n",
    "\n",
    "def get_library_info(\n",
    "    arctic_library: Library,  # arcticdb library\n",
    "    tickers: list[str] | None = None,  # tickers to filter on\n",
    ") -> list[ArcticLibraryInfo]:\n",
    "    \"\"\"Return information about ticker info in database.\"\"\"\n",
    "\n",
    "    arctic_symbols = arctic_library.list_symbols()\n",
    "    if tickers:\n",
    "        if not set(tickers).issubset(set(arctic_symbols)):\n",
    "            raise ValueError(\n",
    "                f\"Some of the tickers specified were not in the databasee. The invalid tickers were {set(tickers) - set(arctic_symbols)}\"\n",
    "            )\n",
    "    else:\n",
    "        tickers = arctic_symbols\n",
    "\n",
    "    arctic_library_infos: list[ArcticLibraryInfo] = []\n",
    "    for ticker in tickers:\n",
    "        q = QueryBuilder()\n",
    "        # there is one auction each morning\n",
    "        q = q[q.event == Event.CROSS_TRADE.value]\n",
    "        df = arctic_library.read(symbol=ticker, query_builder=q).data\n",
    "\n",
    "        dates_series: pd.Series = df.index.date\n",
    "        dates_ndarray: np.ndarray = df.index.to_series().dt.strftime(\"%Y-%m-%d\").values\n",
    "        arctic_library_infos.append(\n",
    "            ArcticLibraryInfo(\n",
    "                ticker=ticker, dates_ndarray=dates_ndarray, dates_series=dates_series\n",
    "            )\n",
    "        )\n",
    "    return arctic_library_infos\n",
    "\n",
    "\n",
    "class Options:\n",
    "    def __init__(self) -> None:\n",
    "        self.db_path = click.option(\n",
    "            \"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\"\n",
    "        )\n",
    "        self.library = click.option(\n",
    "            \"-l\", \"--library\", default=cfg.db.library, help=\"Library name\"\n",
    "        )\n",
    "        self.ticker = click.option(\n",
    "            \"-t\", \"--ticker\", required=True, help=\"ticker to print\"\n",
    "        )\n",
    "        self.start_date = click.option(\n",
    "            \"-s\", \"--start_date\", default=None, help=\"start date\"\n",
    "        )\n",
    "        self.end_date = click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "        self.csv_path = click.option(\n",
    "            \"-c\",\n",
    "            \"--csv_path\",\n",
    "            default=cfg.data_config.csv_files_path,\n",
    "            help=\"csv files path\",\n",
    "        )\n",
    "        self.etf = click.option(\n",
    "            \"--etf\", default=None, help=\"restrict to subset specified by ETF members\"\n",
    "        )\n",
    "        self.zip_path = click.option(\n",
    "            \"-z\",\n",
    "            \"--zip_path\",\n",
    "            default=\"/nfs/lobster_data/lobster_raw/2016\",\n",
    "            help=\"zip files path\",\n",
    "        )\n",
    "        self.tickers = click.option(\n",
    "            \"--tickers\", default=None, multiple=True, type=str, help=\"tickers to dump\"\n",
    "        )\n",
    "        self.max_workers = click.option(\n",
    "            \"-m\", \"--max_workers\", default=20, help=\"max workers for parallelisation\"\n",
    "        )\n",
    "\n",
    "\n",
    "O = Options()\n",
    "\n",
    "\n",
    "class ConsoleNotify:\n",
    "    def warn(self):\n",
    "        click.secho(\"WARNING:\", fg=\"red\", bold=True, underline=True)\n",
    "\n",
    "    def info(self):\n",
    "        click.secho(\"INFO:\", fg=\"yellow\", bold=True, underline=True)\n",
    "\n",
    "    def sucess(self):\n",
    "        click.secho(\"SUCESS\", fg=\"green\", bold=True, underline=True)\n",
    "\n",
    "C = ConsoleNotify()\n",
    "\n",
    "\n",
    "def apply_options(options: list):\n",
    "    def decorator(f):\n",
    "        for option in reversed(options):\n",
    "            f = option(f)\n",
    "        return f\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class ClickCtxObj(TypedDict):\n",
    "    \"\"\"Purely for type hinting. for instance `arctic_library` not always there.\"\"\"\n",
    "\n",
    "    library: str\n",
    "    db_path: str\n",
    "    arctic: Arctic\n",
    "    arctic_library: NotRequired[Library]\n",
    "\n",
    "\n",
    "class ClickCtx(Protocol):\n",
    "    obj: ClickCtxObj\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.argument(\"etf\")\n",
    "@click.option(\n",
    "    \"-s\", \"--sep\", default=\"\\n\", help=\"separator\"\n",
    ")\n",
    "def etf(etf, sep):\n",
    "    \"Output constituents of ETF including the ETF itself\"\n",
    "    click.echo(sep.join([etf] + etf_to_equities[etf]))\n",
    "\n",
    "@click.command()\n",
    "def pfmt():\n",
    "    \"If you don't want to install jq you can use this instead.\"\n",
    "    # NOTE: pformat clashed with pprint.pformat\n",
    "    for line in sys.stdin:\n",
    "        obj = json.loads(line.strip())\n",
    "        click.echo(pformat(obj))\n",
    "\n",
    "@click.group(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-d\", \"--db_path\", default=cfg.db.db_path, envvar=\"DB_PATH\", help=\"Database path\"\n",
    ")\n",
    "@click.option(\n",
    "    \"-l\", \"--library\", default=cfg.db.library, envvar=\"LIBRARY\", help=\"Library name\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--s3\", is_flag=True, default=True, help=\"Use s3 bucket. Temp fix.\"\n",
    ")\n",
    "@click.pass_context\n",
    "def arctic(ctx, db_path, library, s3):\n",
    "    ctx.ensure_object(dict)\n",
    "    if s3:\n",
    "        arctic = Arctic('s3://163.1.179.45:9100:lobster?access=minioadmin&secret=minioadmin')\n",
    "    else:\n",
    "        arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "    ctx.obj.update(\n",
    "        {\n",
    "            \"arctic\": arctic,\n",
    "            \"library\": library,\n",
    "            \"db_path\": db_path,\n",
    "        }\n",
    "    )\n",
    "    try:\n",
    "        ctx.obj[\"arctic_library\"] = arctic[library]\n",
    "    except LibraryNotFound:\n",
    "        pass\n",
    "\n",
    "\n",
    "@arctic.command()\n",
    "@click.pass_context\n",
    "def echo(ctx: ClickCtx) -> None:\n",
    "    \"\"\"Echo back inputs\"\"\"\n",
    "    click.echo(pformat(ctx.obj))\n",
    "\n",
    "\n",
    "# not a good idea ! monkey patch click.echo to accept a color\n",
    "# def new_echo(text, file=None, nl=True, err=False, color=None):\n",
    "#     if color:\n",
    "#         text = click.style(text, fg=color)\n",
    "#     click.echo.original(text, file=file, nl=nl, err=err)\n",
    "\n",
    "# def warn(msg):\n",
    "#     return click.style(msg, fg=\"red\", bold=True, blink=True)\n",
    "\n",
    "\n",
    "@arctic.command()\n",
    "def init():\n",
    "    \"\"\"Initialise autocomplete for arctic CLI\"\"\"\n",
    "    # TODO: improve performance of CLI\n",
    "    os.system(\"_ARCTIC_COMPLETE=bash_source arctic > ~/.arctic-complete.bash\")\n",
    "\n",
    "    with open(os.path.expanduser(\"~/.bashrc\"), \"a\") as f:\n",
    "        f.write(\n",
    "            textwrap.dedent(\n",
    "                \"\"\"\\\n",
    "                # >>> arctic init_autocomplete >>>\n",
    "                # Contents within this block were generated by arctic init_autocomplete\n",
    "                . ~/.arctic-complete.bash\n",
    "                # <<< arctic init_autocomplete <<<\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    click.echo(\n",
    "        \"Autocomplete initialized. Please restart your shell or run `source ~/.bashrc`.\"\n",
    "    )\n",
    "    click.echo(\n",
    "        \"Autocomplete initialized. Please restart your shell or run `source ~/.bashrc`.\"\n",
    "    )\n",
    "\n",
    "\n",
    "@arctic.command()\n",
    "@click.pass_context\n",
    "def create(ctx: ClickCtx) -> None:\n",
    "    \"\"\"Create a blank library\"\"\"\n",
    "    arctic = ctx.obj[\"arctic\"]\n",
    "    library = ctx.obj[\"library\"]\n",
    "    arctic.create_library(library)\n",
    "    click.echo(arctic[library])\n",
    "\n",
    "\n",
    "@arctic.group()\n",
    "@click.pass_context\n",
    "def ls(ctx: ClickCtx):\n",
    "    \"List information about a library\"\n",
    "    # NOTE: Using word list clashed with python type hints!\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@ls.command()\n",
    "@click.pass_context\n",
    "def libraries(ctx: ClickCtx):\n",
    "    arctic = ctx.obj[\"arctic\"]\n",
    "    click.echo(arctic.list_libraries())\n",
    "\n",
    "\n",
    "@ls.command()\n",
    "@click.pass_context\n",
    "def symbols(ctx: ClickCtx):\n",
    "    arctic_library = ctx.obj[\"arctic_library\"]\n",
    "    click.echo(arctic_library.list_symbols())\n",
    "\n",
    "\n",
    "@ls.command()\n",
    "@click.pass_context\n",
    "def versions(ctx: ClickCtx):\n",
    "    arctic_library = ctx.obj[\"arctic_library\"]\n",
    "\n",
    "    click.echo(\n",
    "        (\n",
    "            pd.DataFrame(arctic_library.list_versions())\n",
    "            .transpose()\n",
    "            .drop(columns=[1, 2])\n",
    "            .rename(columns={0: \"created_on\"})\n",
    "            .assign(\n",
    "                created_on=lambda df: df[\"created_on\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "            .rename_axis([\"ticker\", \"version\"])\n",
    "            .sort_index(level=[0, 1], ascending=[True, False])\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_comma_separated(ctx, param, value: str):\n",
    "    \"\"\"Convert a comma (or space) separated option to a list of options\"\"\"\n",
    "    if value is not None:\n",
    "        delimiters = r\"[ ,]\"\n",
    "        option_list = re.split(delimiters, value)\n",
    "        option_list = list(filter(None, option_list))\n",
    "        return option_list\n",
    "\n",
    "\n",
    "@ls.command()\n",
    "# @click.option(\"-t\", \"--tickers\", callback=parse_comma_separated , help=\"Comma or space separated tickers\")\n",
    "@click.option(\n",
    "    \"-t\", \"--tickers\", multiple=True, type=str, help=\"Provide ticker(s) to filter on\"\n",
    ")\n",
    "@click.option(\n",
    "    \"-a\",\n",
    "    \"--all\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    help=\"print all dates not just start and end\",\n",
    ")\n",
    "@click.pass_context\n",
    "def dates(ctx: ClickCtx, tickers, all):\n",
    "    arctic_library = ctx.obj[\"arctic_library\"]\n",
    "\n",
    "    arctic_library_infos = get_library_info(arctic_library, tickers=tickers)\n",
    "\n",
    "    if all:\n",
    "        click.echo(pformat({x.ticker: x.dates_list for x in arctic_library_infos}))\n",
    "    else:\n",
    "        click.echo(\n",
    "            pformat(\n",
    "                {x.ticker: (x.start_date, x.end_date) for x in arctic_library_infos}\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "@arctic.group()\n",
    "@click.pass_context\n",
    "def rm(ctx: ClickCtx):\n",
    "    \"Remove commands\"\n",
    "    # NOTE: Using word del clashed with python!\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: make library an argument here rather than in arctic\n",
    "@rm.command()\n",
    "@click.pass_context\n",
    "def library(ctx: ClickCtx):\n",
    "    arctic = ctx.obj[\"arctic\"]\n",
    "    library = ctx.obj[\"library\"]\n",
    "\n",
    "    if not arctic.has_library(library):\n",
    "        click.echo(\"No library found to delete.\")\n",
    "    else:\n",
    "        arctic_library = arctic[library]\n",
    "        C.info()\n",
    "        click.echo(\n",
    "            textwrap.dedent(\n",
    "                f\"\"\"\\\n",
    "                Library information:\n",
    "                {arctic_library}\n",
    "\n",
    "                Tickers in this library:\n",
    "                {arctic_library.list_symbols()}\"\"\"))\n",
    "        C.warn()\n",
    "\n",
    "        confirmation = click.prompt(f\"Type {library} to confirm the permanent deletion of the library\")        \n",
    "        if confirmation == library:\n",
    "            del ctx.obj[\"arctic_library\"]\n",
    "            del arctic_library\n",
    "            arctic.delete_library(library)\n",
    "            C.sucess()\n",
    "        else:\n",
    "            raise click.Abort()\n",
    "\n",
    "\n",
    "\n",
    "@arctic.command()\n",
    "@click.argument(\"query_template\")\n",
    "def query(query_template: str):\n",
    "    \"Write a custom query using a string template.\"\n",
    "    for line in sys.stdin:\n",
    "        obj = json.loads(line.strip())\n",
    "        query = Template(query_template).substitute(obj)\n",
    "        click.echo(query)\n",
    "\n",
    "\n",
    "\n",
    "@arctic.command()\n",
    "@click.argument(\"tickers\", nargs=-1)\n",
    "def filter(tickers: tuple):\n",
    "    \"Filter by ticker. Reads from stdin and writes to stdout.\"\n",
    "    for line in sys.stdin:\n",
    "        obj = json.loads(line.strip())\n",
    "        if obj['ticker'] in tickers:\n",
    "            click.echo(json.dumps(obj))\n",
    "\n",
    "# @arctic.command()\n",
    "# @click.argument(\"query_template\")\n",
    "# def prep(query_template: str):\n",
    "#     \"Write a custom query\"\n",
    "#     for line in sys.stdin:\n",
    "#         obj = json.loads(line.strip())\n",
    "#         query = Template(query_template).substitute(obj)\n",
    "#         click.echo(query)\n",
    "\n",
    "@arctic.command()\n",
    "@click.option(\n",
    "    \"-f\",\n",
    "    \"--files_path\",\n",
    "    default=cfg.data_config.csv_files_path,\n",
    "    help=\"files path\",\n",
    ")\n",
    "def finfo(files_path):\n",
    "    \"Output \"\n",
    "    l = infer_ticker_dict(files_path)\n",
    "    l = [asdict(x) for x in l]\n",
    "    l = [json.dumps(x) for x in l]\n",
    "    click.echo(\"\\n\".join(l))\n",
    "\n",
    "@arctic.command()\n",
    "@click.pass_context\n",
    "@click.option(\n",
    "    \"-s\", \"--start_date\", envvar=\"ARCTIC_START_DATE\", help=\"start date\"\n",
    ")\n",
    "@click.option(\"-e\", \"--end_date\", envvar=\"END_DATE\", help=\"end date\")\n",
    "@click.option(\n",
    "    \"-c\",\n",
    "    \"--csv_path\",\n",
    "    default=cfg.data_config.csv_files_path,\n",
    "    envvar=\"CSV_PATH\",\n",
    "    help=\"csv files path\",\n",
    ")\n",
    "@click.option(\n",
    "    \"-z\",\n",
    "    \"--zip_path\",\n",
    "    default=\"/nfs/lobster_data/lobster_raw/2016\",\n",
    "    envvar=\"ZIP_PATH\",\n",
    "    help=\"zip files path\",\n",
    ")\n",
    "def attach(ctx, start_date, end_date, csv_path, zip_path):\n",
    "    \"Add extra metadata to JSON read from stdin.\"\n",
    "    for line in sys.stdin:\n",
    "        obj = json.loads(line.strip())\n",
    "        \n",
    "        obj[\"library\"] = ctx.obj[\"library\"]\n",
    "        obj[\"db_path\"] = ctx.obj[\"db_path\"]\n",
    "        obj['csv_path'] = csv_path\n",
    "        obj['zip_path'] = zip_path\n",
    "\n",
    "        if start_date:\n",
    "            obj['start_date'] = start_date\n",
    "        if end_date: \n",
    "            obj['end_date'] = end_date\n",
    "\n",
    "        click.echo(json.dumps(obj))\n",
    "\n",
    "\n",
    "@arctic.command()\n",
    "@click.pass_context\n",
    "@click.option(\n",
    "    \"-c\",\n",
    "    \"--csv_path\",\n",
    "    default=cfg.data_config.csv_files_path,\n",
    "    help=\"csv files path\",\n",
    ")\n",
    "@click.option(\n",
    "    \"--ticker\",\n",
    "    required=True,\n",
    ")\n",
    "@click.option(\n",
    "    \"--start_date\",\n",
    ")\n",
    "@click.option(\n",
    "    \"--end_date\",\n",
    ")\n",
    "def single_write(\n",
    "    ctx,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    \"\"\"Single thread write ticker to database. Useful in conjunction with arctic query.\"\"\"\n",
    "    # i guess this interface is maybe not structured the best \n",
    "    # some stuff from arctic ctx.obj some stuff for this method.\n",
    "    try:\n",
    "        arctic_library = ctx.obj[\"arctic_library\"]\n",
    "    except KeyError:\n",
    "        raise LibraryNotFound\n",
    "    # TODO: as of now only valid if both start and end are provided\n",
    "    if bool(start_date) ^ bool(end_date):\n",
    "        raise NotImplementedError \n",
    "    date_range = (start_date, end_date) if start_date else None\n",
    "\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "    C.info()\n",
    "    print(f\"head of ticker {ticker}\")\n",
    "    print(df.head())\n",
    "\n",
    "    arctic_library.write(symbol=ticker, data=df)\n",
    "\n",
    "    C.sucess()\n",
    "\n",
    "@arctic.command()\n",
    "@click.pass_context\n",
    "@click.option(\n",
    "    \"-c\",\n",
    "    \"--csv_path\",\n",
    "    default=cfg.data_config.csv_files_path,\n",
    "    help=\"csv files path\",\n",
    ")\n",
    "@click.option(\n",
    "    \"--ticker\",\n",
    "    required=True,\n",
    ")\n",
    "@click.option(\n",
    "    \"--date_range\", nargs=2, type=str\n",
    ")\n",
    "@click.option(\n",
    "    \"--update\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    help=\"use update instead of write\"\n",
    ")\n",
    "def mp_single_write(\n",
    "    ctx,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    date_range,\n",
    "    update,\n",
    "):\n",
    "    \"\"\"Single thread write ticker to database. Useful in conjunction with arctic query.\"\"\"\n",
    "    # i guess this interface is maybe not structured the best \n",
    "    # some stuff from arctic ctx.obj some stuff for this method.\n",
    "    # os.nice(0)\n",
    "    try:\n",
    "        arctic_library = ctx.obj[\"arctic_library\"]\n",
    "    except KeyError:\n",
    "        raise LibraryNotFound\n",
    "\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        load='both',\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    click.echo(data)\n",
    "    lobster = MPLobster(data=data)\n",
    "\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "    C.info()\n",
    "    print(f\"head of ticker {ticker}\")\n",
    "    print(df.head())\n",
    "\n",
    "    if update:\n",
    "        # for batched writes for large tickers like SPY\n",
    "        arctic_library.update(symbol=ticker, data=df)\n",
    "    else: \n",
    "        arctic_library.write(symbol=ticker, data=df)\n",
    "    C.sucess()\n",
    "\n",
    "@arctic.command()\n",
    "@click.pass_context\n",
    "def multi_write(ctx):\n",
    "    try:\n",
    "        arctic_library = ctx.obj[\"arctic_library\"]\n",
    "    except KeyError:\n",
    "        raise LibraryNotFound\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "        futures = []\n",
    "        for line in sys.stdin:\n",
    "            obj = json.loads(line.strip())\n",
    "            \n",
    "            csv_path = obj.get(\"csv_path\")\n",
    "            # db_path = obj.get(\"db_path\")\n",
    "            ticker = obj.get(\"ticker\")\n",
    "            start_date = obj.get(\"start_date\")\n",
    "            end_date = obj.get(\"end_date\")\n",
    "            \n",
    "            if bool(start_date) ^ bool(end_date):\n",
    "                raise NotImplementedError \n",
    "            date_range = (start_date, end_date) if start_date else None\n",
    "\n",
    "            data = Data(\n",
    "                directory_path=csv_path,\n",
    "                ticker=ticker,\n",
    "                date_range=date_range,\n",
    "                aggregate_duplicates=False,\n",
    "            )\n",
    "            \n",
    "            # executor.submit(_single_write_within_multi_write, data)\n",
    "            # future = executor.submit(demo, csv_path=csv_path, db_path=db_path, ticker=ticker, start_date=start_date, end_date=end_date)\n",
    "            future = executor.submit(_single_write_within_multi_write, arctic_library=arctic_library, data=data)\n",
    "            futures.append(future)\n",
    "\n",
    "\n",
    "            for f in as_completed(futures):\n",
    "                print(f.result())\n",
    "\n",
    "# rubbish:\n",
    "# @arctic.command()\n",
    "# @click.pass_context\n",
    "# def fd_multi_write(\n",
    "#     ctx,\n",
    "# ):\n",
    "#     \"\"\"BAKCWARDS>>>Multithreaded write database. Reads JSON object (one per line) from stdin and writes straight to database.\"\"\"\n",
    "#     # i guess this implementation is slightly more straightforward\n",
    "#     # but still i'm using the library from context, and not the JSON object..\n",
    "#     # actually quite hard to find something natural\n",
    "#     # if env vars are set then the usage is straightforward. otherwise quite counterintuitive\n",
    "#     try:\n",
    "#         arctic_library = ctx.obj[\"arctic_library\"]\n",
    "#     except KeyError:\n",
    "#         raise LibraryNotFound\n",
    "    \n",
    "#     with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "#     for line in sys.stdin:\n",
    "#         obj = json.loads(line.strip())\n",
    "#         csv_path = obj[\"csv_path\"]\n",
    "#         ticker = obj[\"ticker\"]\n",
    "#         start_date = obj[\"start_date\"]\n",
    "#         end_date = obj[\"end_date\"]\n",
    "\n",
    "#         # TODO: as of now only valid if both start and end are provided\n",
    "#         if bool(start_date) ^ bool(end_date):\n",
    "#             raise NotImplementedError \n",
    "#         date_range = (start_date, end_date) if start_date else None\n",
    "\n",
    "#         data = Data(\n",
    "#             directory_path=csv_path,\n",
    "#             ticker=ticker,\n",
    "#             date_range=date_range,\n",
    "#             aggregate_duplicates=False,\n",
    "#         )\n",
    "        \n",
    "\n",
    "#         # parallelise only the computationally demanding part from here\n",
    "#             executor.submit(_single_write_within_multi_write, arctic_library=arctic_library, data=data)\n",
    "#             # executor.submit(demo, csv_path=csv_path, db_path=db_path, library=library, ticker=ticker, start_date=start_date, end_date=end_date)\n",
    "#                 # actual job\n",
    "#                 # executor.submit(_single_write_within_multi_write, csv_path=csv_path, db_path=db_path, library=library, ticker=ticker, start_date=start_date, end_date=end_date)\n",
    "\n",
    "#                 # futures = [\n",
    "#                 #     executor.submit(_write, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=folder_info.start_date, end_date=folder_info.end_date)\n",
    "#                 #     for folder_info in folder_infos\n",
    "#                 # ]\n",
    "#         C.sucess()\n",
    "\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.ticker, O.start_date, O.end_date])\n",
    "# def write(**kwargs):\n",
    "#     _write(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #bizzare stuff\n",
    "\n",
    "# @list.command()\n",
    "# @click.pass_context\n",
    "# def versions(ctx: ClickCtx):\n",
    "#     arctic = ctx.obj[\"arctic\"]\n",
    "#     library = ctx.obj[\"library\"]\n",
    "#     print(arctic[library].list_versions())\n",
    "\n",
    "#     # grouped information\n",
    "#     print(\"grouped info \\n\")\n",
    "#     # data_ = arctic[library].list_versions()\n",
    "#     # print(data_)\n",
    "#     # absolutely bizzare behaviour of click\n",
    "#     # logger.info(list(data_.items()))\n",
    "#     # print(list(data_.items()), flush=True)\n",
    "#     df = pd.DataFrame(arctic[library].list_versions())\n",
    "#     print(df)\n",
    "#     # df = pd.DataFrame(list(data.items()), columns=[\"Key\", \"Date\"])\n",
    "#     # print(df)\n",
    "#     # print(df)\n",
    "#     # print(df)\n",
    "#     # df['Ticker'] = df['Key'].str.split('_').str[0]\n",
    "#     # df['Version'] = df['Key'].str.split('_').str[1]\n",
    "\n",
    "#     # grouped = df.groupby('Ticker').apply(lambda x: x[['Version', 'Date']].to_dict(orient='records')).to_dict()\n",
    "#     # print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "db_path = cfg.db.db_path\n",
    "library = cfg.db.library\n",
    "arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "arctic_library = arctic[library]\n",
    "print(type(arctic_library))\n",
    "\n",
    "try:\n",
    "    arctic[\"not_there_library\"]\n",
    "except LibraryNotFound:\n",
    "    print('yooo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "db_path = cfg.db.db_path\n",
    "library = cfg.db.library\n",
    "arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "v = arctic[library].list_versions()\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "df = (\n",
    "    pd.DataFrame(v)\n",
    "    .transpose()\n",
    "    .drop(columns=[1, 2])\n",
    "    .rename(columns={0: \"datetime\"})\n",
    "    .rename_axis(['ticker','version'])\n",
    "    .sort_index(level=[0,1], ascending=[True, False])\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# cfg = get_config()\n",
    "db_path = cfg.db.db_path\n",
    "library = cfg.db.library\n",
    "tickers = ['PXD', 'WMB', 'MRO', 'COP', 'SLB', 'KMI', 'XOM', 'DVN', 'PSX', 'OKE', 'FANG', 'CVX', 'HAL']\n",
    "ticker = \"PXD\"\n",
    "arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "q = QueryBuilder()\n",
    "q = q[q.event == Event.CROSS_TRADE.value]\n",
    "df = arctic[library].read(symbol=ticker, query_builder=q).data\n",
    "dates = df.index.date\n",
    "dates_str = df.index.to_series().dt.strftime(\"%Y-%m-%d\").values\n",
    "print(dates_str)\n",
    "\n",
    "\n",
    "# print({ticker:arctic[library].read(ticker).version for ticker in tickers})\n",
    "# df = arctic[library].read(ticker).data\n",
    "# display(df)\n",
    "# display(df.head())\n",
    "# display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_list_symbols(db_path, library) -> None:\n",
    "    \"\"\"List symbols in the arcticdb library.\"\"\"\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "    print(f\"Symbols in library {library}\")\n",
    "    print(arctic_library.list_symbols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_create_new_library(db_path, library) -> None:\n",
    "    \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic.create_library(library) \n",
    "    print(arctic[library])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "def arctic_list_libraries(db_path) -> None:\n",
    "    \"\"\"List arcticdb libraries\"\"\"\n",
    "\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    print(arctic.list_libraries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_delete_library(db_path, library) -> None:\n",
    "    \"\"\"Delete arcticdb library\"\"\"\n",
    "\n",
    "    user_input = input(\"Proceed by deleting this entire library? (y/n): \")\n",
    "    user_input = user_input.lower()\n",
    "    match user_input:\n",
    "        case \"y\":\n",
    "            pass\n",
    "        case \"n\":\n",
    "            sys.exit(0)\n",
    "        case _:\n",
    "            sys.exit(1)\n",
    "\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic.delete_library(library) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to print\")\n",
    "@click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "def arctic_read_symbol(db_path, library, ticker, start_date, end_date,\n",
    "):\n",
    "    \"\"\"Print df.head() and available columns for ticker in arcticdb library.\"\"\"\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    if start_date and end_date:\n",
    "        start_datetime = pd.Timestamp(f\"{start_date}T{NASDAQExchange.exchange_open}\")\n",
    "        end_datetime = pd.Timestamp(f\"{end_date}T{NASDAQExchange.exchange_close}\")\n",
    "        date_range = (start_datetime, end_datetime)\n",
    "        df = arctic_library.read(ticker, date_range=date_range).data\n",
    "    else:\n",
    "        df = arctic_library.read(ticker).data\n",
    "    \n",
    "    print(f\"Printing df.head() and df.tail() for ticker {ticker}\")\n",
    "    print(df.head())\n",
    "    print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to arctic again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to write to db\")\n",
    "@click.option(\"-s\", \"--start_date\", default=\"2020-01-01\", help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=\"2020-02-01\", help=\"end date\")\n",
    "def arctic_write_symbol(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    # if ticker in arctic_library.list_symbols():\n",
    "    #     print(\"warning - there is already data for ths ticker\")\n",
    "    #     user_input = input(\"Proceed by adding data to this symbol? (y/n): \")\n",
    "    #     user_input = user_input.lower()\n",
    "    #     match user_input:\n",
    "    #         case \"y\":\n",
    "    #             pass\n",
    "    #         case \"n\":\n",
    "    #             sys.exit(0)\n",
    "    #         case _:\n",
    "    #             sys.exit(1)\n",
    "\n",
    "    date_range = (start_date, end_date)\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "\n",
    "    arctic_library.write(symbol=ticker, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "def arctic_generate_jobs(csv_path, db_path, library, start_date, end_date):\n",
    "    ticker_date_dict = infer_ticker_to_date_range(csv_path)\n",
    "    with open('arctic_commands.txt', 'w') as f:\n",
    "        for ticker, (inferred_start_date, inferred_end_date) in ticker_date_dict.items():\n",
    "            # if date is None use the inferred date, otherwise use the CLI argument\n",
    "            start_date = start_date or inferred_start_date\n",
    "            end_date = end_date or inferred_end_date\n",
    "            f.write(f\"arctic_write_symbol --csv_path={csv_path} --db_path={db_path} --library={library} --ticker={ticker} --start_date={start_date} --end_date={end_date} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-z\",\n",
    "    \"--zip_path\",\n",
    "    default=\"/nfs/lobster_data/lobster_raw/2016\",\n",
    "    help=\"zip files path\",\n",
    ")\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\n",
    "    \"-e\", \"--etf\", default=None, help=\"restrict to subset specified by ETF members\"\n",
    ")\n",
    "def zip_generate_jobs(zip_path, csv_path, etf):\n",
    "    # ticker_date_dict = infer_ticker_to_ticker_path(zip_path)\n",
    "    # print(ticker_date_dict)\n",
    "    # if etf:\n",
    "    #     print(ETFMembers().mapping[etf])\n",
    "    #     ticker_date_dict = {\n",
    "    #         ticker: ticker_path\n",
    "    #         for ticker, ticker_path in ticker_date_dict.items()\n",
    "    #         if ticker in ETFMembers().mapping[etf] + [etf]\n",
    "    #     }\n",
    "    # print(ticker_date_dict)\n",
    "    ticker_dict = infer_ticker_dict(zip_path)\n",
    "    with open(\"zip_commands.txt\", \"w\") as f:\n",
    "        for ticker, dict_ in ticker_dict.items():\n",
    "            full = dict_[\"full\"]\n",
    "            ticker_till_end = dict_[\"ticker_till_end\"]\n",
    "            f.write(f\"mkdir {csv_path}/{ticker_till_end}\\n\")\n",
    "            f.write(f\"/nfs/home/nicolasp/usr/bin/7z x {full} -o{ticker_till_end}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to write to db\")\n",
    "@click.option(\"-s\", \"--start_date\", default=\"2020-01-01\", help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=\"2020-02-01\", help=\"end date\")\n",
    "def arctic_dump_all(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    if ticker in arctic_library.list_symbols():\n",
    "        print(\"warning - there is already data for ths ticker\")\n",
    "        user_input = input(\"Proceed by adding data to this symbol? (y/n): \")\n",
    "        user_input = user_input.lower()\n",
    "        match user_input:\n",
    "            case \"y\":\n",
    "                pass\n",
    "            case \"n\":\n",
    "                sys.exit(0)\n",
    "            case _:\n",
    "                sys.exit(1)\n",
    "\n",
    "    date_range = (start_date, end_date)\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "    print(df)\n",
    "\n",
    "    arctic_library.append(symbol=ticker, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
