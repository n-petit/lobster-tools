{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arctic\n",
    "\n",
    "> Arctic helper scripts and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp arctic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import click\n",
    "from click.testing import CliRunner\n",
    "from arcticdb import Arctic, QueryBuilder\n",
    "from arcticdb.version_store.library import Library\n",
    "from arcticdb.exceptions import LibraryNotFound\n",
    "import hydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "import textwrap\n",
    "from lobster_tools.config import (\n",
    "    MainConfig,\n",
    "    Overrides,\n",
    "    NASDAQExchange,\n",
    "    ETFMembers,\n",
    "    register_configs,\n",
    "    get_config,\n",
    ")\n",
    "from lobster_tools.preprocessing import Data, Lobster, Event, infer_ticker_to_date_range, infer_ticker_to_ticker_path, infer_ticker_dict\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from logging import Logger\n",
    "from datetime import date\n",
    "from typing import Callable, TypedDict, Protocol, cast\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from inspect import signature\n",
    "from functools import wraps\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, wait\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `@hydra.main` decorator, `register_configs` must be called. If simply using a notebook or writing the CLIs with `click`, it is enough to use `get_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "register_configs()\n",
    "cfg = get_config(overrides=Overrides.full_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "CONTEXT_SETTINGS = dict(\n",
    "    help_option_names=[\"-h\", \"--help\"],\n",
    "    token_normalize_func=lambda x: x.lower() if isinstance(x, str) else x,\n",
    "    show_default=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @click.group()\n",
    "# @click.option('--debug/--no-debug', default=False)\n",
    "# @click.pass_context\n",
    "# def cli(ctx, debug):\n",
    "#     # ensure that ctx.obj exists and is a dict (in case `cli()` is called\n",
    "#     # by means other than the `if` block below)\n",
    "#     ctx.ensure_object(dict)\n",
    "\n",
    "#     ctx.obj['DEBUG'] = debug\n",
    "\n",
    "# @cli.command()\n",
    "# @click.pass_context\n",
    "# def sync(ctx):\n",
    "#     click.echo(f\"Debug is {'on' if ctx.obj['DEBUG'] else 'off'}\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     cli(obj={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# option_dict = {\n",
    "#     'db_path': click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\"),\n",
    "#     'library': click.option(\"-l\", \"--library\", default=cfg.db.db_path, help=\"Library name\"),\n",
    "# }\n",
    "\n",
    "# # Custom decorator to apply options based on a list of names\n",
    "# def apply_options(option_names):\n",
    "#     def decorator(f):\n",
    "#         for option_name in reversed(option_names):\n",
    "#             f = option_dict[option_name](f)\n",
    "#         return f\n",
    "#     return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Options:\n",
    "    def __init__(self) -> None:\n",
    "        self.db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\")\n",
    "        self.library = click.option(\"-l\", \"--library\", default=cfg.db.db_path, help=\"Library name\")\n",
    "\n",
    "def apply_options(options: list):\n",
    "    def decorator(f):\n",
    "        for option in reversed(options):\n",
    "            f = option(f)\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "@click.group()\n",
    "def arctic():\n",
    "    pass\n",
    "\n",
    "O = Options()\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path])\n",
    "def initdb(db_path):\n",
    "    print(f'Initialized the database {db_path}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library])\n",
    "def use_both(db_path, library):\n",
    "    print(f'Initialized the database {db_path} {library}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@arctic.command()\n",
    "def dropdb():\n",
    "    click.echo('Dropped the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "click_db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "click_library = click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "\n",
    "@click.group()\n",
    "def cool():\n",
    "    pass\n",
    "\n",
    "@cool.command()\n",
    "@click_db_path\n",
    "def initdb(db_path):\n",
    "    print(f'Initialized the database {db_path}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@cool.command()\n",
    "def dropdb():\n",
    "    click.echo('Dropped the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@click.group()\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.pass_context\n",
    "def nic(ctx, db_path, library):\n",
    "    ctx.ensure_object(dict)\n",
    "    ctx.obj['library'] = library\n",
    "\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    ctx.obj['arctic'] = arctic\n",
    "\n",
    "@nic.command()\n",
    "@click.option(\"-t\", \"--ticker\", default=\"AMZN\", help=\"ticker\")\n",
    "@click.pass_context\n",
    "def read(ctx, ticker):\n",
    "    arctic = ctx.obj[\"arctic\"]\n",
    "    library = ctx.obj[\"library\"]\n",
    "    df = arctic[library].read(ticker).data\n",
    "    print(f'df.head() {df.head()}')\n",
    "\n",
    "@nic.command()\n",
    "@click.pass_context\n",
    "def dropdb(ctx):\n",
    "    click.echo('Dropped the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_arctic_library(db_path, library):\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic_library = arctic[library]\n",
    "    return arctic_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code had library passed to all. maybe nicer to use the context thing in the end and do sth like\n",
    "arctic --library=testa --db_path=sth NEXT_COMMAND. This maybe makes some of the decorators i had less relevant as i won't be specifying libray and db_path all over the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "# # TODO: csv_path vs csv_files_path. think if this is a problem...maybe unify?\n",
    "# class Options:\n",
    "#     def __init__(self) -> None:\n",
    "#         self.db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\")\n",
    "#         self.library = click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"Library name\")\n",
    "#         self.ticker = click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to print\")\n",
    "#         self.start_date = click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "#         self.end_date = click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "#         self.csv_path = click.option(\"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\")\n",
    "#         self.etf = click.option(\"--etf\", default=None, help=\"restrict to subset specified by ETF members\")\n",
    "#         self.zip_path = click.option(\"-z\", \"--zip_path\", default=\"/nfs/lobster_data/lobster_raw/2016\", help=\"zip files path\")\n",
    "#         self.tickers = click.option(\"--tickers\", default=None, multiple=True, type=str, help=\"tickers to dump\")\n",
    "#         self.max_workers = click.option(\"-m\", \"--max_workers\", default=20, help=\"max workers for parallelisation\")\n",
    "# O = Options()\n",
    "\n",
    "# def apply_options(options: list):\n",
    "#     def decorator(f):\n",
    "#         for option in reversed(options):\n",
    "#             f = option(f)\n",
    "#         return f\n",
    "#     return decorator\n",
    "\n",
    "# def inherit_docstring_from(source_fn):\n",
    "#     def decorator(target_fn):\n",
    "#         target_fn.__doc__ = source_fn.__doc__\n",
    "#         return target_fn\n",
    "#     return decorator\n",
    "\n",
    "# def infer_options(func) -> list[Callable]:\n",
    "#     \"\"\"Works together with the `auto_apply` to automatically infer arguments.\n",
    "    \n",
    "#     Used together this looks like:\n",
    "#     @auto_apply(infer_options)\n",
    "#     \"\"\"\n",
    "#     sig = signature(func)\n",
    "#     param_names = [\n",
    "#         param.name\n",
    "#         for param in sig.parameters.values()\n",
    "#         if param.kind == param.POSITIONAL_OR_KEYWORD\n",
    "#     ]\n",
    "#     options_list = [getattr(O, name) for name in param_names]\n",
    "#     return options_list\n",
    "\n",
    "# def inherits_from(func):\n",
    "#     \"\"\"Inherit docstring and options from `func`. Actually this wasn't the best idea. Keep separate\"\"\"\n",
    "#     options_list = infer_options(func)\n",
    "\n",
    "#     # still stlightly confused about the order of the decorators, oh well\n",
    "#     def decorator(target_fn):\n",
    "#         @inherit_docstring_from(func)\n",
    "#         @apply_options(options_list)\n",
    "#         @wraps(target_fn)\n",
    "#         def wrapper(*args, **kwargs):\n",
    "#             return target_fn(*args, **kwargs)\n",
    "#         return wrapper\n",
    "#     return decorator\n",
    "\n",
    "# # def simple_inherits_from(func):\n",
    "# #     \"\"\"Simple without using functools.wraps\"\"\"\n",
    "# #     options_list = infer_options(func)\n",
    "# #     def decorator(target_fn):\n",
    "# #         decorated = apply_options(options_list)(target_fn)\n",
    "# #         decorated.__doc__ = func.__doc__\n",
    "# #         return decorated\n",
    "# #     return decorator\n",
    "\n",
    "# @click.group(context_settings=CONTEXT_SETTINGS)\n",
    "# def arctic():\n",
    "#     pass\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path])\n",
    "# def list_libraries(db_path) -> None:\n",
    "#     \"\"\"List arcticdb libraries\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic.list_libraries())\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def list_symbols(db_path, library) -> None:\n",
    "#     \"\"\"List symbols in the arcticdb library.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_symbols())\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def create_library(db_path, library) -> None:\n",
    "#     \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     arctic.create_library(library) \n",
    "#     print(arctic[library])\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# @click.confirmation_option(prompt='Are you sure you want to delete the entire library?')\n",
    "# def delete_library(db_path, library) -> None:\n",
    "#     \"\"\"Delete entire arcticdb library\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     arctic.delete_library(library) \n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.ticker, O.start_date, O.end_date])\n",
    "# def read(db_path, library, ticker, start_date, end_date,\n",
    "# ):\n",
    "#     \"\"\"Read ticker and print head and tail.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     if start_date and end_date:\n",
    "#         start_datetime = pd.Timestamp(f\"{start_date}T{NASDAQExchange.exchange_open}\")\n",
    "#         end_datetime = pd.Timestamp(f\"{end_date}T{NASDAQExchange.exchange_close}\")\n",
    "#         date_range = (start_datetime, end_datetime)\n",
    "#         df = arctic[library].read(ticker, date_range=date_range).data\n",
    "#     else:\n",
    "#         print(\"not using start or end dates\")\n",
    "#         df = arctic[library].read(ticker).data\n",
    "    \n",
    "#     print(f\"Printing df.head() and df.tail() for ticker {ticker}\")\n",
    "#     print(df.head())\n",
    "#     print(df.tail())\n",
    "\n",
    "# def _write(\n",
    "#     db_path,\n",
    "#     library,\n",
    "#     csv_path,\n",
    "#     ticker,\n",
    "#     start_date,\n",
    "#     end_date,\n",
    "# ):\n",
    "#     \"\"\"Preprocess and write ticker to database.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     date_range = (start_date, end_date)\n",
    "#     data = Data(\n",
    "#         directory_path=csv_path,\n",
    "#         ticker=ticker,\n",
    "#         date_range=date_range,\n",
    "#         aggregate_duplicates=False,\n",
    "#     )\n",
    "#     lobster = Lobster(data=data)\n",
    "#     df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "#     print(f\"head of ticker {ticker}\")\n",
    "#     print(df.head())\n",
    "\n",
    "#     arctic[library].write(symbol=ticker, data=df)\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.ticker, O.start_date, O.end_date])\n",
    "# def write(**kwargs):\n",
    "#     _write(**kwargs)\n",
    "\n",
    "\n",
    "# # if want to also access _say from other functions then need to do this.\n",
    "# def _say(\n",
    "#     db_path,\n",
    "#     library,\n",
    "# ):\n",
    "#     \"\"\"Print some really important information\"\"\"\n",
    "#     print(db_path, library)\n",
    "\n",
    "\n",
    "# # @arctic.command()\n",
    "# # @apply_options(infer_options(_say))\n",
    "# # @inherit_docstring_from(_say)\n",
    "# # def say(**kwargs):\n",
    "# #     _say(**kwargs)\n",
    "\n",
    "# @arctic.command()\n",
    "# @inherits_from(_say)\n",
    "# def say(**kwargs):\n",
    "#     _say(**kwargs)\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.start_date, O.end_date])\n",
    "# def generate_jobs(db_path, library, csv_path, start_date, end_date):\n",
    "#     ticker_date_dict = infer_ticker_to_date_range(csv_path)\n",
    "#     with open('arctic_commands.txt', 'w') as f:\n",
    "#         for ticker, (inferred_start_date, inferred_end_date) in ticker_date_dict.items():\n",
    "#             # if date is None use the inferred date, otherwise use the CLI argument\n",
    "#             start_date = start_date or inferred_start_date\n",
    "#             end_date = end_date or inferred_end_date\n",
    "#             f.write(f\"arctic write --csv_path={csv_path} --db_path={db_path} --library={library} --ticker={ticker} --start_date={start_date} --end_date={end_date} \\n\")\n",
    "\n",
    "# def sleepy(csv_path, folder_info):\n",
    "#     time.sleep(5)\n",
    "#     print(csv_path, folder_info.full)\n",
    "\n",
    "# def extract_7z(input_path, output_path):\n",
    "#     try:\n",
    "#         subprocess.run([\"7z\", \"x\", input_path, f\"-o{output_path}\"], check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.zip_path, O.csv_path, O.etf, O.max_workers])\n",
    "# def zip(zip_path, csv_path, etf, max_workers):\n",
    "#     folder_infos = infer_ticker_dict(zip_path)\n",
    "\n",
    "#     # filter first\n",
    "#     if etf:\n",
    "#         def in_etf(folder_info):\n",
    "#             return folder_info.ticker in ETFMembers().mapping[etf] + [etf]\n",
    "#         folder_infos = list(filter(in_etf, folder_infos))\n",
    "\n",
    "#     # commands = [f\"mkdir -p {csv_path}/{folder_info.ticker_till_end}\\n\"\n",
    "#     #             for folder_info in folder_infos]\n",
    "\n",
    "#     # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "#     #     [f.write(command) for command in commands]\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # outputs_dirs = [folder_info.ticker_till_end for folder_info in folder_infos]\n",
    "#         futures = [\n",
    "#             executor.submit(os.mkdir, path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "#         wait(futures)\n",
    "#         futures = [\n",
    "#             executor.submit(extract_7z, input_path=folder_info.full, output_path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "\n",
    "\n",
    "#         # for folder_info in folder_infos:\n",
    "#         #     # print(folder_info.ticker)\n",
    "#         #     f.write(f\"examle: mkdir {csv_path}/{folder_info.ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "#     # ticker_date_dict = infer_ticker_to_ticker_path(zip_path)\n",
    "#     # print(ticker_date_dict)\n",
    "#     # if etf:\n",
    "#     #     print(ETFMembers().mapping[etf])\n",
    "#     #     ticker_date_dict = {\n",
    "#     #         ticker: ticker_path\n",
    "#     #         for ticker, ticker_path in ticker_date_dict.items()\n",
    "#     #         if ticker in ETFMembers().mapping[etf] + [etf]\n",
    "#     #     }\n",
    "#     # print(ticker_date_dict)\n",
    "#     # ticker_dict = infer_ticker_dict(zip_path)\n",
    "#     # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "#     #     for ticker, dict_ in ticker_dict.items():\n",
    "#     #         full = dict_[\"full\"]\n",
    "#     #         ticker_till_end = dict_[\"ticker_till_end\"]\n",
    "#     #         f.write(f\"mkdir {csv_path}/{ticker_till_end}\\n\")\n",
    "#     #         f.write(f\"/nfs/home/nicolasp/usr/bin/7z x {full} -o{ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.tickers, O.max_workers])\n",
    "# def dump(\n",
    "#     db_path,\n",
    "#     library,\n",
    "#     csv_path,\n",
    "#     tickers,\n",
    "#     max_workers,\n",
    "# ):\n",
    "#     \"\"\"Dump all csv to arctic_db inferring start and end date from folder.\"\"\"\n",
    "#     folder_infos = infer_ticker_dict(csv_path)\n",
    "#     print(\"inferred from folder\")\n",
    "#     print(folder_infos)\n",
    "\n",
    "#     if tickers:\n",
    "#         folder_infos = [folder_info for folder_info in folder_infos if folder_info.ticker in tickers]\n",
    "\n",
    "#     print(\"filtered folder_info after filtering for tickers.\")\n",
    "#     print(folder_infos)\n",
    "\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # small job with only a few dates\n",
    "#         # futures = [\n",
    "#         #     executor.submit(write_, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=\"2016-01-01\", end_date=\"2016-01-04\")\n",
    "#         #     for folder_info in folder_infos\n",
    "#         # ]\n",
    "#         # full job with whole year\n",
    "#         futures = [\n",
    "#             executor.submit(_write, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=folder_info.start_date, end_date=folder_info.end_date)\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "#     print('done')\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class ArcticDBInfo:\n",
    "#     ticker: str\n",
    "#     dates_ndarray: np.ndarray\n",
    "#     dates_series: pd.Series\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         self.dates_list: list[str] = list(self.dates_ndarray)\n",
    "#         self.start_date = min(self.dates_ndarray)\n",
    "#         self.end_date = max(self.dates_ndarray)\n",
    "\n",
    "# def _info(db_path, library) -> list[ArcticDBInfo]:\n",
    "#     \"\"\"Return information about ticker info in database.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     arcticdb_infos: list[ArcticDBInfo] = []\n",
    "#     for ticker in arctic[library].list_symbols():\n",
    "#         q = QueryBuilder()\n",
    "#         # there is one auction each morning\n",
    "#         q = q[q.event == Event.CROSS_TRADE.value]\n",
    "#         df = arctic[library].read(symbol=ticker, query_builder=q).data\n",
    "\n",
    "#         dates_series: pd.Series = df.index.date\n",
    "#         dates_ndarray: np.ndarray = df.index.to_series().dt.strftime(\"%Y-%m-%d\").values\n",
    "#         arcticdb_infos.append(\n",
    "#             ArcticDBInfo(ticker=ticker, dates_ndarray=dates_ndarray, dates_series=dates_series)\n",
    "#         )\n",
    "#     return arcticdb_infos\n",
    "\n",
    "\n",
    "# @arctic.group()\n",
    "# @inherits_from(_info)\n",
    "# def info(**kwargs):\n",
    "#     pass\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options(infer_options(_info))\n",
    "# def tickers(db_path, library):\n",
    "#     \"\"\"Print tickers in db.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_symbols())\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def versions(db_path, library):\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_versions())\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options(infer_options(_info))\n",
    "# def dates(**kwargs):\n",
    "#     \"\"\"Print ticker information\"\"\"\n",
    "#     arcticdb_infos = _info(**kwargs)\n",
    "#     print({x.ticker: (x.start_date, x.end_date) for x in arcticdb_infos})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "# # TODO: csv_path vs csv_files_path. think if this is a problem...maybe unify?\n",
    "# class Options:\n",
    "#     def __init__(self) -> None:\n",
    "#         self.db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\")\n",
    "#         self.library = click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"Library name\")\n",
    "#         self.ticker = click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to print\")\n",
    "#         self.start_date = click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "#         self.end_date = click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "#         self.csv_path = click.option(\"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\")\n",
    "#         self.etf = click.option(\"--etf\", default=None, help=\"restrict to subset specified by ETF members\")\n",
    "#         self.zip_path = click.option(\"-z\", \"--zip_path\", default=\"/nfs/lobster_data/lobster_raw/2016\", help=\"zip files path\")\n",
    "#         self.tickers = click.option(\"--tickers\", default=None, multiple=True, type=str, help=\"tickers to dump\")\n",
    "#         self.max_workers = click.option(\"-m\", \"--max_workers\", default=20, help=\"max workers for parallelisation\")\n",
    "# O = Options()\n",
    "\n",
    "# def apply_options(options: list):\n",
    "#     def decorator(f):\n",
    "#         for option in reversed(options):\n",
    "#             f = option(f)\n",
    "#         return f\n",
    "#     return decorator\n",
    "\n",
    "# def inherit_docstring_from(source_fn):\n",
    "#     def decorator(target_fn):\n",
    "#         target_fn.__doc__ = source_fn.__doc__\n",
    "#         return target_fn\n",
    "#     return decorator\n",
    "\n",
    "# def infer_options(func) -> list[Callable]:\n",
    "#     \"\"\"Works together with the `auto_apply` to automatically infer arguments.\n",
    "    \n",
    "#     Used together this looks like:\n",
    "#     @auto_apply(infer_options)\n",
    "#     \"\"\"\n",
    "#     sig = signature(func)\n",
    "#     param_names = [\n",
    "#         param.name\n",
    "#         for param in sig.parameters.values()\n",
    "#         if param.kind == param.POSITIONAL_OR_KEYWORD\n",
    "#     ]\n",
    "#     options_list = [getattr(O, name) for name in param_names]\n",
    "#     return options_list\n",
    "\n",
    "# def inherits_from(func):\n",
    "#     \"\"\"Inherit docstring and options from `func`. Actually this wasn't the best idea. Keep separate\"\"\"\n",
    "#     options_list = infer_options(func)\n",
    "\n",
    "#     # still stlightly confused about the order of the decorators, oh well\n",
    "#     def decorator(target_fn):\n",
    "#         @inherit_docstring_from(func)\n",
    "#         @apply_options(options_list)\n",
    "#         @wraps(target_fn)\n",
    "#         def wrapper(*args, **kwargs):\n",
    "#             return target_fn(*args, **kwargs)\n",
    "#         return wrapper\n",
    "#     return decorator\n",
    "\n",
    "# # def simple_inherits_from(func):\n",
    "# #     \"\"\"Simple without using functools.wraps\"\"\"\n",
    "# #     options_list = infer_options(func)\n",
    "# #     def decorator(target_fn):\n",
    "# #         decorated = apply_options(options_list)(target_fn)\n",
    "# #         decorated.__doc__ = func.__doc__\n",
    "# #         return decorated\n",
    "# #     return decorator\n",
    "\n",
    "# @click.group(context_settings=CONTEXT_SETTINGS)\n",
    "# def arctic():\n",
    "#     pass\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path])\n",
    "# def list_libraries(db_path) -> None:\n",
    "#     \"\"\"List arcticdb libraries\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic.list_libraries())\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def list_symbols(db_path, library) -> None:\n",
    "#     \"\"\"List symbols in the arcticdb library.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_symbols())\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def create_library(db_path, library) -> None:\n",
    "#     \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     arctic.create_library(library) \n",
    "#     print(arctic[library])\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# @click.confirmation_option(prompt='Are you sure you want to delete the entire library?')\n",
    "# def delete_library(db_path, library) -> None:\n",
    "#     \"\"\"Delete entire arcticdb library\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     arctic.delete_library(library) \n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.ticker, O.start_date, O.end_date])\n",
    "# def read(db_path, library, ticker, start_date, end_date,\n",
    "# ):\n",
    "#     \"\"\"Read ticker and print head and tail.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     if start_date and end_date:\n",
    "#         start_datetime = pd.Timestamp(f\"{start_date}T{NASDAQExchange.exchange_open}\")\n",
    "#         end_datetime = pd.Timestamp(f\"{end_date}T{NASDAQExchange.exchange_close}\")\n",
    "#         date_range = (start_datetime, end_datetime)\n",
    "#         df = arctic[library].read(ticker, date_range=date_range).data\n",
    "#     else:\n",
    "#         print(\"not using start or end dates\")\n",
    "#         df = arctic[library].read(ticker).data\n",
    "    \n",
    "#     print(f\"Printing df.head() and df.tail() for ticker {ticker}\")\n",
    "#     print(df.head())\n",
    "#     print(df.tail())\n",
    "\n",
    "# def _write(\n",
    "#     db_path,\n",
    "#     library,\n",
    "#     csv_path,\n",
    "#     ticker,\n",
    "#     start_date,\n",
    "#     end_date,\n",
    "# ):\n",
    "#     \"\"\"Preprocess and write ticker to database.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     date_range = (start_date, end_date)\n",
    "#     data = Data(\n",
    "#         directory_path=csv_path,\n",
    "#         ticker=ticker,\n",
    "#         date_range=date_range,\n",
    "#         aggregate_duplicates=False,\n",
    "#     )\n",
    "#     lobster = Lobster(data=data)\n",
    "#     df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "#     print(f\"head of ticker {ticker}\")\n",
    "#     print(df.head())\n",
    "\n",
    "#     arctic[library].write(symbol=ticker, data=df)\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.ticker, O.start_date, O.end_date])\n",
    "# def write(**kwargs):\n",
    "#     _write(**kwargs)\n",
    "\n",
    "\n",
    "# # if want to also access _say from other functions then need to do this.\n",
    "# def _say(\n",
    "#     db_path,\n",
    "#     library,\n",
    "# ):\n",
    "#     \"\"\"Print some really important information\"\"\"\n",
    "#     print(db_path, library)\n",
    "\n",
    "\n",
    "# # @arctic.command()\n",
    "# # @apply_options(infer_options(_say))\n",
    "# # @inherit_docstring_from(_say)\n",
    "# # def say(**kwargs):\n",
    "# #     _say(**kwargs)\n",
    "\n",
    "# @arctic.command()\n",
    "# @inherits_from(_say)\n",
    "# def say(**kwargs):\n",
    "#     _say(**kwargs)\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.start_date, O.end_date])\n",
    "# def generate_jobs(db_path, library, csv_path, start_date, end_date):\n",
    "#     ticker_date_dict = infer_ticker_to_date_range(csv_path)\n",
    "#     with open('arctic_commands.txt', 'w') as f:\n",
    "#         for ticker, (inferred_start_date, inferred_end_date) in ticker_date_dict.items():\n",
    "#             # if date is None use the inferred date, otherwise use the CLI argument\n",
    "#             start_date = start_date or inferred_start_date\n",
    "#             end_date = end_date or inferred_end_date\n",
    "#             f.write(f\"arctic write --csv_path={csv_path} --db_path={db_path} --library={library} --ticker={ticker} --start_date={start_date} --end_date={end_date} \\n\")\n",
    "\n",
    "# def sleepy(csv_path, folder_info):\n",
    "#     time.sleep(5)\n",
    "#     print(csv_path, folder_info.full)\n",
    "\n",
    "# def extract_7z(input_path, output_path):\n",
    "#     try:\n",
    "#         subprocess.run([\"7z\", \"x\", input_path, f\"-o{output_path}\"], check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.zip_path, O.csv_path, O.etf, O.max_workers])\n",
    "# def zip(zip_path, csv_path, etf, max_workers):\n",
    "#     folder_infos = infer_ticker_dict(zip_path)\n",
    "\n",
    "#     # filter first\n",
    "#     if etf:\n",
    "#         def in_etf(folder_info):\n",
    "#             return folder_info.ticker in ETFMembers().mapping[etf] + [etf]\n",
    "#         folder_infos = list(filter(in_etf, folder_infos))\n",
    "\n",
    "#     # commands = [f\"mkdir -p {csv_path}/{folder_info.ticker_till_end}\\n\"\n",
    "#     #             for folder_info in folder_infos]\n",
    "\n",
    "#     # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "#     #     [f.write(command) for command in commands]\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # outputs_dirs = [folder_info.ticker_till_end for folder_info in folder_infos]\n",
    "#         futures = [\n",
    "#             executor.submit(os.mkdir, path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "#         wait(futures)\n",
    "#         futures = [\n",
    "#             executor.submit(extract_7z, input_path=folder_info.full, output_path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "\n",
    "\n",
    "#         # for folder_info in folder_infos:\n",
    "#         #     # print(folder_info.ticker)\n",
    "#         #     f.write(f\"examle: mkdir {csv_path}/{folder_info.ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "#     # ticker_date_dict = infer_ticker_to_ticker_path(zip_path)\n",
    "#     # print(ticker_date_dict)\n",
    "#     # if etf:\n",
    "#     #     print(ETFMembers().mapping[etf])\n",
    "#     #     ticker_date_dict = {\n",
    "#     #         ticker: ticker_path\n",
    "#     #         for ticker, ticker_path in ticker_date_dict.items()\n",
    "#     #         if ticker in ETFMembers().mapping[etf] + [etf]\n",
    "#     #     }\n",
    "#     # print(ticker_date_dict)\n",
    "#     # ticker_dict = infer_ticker_dict(zip_path)\n",
    "#     # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "#     #     for ticker, dict_ in ticker_dict.items():\n",
    "#     #         full = dict_[\"full\"]\n",
    "#     #         ticker_till_end = dict_[\"ticker_till_end\"]\n",
    "#     #         f.write(f\"mkdir {csv_path}/{ticker_till_end}\\n\")\n",
    "#     #         f.write(f\"/nfs/home/nicolasp/usr/bin/7z x {full} -o{ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "# @arctic.command()\n",
    "# @apply_options([O.db_path, O.library, O.csv_path, O.tickers, O.max_workers])\n",
    "# def dump(\n",
    "#     db_path,\n",
    "#     library,\n",
    "#     csv_path,\n",
    "#     tickers,\n",
    "#     max_workers,\n",
    "# ):\n",
    "#     \"\"\"Dump all csv to arctic_db inferring start and end date from folder.\"\"\"\n",
    "#     folder_infos = infer_ticker_dict(csv_path)\n",
    "#     print(\"inferred from folder\")\n",
    "#     print(folder_infos)\n",
    "\n",
    "#     if tickers:\n",
    "#         folder_infos = [folder_info for folder_info in folder_infos if folder_info.ticker in tickers]\n",
    "\n",
    "#     print(\"filtered folder_info after filtering for tickers.\")\n",
    "#     print(folder_infos)\n",
    "\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # small job with only a few dates\n",
    "#         # futures = [\n",
    "#         #     executor.submit(write_, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=\"2016-01-01\", end_date=\"2016-01-04\")\n",
    "#         #     for folder_info in folder_infos\n",
    "#         # ]\n",
    "#         # full job with whole year\n",
    "#         futures = [\n",
    "#             executor.submit(_write, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=folder_info.start_date, end_date=folder_info.end_date)\n",
    "#             for folder_info in folder_infos\n",
    "#         ]\n",
    "#     print('done')\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class ArcticLibraryInfo:\n",
    "#     ticker: str\n",
    "#     dates_ndarray: np.ndarray\n",
    "#     dates_series: pd.Series\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         self.dates_list: list[str] = list(self.dates_ndarray)\n",
    "#         self.start_date = min(self.dates_ndarray)\n",
    "#         self.end_date = max(self.dates_ndarray)\n",
    "\n",
    "# def _info(db_path, library) -> list[ArcticLibraryInfo]:\n",
    "#     \"\"\"Return information about ticker info in database.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "#     arcticdb_infos: list[ArcticLibraryInfo] = []\n",
    "#     for ticker in arctic[library].list_symbols():\n",
    "#         q = QueryBuilder()\n",
    "#         # there is one auction each morning\n",
    "#         q = q[q.event == Event.CROSS_TRADE.value]\n",
    "#         df = arctic[library].read(symbol=ticker, query_builder=q).data\n",
    "\n",
    "#         dates_series: pd.Series = df.index.date\n",
    "#         dates_ndarray: np.ndarray = df.index.to_series().dt.strftime(\"%Y-%m-%d\").values\n",
    "#         arcticdb_infos.append(\n",
    "#             ArcticLibraryInfo(ticker=ticker, dates_ndarray=dates_ndarray, dates_series=dates_series)\n",
    "#         )\n",
    "#     return arcticdb_infos\n",
    "\n",
    "\n",
    "# @arctic.group()\n",
    "# @inherits_from(_info)\n",
    "# def info(**kwargs):\n",
    "#     pass\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options(infer_options(_info))\n",
    "# def tickers(db_path, library):\n",
    "#     \"\"\"Print tickers in db.\"\"\"\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_symbols())\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options([O.db_path, O.library])\n",
    "# def versions(db_path, library):\n",
    "#     arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "#     print(arctic[library].list_versions())\n",
    "\n",
    "# @info.command()\n",
    "# @apply_options(infer_options(_info))\n",
    "# def dates(**kwargs):\n",
    "#     \"\"\"Print ticker information\"\"\"\n",
    "#     arcticdb_infos = _info(**kwargs)\n",
    "#     print({x.ticker: (x.start_date, x.end_date) for x in arcticdb_infos})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor so that arctic gets Arctci() and passes that down to subcommands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ArcticLibraryInfo:\n",
    "    ticker: str\n",
    "    dates_ndarray: np.ndarray\n",
    "    dates_series: pd.Series\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.dates_list: list[str] = list(self.dates_ndarray)\n",
    "        self.start_date = min(self.dates_ndarray)\n",
    "        self.end_date = max(self.dates_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "CONTEXT_SETTINGS = dict(\n",
    "    help_option_names=[\"-h\", \"--help\"],\n",
    "    token_normalize_func=lambda x: x.lower() if isinstance(x, str) else x,\n",
    "    show_default=True,\n",
    "    auto_envvar_prefix=\"ARCTIC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Library' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#Z1001sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# | export\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#Z1001sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# REFACTORINOOOOO\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#Z1001sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_library_info\u001b[39m(\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#Z1001sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     arctic_library: Library,  \u001b[39m# arcticdb library\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#Z1001sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     tickers: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,  \u001b[39m# tickers to filter on\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#Z1001sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[ArcticLibraryInfo]:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#Z1001sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return information about ticker info in database.\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#Z1001sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     arctic_symbols \u001b[39m=\u001b[39m arctic_library\u001b[39m.\u001b[39mlist_symbols()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Library' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "# REFACTORINOOOOO\n",
    "\n",
    "\n",
    "def get_library_info(\n",
    "    arctic_library: Library,  # arcticdb library\n",
    "    tickers: list[str] | None = None,  # tickers to filter on\n",
    ") -> list[ArcticLibraryInfo]:\n",
    "    \"\"\"Return information about ticker info in database.\"\"\"\n",
    "\n",
    "    arctic_symbols = arctic_library.list_symbols()\n",
    "    if tickers:\n",
    "        if not set(tickers).issubset(set(arctic_symbols)):\n",
    "            raise ValueError(\n",
    "                f\"Some of the tickers specified were not in the databasee. The invalid tickers were {set(tickers) - set(arctic_symbols)}\"\n",
    "            )\n",
    "    else:\n",
    "        tickers = arctic_symbols\n",
    "\n",
    "    arctic_library_infos: list[ArcticLibraryInfo] = []\n",
    "    for ticker in tickers:\n",
    "        q = QueryBuilder()\n",
    "        # there is one auction each morning\n",
    "        q = q[q.event == Event.CROSS_TRADE.value]\n",
    "        df = arctic_library.read(symbol=ticker, query_builder=q).data\n",
    "\n",
    "        dates_series: pd.Series = df.index.date\n",
    "        dates_ndarray: np.ndarray = df.index.to_series().dt.strftime(\"%Y-%m-%d\").values\n",
    "        arctic_library_infos.append(\n",
    "            ArcticLibraryInfo(\n",
    "                ticker=ticker, dates_ndarray=dates_ndarray, dates_series=dates_series\n",
    "            )\n",
    "        )\n",
    "    return arctic_library_infos\n",
    "\n",
    "\n",
    "class Options:\n",
    "    def __init__(self) -> None:\n",
    "        self.db_path = click.option(\n",
    "            \"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\"\n",
    "        )\n",
    "        self.library = click.option(\n",
    "            \"-l\", \"--library\", default=cfg.db.library, help=\"Library name\"\n",
    "        )\n",
    "        self.ticker = click.option(\n",
    "            \"-t\", \"--ticker\", required=True, help=\"ticker to print\"\n",
    "        )\n",
    "        self.start_date = click.option(\n",
    "            \"-s\", \"--start_date\", default=None, help=\"start date\"\n",
    "        )\n",
    "        self.end_date = click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "        self.csv_path = click.option(\n",
    "            \"-c\",\n",
    "            \"--csv_path\",\n",
    "            default=cfg.data_config.csv_files_path,\n",
    "            help=\"csv files path\",\n",
    "        )\n",
    "        self.etf = click.option(\n",
    "            \"--etf\", default=None, help=\"restrict to subset specified by ETF members\"\n",
    "        )\n",
    "        self.zip_path = click.option(\n",
    "            \"-z\",\n",
    "            \"--zip_path\",\n",
    "            default=\"/nfs/lobster_data/lobster_raw/2016\",\n",
    "            help=\"zip files path\",\n",
    "        )\n",
    "        self.tickers = click.option(\n",
    "            \"--tickers\", default=None, multiple=True, type=str, help=\"tickers to dump\"\n",
    "        )\n",
    "        self.max_workers = click.option(\n",
    "            \"-m\", \"--max_workers\", default=20, help=\"max workers for parallelisation\"\n",
    "        )\n",
    "\n",
    "\n",
    "O = Options()\n",
    "\n",
    "\n",
    "class Notify:\n",
    "    @property\n",
    "    def warn(self):\n",
    "        click.secho(\"WARNING:\", fg=\"red\", bold=True)\n",
    "\n",
    "    @property\n",
    "    def info(self):\n",
    "        click.secho(\"INFO:\", fg=\"yellow\", bold=True)\n",
    "\n",
    "N = Notify()\n",
    "\n",
    "def apply_options(options: list):\n",
    "    def decorator(f):\n",
    "        for option in reversed(options):\n",
    "            f = option(f)\n",
    "        return f\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class ClickCtxObj(TypedDict):\n",
    "    \"\"\"Purely for type hinting. for instance `arctic_library` not always there.\"\"\"\n",
    "\n",
    "    library: str\n",
    "    db_path: str\n",
    "    arctic: Arctic\n",
    "    arctic_library: Library\n",
    "\n",
    "\n",
    "class ClickCtx(Protocol):\n",
    "    obj: ClickCtxObj\n",
    "\n",
    "\n",
    "@click.group(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-d\", \"--db_path\", default=cfg.db.db_path, envvar=\"DB_PATH\", help=\"Database path\"\n",
    ")\n",
    "@click.option(\n",
    "    \"-l\", \"--library\", default=cfg.db.library, envvar=\"LIBRARY\", help=\"Library name\"\n",
    ")\n",
    "@click.option(\"--debug\", is_flag=True, default=False, help=\"print logging info\")\n",
    "@click.pass_context\n",
    "def arctic(ctx, db_path, library):\n",
    "    ctx.ensure_object(dict)\n",
    "    arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "    ctx.obj.update(\n",
    "        {\n",
    "            \"arctic\": arctic,\n",
    "            \"library\": library,\n",
    "            \"db_path\": db_path,\n",
    "        }\n",
    "    )\n",
    "    try:\n",
    "        ctx.obj[\"arctic_library\"] = arctic[library]\n",
    "    except LibraryNotFound:\n",
    "        pass\n",
    "\n",
    "@arctic.command()\n",
    "@click.pass_context\n",
    "def echo(ctx: ClickCtx) -> None:\n",
    "    \"\"\"Echo back inputs.\"\"\"\n",
    "    click.echo(pformat(ctx.obj))\n",
    "\n",
    "\n",
    "# not a good idea ! monkey patch click.echo to accept a color\n",
    "# def new_echo(text, file=None, nl=True, err=False, color=None):\n",
    "#     if color:\n",
    "#         text = click.style(text, fg=color)\n",
    "#     click.echo.original(text, file=file, nl=nl, err=err)\n",
    "\n",
    "# def warn(msg):\n",
    "#     return click.style(msg, fg=\"red\", bold=True, blink=True)\n",
    "\n",
    "@arctic.command()\n",
    "def init_autocomplete():\n",
    "    \"\"\"Initialise autocomplete for arctic CLI.\"\"\"\n",
    "    # TODO: improve performance of CLI\n",
    "    os.system(\"_ARCTIC_COMPLETE=bash_source arctic > ~/.arctic-complete.bash\")\n",
    "\n",
    "    with open(os.path.expanduser(\"~/.bashrc\"), \"a\") as f:\n",
    "        f.write(\n",
    "            textwrap.dedent(\n",
    "                \"\"\"\\\n",
    "                # >>> arctic init_autocomplete >>>\n",
    "                # Contents within this block were generated by arctic init_autocomplete\n",
    "                . ~/.arctic-complete.bash\n",
    "                # <<< arctic init_autocomplete <<<\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    click.echo(\n",
    "        \"Autocomplete initialized. Please restart your shell or run `source ~/.bashrc`.\"\n",
    "    )\n",
    "    click.echo(\n",
    "        \"Autocomplete initialized. Please restart your shell or run `source ~/.bashrc`.\"\n",
    "    )\n",
    "\n",
    "\n",
    "@arctic.command()\n",
    "@click.pass_context\n",
    "def create(ctx: ClickCtx) -> None:\n",
    "    \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "    arctic = ctx.obj[\"arctic\"]\n",
    "    library = ctx.obj[\"library\"]\n",
    "    arctic.create_library(library)\n",
    "    click.echo(arctic[library])\n",
    "\n",
    "\n",
    "@arctic.group()\n",
    "@click.pass_context\n",
    "def ls(ctx: ClickCtx):\n",
    "    \"\"\"List information about database.\"\"\"\n",
    "    # NOTE: Using word list clashed with python type hints!\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@ls.command()\n",
    "@click.pass_context\n",
    "def libraries(ctx: ClickCtx):\n",
    "    arctic = ctx.obj[\"arctic\"]\n",
    "    click.echo(arctic.list_libraries())\n",
    "\n",
    "\n",
    "@ls.command()\n",
    "@click.pass_context\n",
    "def symbols(ctx: ClickCtx):\n",
    "    arctic_library = ctx.obj[\"arctic_library\"]\n",
    "    click.echo(arctic_library.list_symbols())\n",
    "\n",
    "\n",
    "@ls.command()\n",
    "@click.pass_context\n",
    "def versions(ctx: ClickCtx):\n",
    "    arctic_library = ctx.obj[\"arctic_library\"]\n",
    "\n",
    "    click.echo(\n",
    "        (\n",
    "            pd.DataFrame(arctic_library.list_versions())\n",
    "            .transpose()\n",
    "            .drop(columns=[1, 2])\n",
    "            .rename(columns={0: \"created_on\"})\n",
    "            .assign(\n",
    "                created_on=lambda df: df[\"created_on\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "            .rename_axis([\"ticker\", \"version\"])\n",
    "            .sort_index(level=[0, 1], ascending=[True, False])\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_comma_separated(ctx, param, value: str):\n",
    "    \"\"\"Convert a comma (or space) separated option to a list of options\"\"\"\n",
    "    if value is not None:\n",
    "        delimiters = r\"[ ,]\"\n",
    "        option_list = re.split(delimiters, value)\n",
    "        option_list = list(filter(None, option_list))\n",
    "        return option_list\n",
    "\n",
    "\n",
    "@ls.command()\n",
    "# @click.option(\"-t\", \"--tickers\", callback=parse_comma_separated , help=\"Comma or space separated tickers\")\n",
    "@click.option(\n",
    "    \"-t\", \"--tickers\", multiple=True, type=str, help=\"Provide ticker(s) to filter on\"\n",
    ")\n",
    "@click.option(\n",
    "    \"-a\",\n",
    "    \"--all\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    help=\"print all dates not just start and end\",\n",
    ")\n",
    "@click.pass_context\n",
    "def dates(ctx: ClickCtx, tickers, all):\n",
    "    arctic_library = ctx.obj[\"arctic_library\"]\n",
    "\n",
    "    arctic_library_infos = get_library_info(arctic_library, tickers=tickers)\n",
    "\n",
    "    if all:\n",
    "        click.echo(pformat({x.ticker: x.dates_list for x in arctic_library_infos}))\n",
    "    else:\n",
    "        click.echo(\n",
    "            pformat(\n",
    "                {x.ticker: (x.start_date, x.end_date) for x in arctic_library_infos}\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "@arctic.group()\n",
    "@click.pass_context\n",
    "def rm(ctx: ClickCtx):\n",
    "    \"\"\"List information about database.\"\"\"\n",
    "    # NOTE: Using word del clashed with python!\n",
    "    pass\n",
    "\n",
    "\n",
    "@rm.command()\n",
    "@click.pass_context\n",
    "@click.option(\n",
    "    \"-l\", \"--library\", required=True, type=str, help=\"Library to permanently delete\"\n",
    ")\n",
    "@click.option(\n",
    "    \"-s\",\n",
    "    \"--simple\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    help=\"For dealing with 'Error removing LMDB tree at path=...'\",\n",
    ")\n",
    "def library(ctx: ClickCtx, library, simple):\n",
    "    arctic = ctx.obj[\"arctic\"]\n",
    "\n",
    "    def _simple_delete_library(arctic, library):\n",
    "        if arctic.has_library(library):\n",
    "            if click.confirm(\n",
    "                \"Are you sure you want to permanently delete the library?\"\n",
    "            ):\n",
    "                arctic.delete_library(library)\n",
    "        else:\n",
    "            click.echo(\"No library found to delete.\")\n",
    "\n",
    "    def _normal_delete_library(arctic, library):\n",
    "        try:\n",
    "            arctic_library = arctic[library]\n",
    "            click.echo(\n",
    "                textwrap.dedent(\n",
    "                    f\"\"\"\\\n",
    "                    Library information:\n",
    "                    {arctic_library}\n",
    "\n",
    "                    Tickers in this library:\n",
    "                    {arctic_library.list_symbols()}\"\"\"))\n",
    "            N.warn\n",
    "            if click.confirm((\"Are you sure you want to permanently delete the library?\")):\n",
    "                # i'm not sure if the below will free the connection 100% of the time. use simple if encountering problems\n",
    "                del arctic_library\n",
    "                gc.collect()\n",
    "                arctic.delete_library(library)\n",
    "        except LibraryNotFound:\n",
    "            click.echo(\"No library found to delete.\")\n",
    "\n",
    "    if simple:\n",
    "        _simple_delete_library(arctic=arctic, library=library)\n",
    "    else:\n",
    "        _normal_delete_library(arctic=arctic, library=library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COP', 'APA', 'MRO']\n"
     ]
    }
   ],
   "source": [
    "tickers=\"COP, APA, MRO\"\n",
    "# tickers=\"COP APA\"\n",
    "tickers = re.split(r'[ ,]', tickers)\n",
    "tickers = list(filter(None, tickers))\n",
    "# [ticker for ticker in tickers if ticker]\n",
    "# tickers = [ticker.strip() for ticker in tickers if ticker.strip()]\n",
    "\n",
    "print(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'APA '"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" APA, \".strip().strip(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COP', 'APA']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #bizzare stuff\n",
    "\n",
    "# @list.command()\n",
    "# @click.pass_context\n",
    "# def versions(ctx: ClickCtx):\n",
    "#     arctic = ctx.obj[\"arctic\"]\n",
    "#     library = ctx.obj[\"library\"]\n",
    "#     print(arctic[library].list_versions())\n",
    "\n",
    "#     # grouped information\n",
    "#     print(\"grouped info \\n\")\n",
    "#     # data_ = arctic[library].list_versions()\n",
    "#     # print(data_)\n",
    "#     # absolutely bizzare behaviour of click\n",
    "#     # logger.info(list(data_.items()))\n",
    "#     # print(list(data_.items()), flush=True)\n",
    "#     df = pd.DataFrame(arctic[library].list_versions())\n",
    "#     print(df)\n",
    "#     # df = pd.DataFrame(list(data.items()), columns=[\"Key\", \"Date\"])\n",
    "#     # print(df)\n",
    "#     # print(df)\n",
    "#     # print(df)\n",
    "#     # df['Ticker'] = df['Key'].str.split('_').str[0]\n",
    "#     # df['Version'] = df['Key'].str.split('_').str[1]\n",
    "\n",
    "#     # grouped = df.groupby('Ticker').apply(lambda x: x[['Version', 'Date']].to_dict(orient='records')).to_dict()\n",
    "#     # print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'arcticdb.version_store.library.Library'>\n",
      "yooo\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "db_path = cfg.db.db_path\n",
    "library = cfg.db.library\n",
    "arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "arctic_library = arctic[library]\n",
    "print(type(arctic_library))\n",
    "\n",
    "try:\n",
    "    arctic[\"not_there_library\"]\n",
    "except LibraryNotFound:\n",
    "    print('yooo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{XOM_v2: (date=2023-09-27 02:26:44.171414752+00:00),\n",
       " XOM_v1: (date=2023-09-27 02:23:08.300722194+00:00),\n",
       " XOM_v0: (date=2023-09-27 02:21:24.583990502+00:00),\n",
       " WMB_v1: (date=2023-09-27 02:26:37.634894225+00:00),\n",
       " WMB_v0: (date=2023-09-27 02:23:01.919661098+00:00),\n",
       " SLB_v2: (date=2023-09-27 02:52:34.397110262+00:00),\n",
       " SLB_v1: (date=2023-09-27 02:26:40.570974982+00:00),\n",
       " SLB_v0: (date=2023-09-27 02:23:04.757768323+00:00),\n",
       " PXD_v2: (date=2023-09-27 02:36:05.408726228+00:00),\n",
       " PXD_v1: (date=2023-09-27 02:26:33.943345840+00:00),\n",
       " PXD_v0: (date=2023-09-27 02:22:58.175370382+00:00),\n",
       " PSX_v2: (date=2023-09-27 02:42:50.364743713+00:00),\n",
       " PSX_v1: (date=2023-09-27 02:26:35.778975550+00:00),\n",
       " PSX_v0: (date=2023-09-27 02:23:00.022232307+00:00),\n",
       " OKE_v2: (date=2023-09-27 02:42:27.439229401+00:00),\n",
       " OKE_v1: (date=2023-09-27 02:26:35.475166884+00:00),\n",
       " OKE_v0: (date=2023-09-27 02:22:59.797585503+00:00),\n",
       " MRO_v1: (date=2023-09-27 02:26:38.848555778+00:00),\n",
       " MRO_v0: (date=2023-09-27 02:23:03.547991502+00:00),\n",
       " KMI_v2: (date=2023-09-27 13:55:23.453678394+00:00),\n",
       " KMI_v1: (date=2023-09-27 02:26:40.622554808+00:00),\n",
       " KMI_v0: (date=2023-09-27 02:23:04.912898895+00:00),\n",
       " HAL_v1: (date=2023-09-27 02:26:39.685477994+00:00),\n",
       " HAL_v0: (date=2023-09-27 02:23:04.104266462+00:00),\n",
       " FANG_v2: (date=2023-09-27 02:35:11.487607770+00:00),\n",
       " FANG_v1: (date=2023-09-27 02:26:33.445289575+00:00),\n",
       " FANG_v0: (date=2023-09-27 02:22:57.693123744+00:00),\n",
       " DVN_v1: (date=2023-09-27 02:26:37.936182529+00:00),\n",
       " DVN_v0: (date=2023-09-27 02:23:02.413986022+00:00),\n",
       " CVX_v2: (date=2023-09-27 02:26:43.056291198+00:00),\n",
       " CVX_v1: (date=2023-09-27 02:23:07.055408249+00:00),\n",
       " CVX_v0: (date=2023-09-27 02:21:23.782315110+00:00),\n",
       " COP_v1: (date=2023-09-27 02:26:45.534948670+00:00),\n",
       " COP_v0: (date=2023-09-27 02:23:10.412711442+00:00)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | eval: false\n",
    "db_path = cfg.db.db_path\n",
    "library = cfg.db.library\n",
    "arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "v = arctic[library].list_versions()\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th>version</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">COP</th>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:45.534948670+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:23:10.412711442+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">CVX</th>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-27 02:26:43.056291198+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:23:07.055408249+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:21:23.782315110+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">DVN</th>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:37.936182529+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:23:02.413986022+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FANG</th>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-27 02:35:11.487607770+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:33.445289575+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:22:57.693123744+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">HAL</th>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:39.685477994+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:23:04.104266462+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">KMI</th>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-27 13:55:23.453678394+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:40.622554808+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:23:04.912898895+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MRO</th>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:38.848555778+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:23:03.547991502+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">OKE</th>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-27 02:42:27.439229401+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:35.475166884+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:22:59.797585503+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">PSX</th>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-27 02:42:50.364743713+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:35.778975550+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:23:00.022232307+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">PXD</th>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-27 02:36:05.408726228+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:33.943345840+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:22:58.175370382+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SLB</th>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-27 02:52:34.397110262+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:40.570974982+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:23:04.757768323+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">WMB</th>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:26:37.634894225+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:23:01.919661098+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">XOM</th>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-27 02:26:44.171414752+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-27 02:23:08.300722194+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-27 02:21:24.583990502+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          datetime\n",
       "ticker version                                    \n",
       "COP    1       2023-09-27 02:26:45.534948670+00:00\n",
       "       0       2023-09-27 02:23:10.412711442+00:00\n",
       "CVX    2       2023-09-27 02:26:43.056291198+00:00\n",
       "       1       2023-09-27 02:23:07.055408249+00:00\n",
       "       0       2023-09-27 02:21:23.782315110+00:00\n",
       "DVN    1       2023-09-27 02:26:37.936182529+00:00\n",
       "       0       2023-09-27 02:23:02.413986022+00:00\n",
       "FANG   2       2023-09-27 02:35:11.487607770+00:00\n",
       "       1       2023-09-27 02:26:33.445289575+00:00\n",
       "       0       2023-09-27 02:22:57.693123744+00:00\n",
       "HAL    1       2023-09-27 02:26:39.685477994+00:00\n",
       "       0       2023-09-27 02:23:04.104266462+00:00\n",
       "KMI    2       2023-09-27 13:55:23.453678394+00:00\n",
       "       1       2023-09-27 02:26:40.622554808+00:00\n",
       "       0       2023-09-27 02:23:04.912898895+00:00\n",
       "MRO    1       2023-09-27 02:26:38.848555778+00:00\n",
       "       0       2023-09-27 02:23:03.547991502+00:00\n",
       "OKE    2       2023-09-27 02:42:27.439229401+00:00\n",
       "       1       2023-09-27 02:26:35.475166884+00:00\n",
       "       0       2023-09-27 02:22:59.797585503+00:00\n",
       "PSX    2       2023-09-27 02:42:50.364743713+00:00\n",
       "       1       2023-09-27 02:26:35.778975550+00:00\n",
       "       0       2023-09-27 02:23:00.022232307+00:00\n",
       "PXD    2       2023-09-27 02:36:05.408726228+00:00\n",
       "       1       2023-09-27 02:26:33.943345840+00:00\n",
       "       0       2023-09-27 02:22:58.175370382+00:00\n",
       "SLB    2       2023-09-27 02:52:34.397110262+00:00\n",
       "       1       2023-09-27 02:26:40.570974982+00:00\n",
       "       0       2023-09-27 02:23:04.757768323+00:00\n",
       "WMB    1       2023-09-27 02:26:37.634894225+00:00\n",
       "       0       2023-09-27 02:23:01.919661098+00:00\n",
       "XOM    2       2023-09-27 02:26:44.171414752+00:00\n",
       "       1       2023-09-27 02:23:08.300722194+00:00\n",
       "       0       2023-09-27 02:21:24.583990502+00:00"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | eval: false\n",
    "df = (\n",
    "    pd.DataFrame(v)\n",
    "    .transpose()\n",
    "    .drop(columns=[1, 2])\n",
    "    .rename(columns={0: \"datetime\"})\n",
    "    .rename_axis(['ticker','version'])\n",
    "    .sort_index(level=[0,1], ascending=[True, False])\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# cfg = get_config()\n",
    "db_path = cfg.db.db_path\n",
    "library = cfg.db.library\n",
    "tickers = ['PXD', 'WMB', 'MRO', 'COP', 'SLB', 'KMI', 'XOM', 'DVN', 'PSX', 'OKE', 'FANG', 'CVX', 'HAL']\n",
    "ticker = \"PXD\"\n",
    "arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "q = QueryBuilder()\n",
    "q = q[q.event == Event.CROSS_TRADE.value]\n",
    "df = arctic[library].read(symbol=ticker, query_builder=q).data\n",
    "dates = df.index.date\n",
    "dates_str = df.index.to_series().dt.strftime(\"%Y-%m-%d\").values\n",
    "print(dates_str)\n",
    "\n",
    "\n",
    "# print({ticker:arctic[library].read(ticker).version for ticker in tickers})\n",
    "# df = arctic[library].read(ticker).data\n",
    "# display(df)\n",
    "# display(df.head())\n",
    "# display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_list_symbols(db_path, library) -> None:\n",
    "    \"\"\"List symbols in the arcticdb library.\"\"\"\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "    print(f\"Symbols in library {library}\")\n",
    "    print(arctic_library.list_symbols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_create_new_library(db_path, library) -> None:\n",
    "    \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic.create_library(library) \n",
    "    print(arctic[library])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "def arctic_list_libraries(db_path) -> None:\n",
    "    \"\"\"List arcticdb libraries\"\"\"\n",
    "\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    print(arctic.list_libraries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_delete_library(db_path, library) -> None:\n",
    "    \"\"\"Delete arcticdb library\"\"\"\n",
    "\n",
    "    user_input = input(\"Proceed by deleting this entire library? (y/n): \")\n",
    "    user_input = user_input.lower()\n",
    "    match user_input:\n",
    "        case \"y\":\n",
    "            pass\n",
    "        case \"n\":\n",
    "            sys.exit(0)\n",
    "        case _:\n",
    "            sys.exit(1)\n",
    "\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic.delete_library(library) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to print\")\n",
    "@click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "def arctic_read_symbol(db_path, library, ticker, start_date, end_date,\n",
    "):\n",
    "    \"\"\"Print df.head() and available columns for ticker in arcticdb library.\"\"\"\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    if start_date and end_date:\n",
    "        start_datetime = pd.Timestamp(f\"{start_date}T{NASDAQExchange.exchange_open}\")\n",
    "        end_datetime = pd.Timestamp(f\"{end_date}T{NASDAQExchange.exchange_close}\")\n",
    "        date_range = (start_datetime, end_datetime)\n",
    "        df = arctic_library.read(ticker, date_range=date_range).data\n",
    "    else:\n",
    "        df = arctic_library.read(ticker).data\n",
    "    \n",
    "    print(f\"Printing df.head() and df.tail() for ticker {ticker}\")\n",
    "    print(df.head())\n",
    "    print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to arctic again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to write to db\")\n",
    "@click.option(\"-s\", \"--start_date\", default=\"2020-01-01\", help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=\"2020-02-01\", help=\"end date\")\n",
    "def arctic_write_symbol(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    # if ticker in arctic_library.list_symbols():\n",
    "    #     print(\"warning - there is already data for ths ticker\")\n",
    "    #     user_input = input(\"Proceed by adding data to this symbol? (y/n): \")\n",
    "    #     user_input = user_input.lower()\n",
    "    #     match user_input:\n",
    "    #         case \"y\":\n",
    "    #             pass\n",
    "    #         case \"n\":\n",
    "    #             sys.exit(0)\n",
    "    #         case _:\n",
    "    #             sys.exit(1)\n",
    "\n",
    "    date_range = (start_date, end_date)\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "\n",
    "    arctic_library.write(symbol=ticker, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "def arctic_generate_jobs(csv_path, db_path, library, start_date, end_date):\n",
    "    ticker_date_dict = infer_ticker_to_date_range(csv_path)\n",
    "    with open('arctic_commands.txt', 'w') as f:\n",
    "        for ticker, (inferred_start_date, inferred_end_date) in ticker_date_dict.items():\n",
    "            # if date is None use the inferred date, otherwise use the CLI argument\n",
    "            start_date = start_date or inferred_start_date\n",
    "            end_date = end_date or inferred_end_date\n",
    "            f.write(f\"arctic_write_symbol --csv_path={csv_path} --db_path={db_path} --library={library} --ticker={ticker} --start_date={start_date} --end_date={end_date} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-z\",\n",
    "    \"--zip_path\",\n",
    "    default=\"/nfs/lobster_data/lobster_raw/2016\",\n",
    "    help=\"zip files path\",\n",
    ")\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\n",
    "    \"-e\", \"--etf\", default=None, help=\"restrict to subset specified by ETF members\"\n",
    ")\n",
    "def zip_generate_jobs(zip_path, csv_path, etf):\n",
    "    # ticker_date_dict = infer_ticker_to_ticker_path(zip_path)\n",
    "    # print(ticker_date_dict)\n",
    "    # if etf:\n",
    "    #     print(ETFMembers().mapping[etf])\n",
    "    #     ticker_date_dict = {\n",
    "    #         ticker: ticker_path\n",
    "    #         for ticker, ticker_path in ticker_date_dict.items()\n",
    "    #         if ticker in ETFMembers().mapping[etf] + [etf]\n",
    "    #     }\n",
    "    # print(ticker_date_dict)\n",
    "    ticker_dict = infer_ticker_dict(zip_path)\n",
    "    with open(\"zip_commands.txt\", \"w\") as f:\n",
    "        for ticker, dict_ in ticker_dict.items():\n",
    "            full = dict_[\"full\"]\n",
    "            ticker_till_end = dict_[\"ticker_till_end\"]\n",
    "            f.write(f\"mkdir {csv_path}/{ticker_till_end}\\n\")\n",
    "            f.write(f\"/nfs/home/nicolasp/usr/bin/7z x {full} -o{ticker_till_end}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to write to db\")\n",
    "@click.option(\"-s\", \"--start_date\", default=\"2020-01-01\", help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=\"2020-02-01\", help=\"end date\")\n",
    "def arctic_dump_all(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    if ticker in arctic_library.list_symbols():\n",
    "        print(\"warning - there is already data for ths ticker\")\n",
    "        user_input = input(\"Proceed by adding data to this symbol? (y/n): \")\n",
    "        user_input = user_input.lower()\n",
    "        match user_input:\n",
    "            case \"y\":\n",
    "                pass\n",
    "            case \"n\":\n",
    "                sys.exit(0)\n",
    "            case _:\n",
    "                sys.exit(1)\n",
    "\n",
    "    date_range = (start_date, end_date)\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "    print(df)\n",
    "\n",
    "    arctic_library.append(symbol=ticker, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
