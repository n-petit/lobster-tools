{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arctic\n",
    "\n",
    "> Arctic helper scripts and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp arctic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "\n",
    "import click\n",
    "from arcticdb import Arctic, LibraryOptions\n",
    "import hydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from lobster_tools.config import (\n",
    "    MainConfig,\n",
    "    Overrides,\n",
    "    NASDAQExchange,\n",
    "    ETFMembers,\n",
    "    register_configs,\n",
    "    get_config,\n",
    ")\n",
    "from lobster_tools.preprocessing import Data, Lobster, infer_ticker_to_date_range, infer_ticker_to_ticker_path, infer_ticker_dict\n",
    "import sys\n",
    "import pandas as pd\n",
    "from logging import Logger\n",
    "from datetime import date\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, wait\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `@hydra.main` decorator, `register_configs` must be called. If simply using a notebook or writing the CLIs with `click`, it is enough to use `get_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "register_configs()\n",
    "cfg = get_config(overrides=Overrides.full_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "CONTEXT_SETTINGS = dict(\n",
    "    help_option_names=[\"-h\", \"--help\"],\n",
    "    token_normalize_func=lambda x: x.lower() if isinstance(x, str) else x,\n",
    "    show_default=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @click.group()\n",
    "# @click.option('--debug/--no-debug', default=False)\n",
    "# @click.pass_context\n",
    "# def cli(ctx, debug):\n",
    "#     # ensure that ctx.obj exists and is a dict (in case `cli()` is called\n",
    "#     # by means other than the `if` block below)\n",
    "#     ctx.ensure_object(dict)\n",
    "\n",
    "#     ctx.obj['DEBUG'] = debug\n",
    "\n",
    "# @cli.command()\n",
    "# @click.pass_context\n",
    "# def sync(ctx):\n",
    "#     click.echo(f\"Debug is {'on' if ctx.obj['DEBUG'] else 'off'}\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     cli(obj={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# option_dict = {\n",
    "#     'db_path': click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\"),\n",
    "#     'library': click.option(\"-l\", \"--library\", default=cfg.db.db_path, help=\"Library name\"),\n",
    "# }\n",
    "\n",
    "# # Custom decorator to apply options based on a list of names\n",
    "# def apply_options(option_names):\n",
    "#     def decorator(f):\n",
    "#         for option_name in reversed(option_names):\n",
    "#             f = option_dict[option_name](f)\n",
    "#         return f\n",
    "#     return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'click' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# | export\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Define options in a dictionary\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m option_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdb_path\u001b[39m\u001b[39m'\u001b[39m: click\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39m-d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m--db_path\u001b[39m\u001b[39m\"\u001b[39m, default\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mdb\u001b[39m.\u001b[39mdb_path, help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDatabase path\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlibrary\u001b[39m\u001b[39m'\u001b[39m: click\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39m-l\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m--library\u001b[39m\u001b[39m\"\u001b[39m, default\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mdb\u001b[39m.\u001b[39mdb_path, help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLibrary name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Custom decorator to apply options based on a list of names\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brapid-01/nfs/home/nicolasp/home/code/lobster-tools/notebooks/07_arctic.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_options\u001b[39m(option_names):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'click' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "class Options:\n",
    "    def __init__(self) -> None:\n",
    "        self.db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\")\n",
    "        self.library = click.option(\"-l\", \"--library\", default=cfg.db.db_path, help=\"Library name\")\n",
    "\n",
    "def apply_options(options: list):\n",
    "    def decorator(f):\n",
    "        for option in reversed(options):\n",
    "            f = option(f)\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "@click.group()\n",
    "def arctic():\n",
    "    pass\n",
    "\n",
    "O = Options()\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path])\n",
    "def initdb(db_path):\n",
    "    print(f'Initialized the database {db_path}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library])\n",
    "def use_both(db_path, library):\n",
    "    print(f'Initialized the database {db_path} {library}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@arctic.command()\n",
    "def dropdb():\n",
    "    click.echo('Dropped the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "click_db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "click_library = click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "\n",
    "@click.group()\n",
    "def cool():\n",
    "    pass\n",
    "\n",
    "@cool.command()\n",
    "@click_db_path\n",
    "def initdb(db_path):\n",
    "    print(f'Initialized the database {db_path}')\n",
    "    # click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@cool.command()\n",
    "def dropdb():\n",
    "    click.echo('Dropped the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@click.group()\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.pass_context\n",
    "def nic(ctx, db_path, library):\n",
    "    ctx.ensure_object(dict)\n",
    "    ctx.obj['DB_PATH'] = db_path\n",
    "    ctx.obj['LIBRARY'] = library\n",
    "\n",
    "@nic.command()\n",
    "@click.pass_context\n",
    "def initdb(ctx):\n",
    "    db_path = ctx.obj['DB_PATH']\n",
    "    print(f'Initialized the database {db_path}')\n",
    "    click.echo(f'Initialized the database {db_path}')\n",
    "\n",
    "@nic.command()\n",
    "@click.pass_context\n",
    "def dropdb(ctx):\n",
    "    click.echo('Dropped the database')\n",
    "\n",
    "def nic_entrypoint():\n",
    "    nic(obj={}, auto_envvar_prefix='NIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_arctic_library(db_path, library):\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic_library = arctic[library]\n",
    "    return arctic_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# TODO: csv_path vs csv_files_path. think if this is a problem...maybe unify?\n",
    "class Options:\n",
    "    def __init__(self) -> None:\n",
    "        self.db_path = click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"Database path\")\n",
    "        self.library = click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"Library name\")\n",
    "        self.ticker = click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to print\")\n",
    "        self.start_date = click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "        self.end_date = click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "        self.csv_path = click.option(\"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\")\n",
    "        self.etf = click.option(\"--etf\", default=None, help=\"restrict to subset specified by ETF members\")\n",
    "        self.zip_path = click.option(\"-z\", \"--zip_path\", default=\"/nfs/lobster_data/lobster_raw/2016\", help=\"zip files path\")\n",
    "        self.max_workers = click.option(\"-m\", \"--max_workers\", default=20, help=\"max workers for parallelisation\")\n",
    "O = Options()\n",
    "\n",
    "def apply_options(options: list):\n",
    "    def decorator(f):\n",
    "        for option in reversed(options):\n",
    "            f = option(f)\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "@click.group(context_settings=CONTEXT_SETTINGS)\n",
    "def arctic():\n",
    "    pass\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path])\n",
    "def list_libraries(db_path) -> None:\n",
    "    \"\"\"List arcticdb libraries\"\"\"\n",
    "    arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "    print(arctic.list_libraries())\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library])\n",
    "def list_symbols(db_path, library) -> None:\n",
    "    \"\"\"List symbols in the arcticdb library.\"\"\"\n",
    "    arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "    print(arctic[library].list_symbols())\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library])\n",
    "def create_library(db_path, library) -> None:\n",
    "    \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "    arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "    arctic.create_library(library) \n",
    "    print(arctic[library])\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library])\n",
    "@click.confirmation_option(prompt='Are you sure you want to delete the entire library?')\n",
    "def delete_library(db_path, library) -> None:\n",
    "    \"\"\"Delete entire arcticdb library\"\"\"\n",
    "    arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "    arctic.delete_library(library) \n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library, O.ticker, O.start_date, O.end_date])\n",
    "def read(db_path, library, ticker, start_date, end_date,\n",
    "):\n",
    "    \"\"\"Read ticker and print head and tail.\"\"\"\n",
    "    arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "    if start_date and end_date:\n",
    "        start_datetime = pd.Timestamp(f\"{start_date}T{NASDAQExchange.exchange_open}\")\n",
    "        end_datetime = pd.Timestamp(f\"{end_date}T{NASDAQExchange.exchange_close}\")\n",
    "        date_range = (start_datetime, end_datetime)\n",
    "        df = arctic[library].read(ticker, date_range=date_range).data\n",
    "    else:\n",
    "        df = arctic[library].read(ticker).data\n",
    "    \n",
    "    print(f\"Printing df.head() and df.tail() for ticker {ticker}\")\n",
    "    print(df.head())\n",
    "    print(df.tail())\n",
    "\n",
    "def write_(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    \"\"\"Preprocess and write ticker to database.\"\"\"\n",
    "    arctic = Arctic(f\"lmdb://{db_path}\")\n",
    "\n",
    "    date_range = (start_date, end_date)\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "    print(f\"head of ticker {ticker}\")\n",
    "    print(df.head())\n",
    "\n",
    "    arctic[library].write(symbol=ticker, data=df)\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library, O.csv_path, O.ticker, O.start_date, O.end_date])\n",
    "def write(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    \"\"\"Preprocess and write ticker to database.\"\"\"\n",
    "    write(\n",
    "    db_path=db_path,\n",
    "    library=library,\n",
    "    csv_path=csv_path,\n",
    "    ticker=ticker,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    )\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library, O.csv_path, O.start_date, O.end_date])\n",
    "def generate_jobs(db_path, library, csv_path, start_date, end_date):\n",
    "    ticker_date_dict = infer_ticker_to_date_range(csv_path)\n",
    "    with open('arctic_commands.txt', 'w') as f:\n",
    "        for ticker, (inferred_start_date, inferred_end_date) in ticker_date_dict.items():\n",
    "            # if date is None use the inferred date, otherwise use the CLI argument\n",
    "            start_date = start_date or inferred_start_date\n",
    "            end_date = end_date or inferred_end_date\n",
    "            f.write(f\"arctic write --csv_path={csv_path} --db_path={db_path} --library={library} --ticker={ticker} --start_date={start_date} --end_date={end_date} \\n\")\n",
    "\n",
    "def sleepy(csv_path, folder_info):\n",
    "    time.sleep(5)\n",
    "    print(csv_path, folder_info.full)\n",
    "\n",
    "def extract_7z(input_path, output_path):\n",
    "    try:\n",
    "        subprocess.run([\"7z\", \"x\", input_path, f\"-o{output_path}\"], check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.zip_path, O.csv_path, O.etf, O.max_workers])\n",
    "def zip(zip_path, csv_path, etf, max_workers):\n",
    "    folder_infos = infer_ticker_dict(zip_path)\n",
    "\n",
    "    # filter first\n",
    "    if etf:\n",
    "        def in_etf(folder_info):\n",
    "            return folder_info.ticker in ETFMembers().mapping[etf] + [etf]\n",
    "        folder_infos = list(filter(in_etf, folder_infos))\n",
    "\n",
    "    # commands = [f\"mkdir -p {csv_path}/{folder_info.ticker_till_end}\\n\"\n",
    "    #             for folder_info in folder_infos]\n",
    "\n",
    "    # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "    #     [f.write(command) for command in commands]\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # outputs_dirs = [folder_info.ticker_till_end for folder_info in folder_infos]\n",
    "        futures = [\n",
    "            executor.submit(os.mkdir, path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "            for folder_info in folder_infos\n",
    "        ]\n",
    "        wait(futures)\n",
    "        futures = [\n",
    "            executor.submit(extract_7z, input_path=folder_info.full, output_path=f\"{csv_path}/{folder_info.ticker_till_end}\")\n",
    "            for folder_info in folder_infos\n",
    "        ]\n",
    "\n",
    "\n",
    "        # for folder_info in folder_infos:\n",
    "        #     # print(folder_info.ticker)\n",
    "        #     f.write(f\"examle: mkdir {csv_path}/{folder_info.ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "    # ticker_date_dict = infer_ticker_to_ticker_path(zip_path)\n",
    "    # print(ticker_date_dict)\n",
    "    # if etf:\n",
    "    #     print(ETFMembers().mapping[etf])\n",
    "    #     ticker_date_dict = {\n",
    "    #         ticker: ticker_path\n",
    "    #         for ticker, ticker_path in ticker_date_dict.items()\n",
    "    #         if ticker in ETFMembers().mapping[etf] + [etf]\n",
    "    #     }\n",
    "    # print(ticker_date_dict)\n",
    "    # ticker_dict = infer_ticker_dict(zip_path)\n",
    "    # with open(\"zip_commands.txt\", \"w\") as f:\n",
    "    #     for ticker, dict_ in ticker_dict.items():\n",
    "    #         full = dict_[\"full\"]\n",
    "    #         ticker_till_end = dict_[\"ticker_till_end\"]\n",
    "    #         f.write(f\"mkdir {csv_path}/{ticker_till_end}\\n\")\n",
    "    #         f.write(f\"/nfs/home/nicolasp/usr/bin/7z x {full} -o{ticker_till_end}\\n\")\n",
    "\n",
    "\n",
    "@arctic.command()\n",
    "@apply_options([O.db_path, O.library, O.csv_path, O.max_workers])\n",
    "def dump(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    max_workers,\n",
    "):\n",
    "    \"\"\"Dump all csv to arctic_db inferring start and end date from folder.\"\"\"\n",
    "    folder_infos = infer_ticker_dict(csv_path)\n",
    "    print(folder_infos)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(write_, csv_path=csv_path, db_path=db_path, library=library, ticker=folder_info.ticker, start_date=folder_info.start_date, end_date=folder_info.end_date)\n",
    "            for folder_info in folder_infos\n",
    "        ]\n",
    "        for future in futures:\n",
    "            print(future.result())\n",
    "    print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_list_symbols(db_path, library) -> None:\n",
    "    \"\"\"List symbols in the arcticdb library.\"\"\"\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "    print(f\"Symbols in library {library}\")\n",
    "    print(arctic_library.list_symbols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_create_new_library(db_path, library) -> None:\n",
    "    \"\"\"Create a blank new arcticdb library.\"\"\"\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic.create_library(library) \n",
    "    print(arctic[library])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "def arctic_list_libraries(db_path) -> None:\n",
    "    \"\"\"List arcticdb libraries\"\"\"\n",
    "\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    print(arctic.list_libraries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "def arctic_delete_library(db_path, library) -> None:\n",
    "    \"\"\"Delete arcticdb library\"\"\"\n",
    "\n",
    "    user_input = input(\"Proceed by deleting this entire library? (y/n): \")\n",
    "    user_input = user_input.lower()\n",
    "    match user_input:\n",
    "        case \"y\":\n",
    "            pass\n",
    "        case \"n\":\n",
    "            sys.exit(0)\n",
    "        case _:\n",
    "            sys.exit(1)\n",
    "\n",
    "    conn = f\"lmdb://{db_path}\"\n",
    "    arctic = Arctic(conn)\n",
    "    arctic.delete_library(library) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to print\")\n",
    "@click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "def arctic_read_symbol(db_path, library, ticker, start_date, end_date,\n",
    "):\n",
    "    \"\"\"Print df.head() and available columns for ticker in arcticdb library.\"\"\"\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    if start_date and end_date:\n",
    "        start_datetime = pd.Timestamp(f\"{start_date}T{NASDAQExchange.exchange_open}\")\n",
    "        end_datetime = pd.Timestamp(f\"{end_date}T{NASDAQExchange.exchange_close}\")\n",
    "        date_range = (start_datetime, end_datetime)\n",
    "        df = arctic_library.read(ticker, date_range=date_range).data\n",
    "    else:\n",
    "        df = arctic_library.read(ticker).data\n",
    "    \n",
    "    print(f\"Printing df.head() and df.tail() for ticker {ticker}\")\n",
    "    print(df.head())\n",
    "    print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to arctic again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to write to db\")\n",
    "@click.option(\"-s\", \"--start_date\", default=\"2020-01-01\", help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=\"2020-02-01\", help=\"end date\")\n",
    "def arctic_write_symbol(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    # if ticker in arctic_library.list_symbols():\n",
    "    #     print(\"warning - there is already data for ths ticker\")\n",
    "    #     user_input = input(\"Proceed by adding data to this symbol? (y/n): \")\n",
    "    #     user_input = user_input.lower()\n",
    "    #     match user_input:\n",
    "    #         case \"y\":\n",
    "    #             pass\n",
    "    #         case \"n\":\n",
    "    #             sys.exit(0)\n",
    "    #         case _:\n",
    "    #             sys.exit(1)\n",
    "\n",
    "    date_range = (start_date, end_date)\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "\n",
    "    arctic_library.write(symbol=ticker, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-s\", \"--start_date\", default=None, help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=None, help=\"end date\")\n",
    "def arctic_generate_jobs(csv_path, db_path, library, start_date, end_date):\n",
    "    ticker_date_dict = infer_ticker_to_date_range(csv_path)\n",
    "    with open('arctic_commands.txt', 'w') as f:\n",
    "        for ticker, (inferred_start_date, inferred_end_date) in ticker_date_dict.items():\n",
    "            # if date is None use the inferred date, otherwise use the CLI argument\n",
    "            start_date = start_date or inferred_start_date\n",
    "            end_date = end_date or inferred_end_date\n",
    "            f.write(f\"arctic_write_symbol --csv_path={csv_path} --db_path={db_path} --library={library} --ticker={ticker} --start_date={start_date} --end_date={end_date} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-z\",\n",
    "    \"--zip_path\",\n",
    "    default=\"/nfs/lobster_data/lobster_raw/2016\",\n",
    "    help=\"zip files path\",\n",
    ")\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\n",
    "    \"-e\", \"--etf\", default=None, help=\"restrict to subset specified by ETF members\"\n",
    ")\n",
    "def zip_generate_jobs(zip_path, csv_path, etf):\n",
    "    # ticker_date_dict = infer_ticker_to_ticker_path(zip_path)\n",
    "    # print(ticker_date_dict)\n",
    "    # if etf:\n",
    "    #     print(ETFMembers().mapping[etf])\n",
    "    #     ticker_date_dict = {\n",
    "    #         ticker: ticker_path\n",
    "    #         for ticker, ticker_path in ticker_date_dict.items()\n",
    "    #         if ticker in ETFMembers().mapping[etf] + [etf]\n",
    "    #     }\n",
    "    # print(ticker_date_dict)\n",
    "    ticker_dict = infer_ticker_dict(zip_path)\n",
    "    with open(\"zip_commands.txt\", \"w\") as f:\n",
    "        for ticker, dict_ in ticker_dict.items():\n",
    "            full = dict_[\"full\"]\n",
    "            ticker_till_end = dict_[\"ticker_till_end\"]\n",
    "            f.write(f\"mkdir {csv_path}/{ticker_till_end}\\n\")\n",
    "            f.write(f\"/nfs/home/nicolasp/usr/bin/7z x {full} -o{ticker_till_end}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "# | code-fold: true\n",
    "@click.command(context_settings=CONTEXT_SETTINGS)\n",
    "@click.option(\n",
    "    \"-c\", \"--csv_path\", default=cfg.data_config.csv_files_path, help=\"csv files path\"\n",
    ")\n",
    "@click.option(\"-d\", \"--db_path\", default=cfg.db.db_path, help=\"database path\")\n",
    "@click.option(\"-l\", \"--library\", default=cfg.db.library, help=\"library name\")\n",
    "@click.option(\"-t\", \"--ticker\", required=True, help=\"ticker to write to db\")\n",
    "@click.option(\"-s\", \"--start_date\", default=\"2020-01-01\", help=\"start date\")\n",
    "@click.option(\"-e\", \"--end_date\", default=\"2020-02-01\", help=\"end date\")\n",
    "def arctic_dump_all(\n",
    "    db_path,\n",
    "    library,\n",
    "    csv_path,\n",
    "    ticker,\n",
    "    start_date,\n",
    "    end_date,\n",
    "):\n",
    "    arctic_library = get_arctic_library(db_path=db_path, library=library)\n",
    "\n",
    "    if ticker in arctic_library.list_symbols():\n",
    "        print(\"warning - there is already data for ths ticker\")\n",
    "        user_input = input(\"Proceed by adding data to this symbol? (y/n): \")\n",
    "        user_input = user_input.lower()\n",
    "        match user_input:\n",
    "            case \"y\":\n",
    "                pass\n",
    "            case \"n\":\n",
    "                sys.exit(0)\n",
    "            case _:\n",
    "                sys.exit(1)\n",
    "\n",
    "    date_range = (start_date, end_date)\n",
    "    data = Data(\n",
    "        directory_path=csv_path,\n",
    "        ticker=ticker,\n",
    "        date_range=date_range,\n",
    "        aggregate_duplicates=False,\n",
    "    )\n",
    "    lobster = Lobster(data=data)\n",
    "    df = pd.concat([lobster.messages, lobster.book], axis=1)\n",
    "    print(df)\n",
    "\n",
    "    arctic_library.append(symbol=ticker, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
