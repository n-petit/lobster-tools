[
  {
    "objectID": "data_downloading.html",
    "href": "data_downloading.html",
    "title": "Data downloading",
    "section": "",
    "text": "Code\nCONTEXT_SETTINGS = dict(\n    help_option_names=[\"-h\", \"--help\"],\n    token_normalize_func=lambda x: x.lower() if isinstance(x, str) else x,\n    show_default=True,\n)\n\n\n@click.command(context_settings=CONTEXT_SETTINGS)\n@click.option(\"-t\", \"--ticker\", default=cfg.sample_data.ticker, help=\"ticker\")\n@click.option(\"-l\", \"--levels\", default=cfg.sample_data.levels, help=\"number of levels\")\ndef get_sample_data(\n    ticker: Literal[\"AMZN\", \"AAPL\", \"GOOG\", \"INTC\", \"MSFT\"],\n    levels: Literal[1, 5, 10],\n):\n    \"\"\"Download and extract sample data from LOBSTER website.\"\"\"\n    SAMPLE_DATA_DATE = \"2012-06-21\"\n    url = f\"https://lobsterdata.com/info/sample/LOBSTER_SampleFile_{ticker}_{SAMPLE_DATA_DATE}_{levels}.zip\"\n    print(f\"Downloading data from {url}\")\n\n    default_directory_name = (\n        f\"data/{ticker}_{SAMPLE_DATA_DATE}_{SAMPLE_DATA_DATE}_{levels}\"\n    )\n    output_dir = os.path.join(os.getcwd(), default_directory_name)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    response = requests.get(url)\n    if response.status_code != 200:\n        print(f\"Failed to download data. HTTP Status Code: {response.status_code}\")\n        return\n\n    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n        zip_ref.extractall(output_dir)\n\n\n\n\n\n\n &lt;Command get-sample-data&gt; (*args:Any, **kwargs:Any)\n\nDownload and extract sample data from LOBSTER website.\nTo download and extract the Lobster sample data, run the following command in the directory of your choice.\n$ get_sample_data --ticker=GOOG --levels=10"
  },
  {
    "objectID": "etf_pipeline_s3.html",
    "href": "etf_pipeline_s3.html",
    "title": "ETF Pipeline2 using s3",
    "section": "",
    "text": "cfg = get_config(overrides=Overrides.full_server)\ndirectory_path = cfg.data_config.csv_files_path\netfs = cfg.universe.etfs\nequities = cfg.universe.equities\n\nmarkouts = cfg.hyperparameters.markouts\nfinest_resample = cfg.hyperparameters.finest_resample\nmax_markout = cfg.hyperparameters.max_markout\n\ndate_range = dt.date(2021, 1, 6), dt.datetime(2021, 1, 6, 16, 0)\n# temp\netfs = [\"SPY\"]\nequities = etf_to_equities[\"SPY\"]\ntolerances = [\"250us\", \"500us\"]\nmarkouts = [\"30S\", \"1min\", \"2min\", \"4min\"]\nfinest_resample = \"30S\"\nmax_markout = \"4min\"\ndates = NASDAQExchange().trading_days"
  },
  {
    "objectID": "etf_pipeline_s3.html#poolprocess-for-many-dates",
    "href": "etf_pipeline_s3.html#poolprocess-for-many-dates",
    "title": "ETF Pipeline2 using s3",
    "section": "PoolProcess for many dates",
    "text": "PoolProcess for many dates\n\ndef dump_to_pickle(date):\n    \"This is not useful..right?\"\n    print(etfs)\n    print(equities)\n    equity_executions = read_multiple_tickers_executions(\n        tickers=equities, date_range=date\n    )\n    etf_executions = read_multiple_tickers_executions(tickers=etfs, date_range=date)\n\n    etf_executions_neighbors = add_neighbors(\n        etf_executions=etf_executions,\n        equity_executions=equity_executions,\n        tolerances=tolerances,\n    )\n    etf_executions_features = append_features(\n        etf_executions=etf_executions_neighbors, equity_executions=equity_executions\n    )\n    etf_executions_features_pnl = add_pnl_columns(etf_executions_features)\n\n    # for now just one etf\n\n    # if want to pickle\n    # assert len(etfs) == 1\n    # etf_name = etfs[0]\n    date_str = dt.datetime.combine(date[0], dt.datetime.min.time()).strftime(\"%Y-%m-%d\")\n    # etf_executions_features_pnl.to_pickle(f\"../features/{etf_name}_{date_str}\")\n    print(f\"done {date_str}\")\n    return etf_executions_features_pnl\n\n\ndef filter_to_existing_equities(equities):\n    existing_symbols = arctic_library.list_symbols()\n    return list(set(equities).intersection(set(existing_symbols)))\n\n\nequities = filter_to_existing_equities(equities)\nprint(equities)\n\n\nfrom concurrent.futures import ProcessPoolExecutor, as_completed, wait\n\ndates = NASDAQExchange().trading_days\ndates = dates\ndates = [date_to_tuple(x) for x in dates]\ndates = dates[:70]\n\nwith ProcessPoolExecutor(max_workers=70) as executor:\n    dfs = list(executor.map(dump_to_pickle, dates))\n\ndf = pd.concat(dfs)\ndf.to_pickle(\"../features/SPY_all_10\")\n\n\ndf.to_pickle(\"../features/XLC_all\")\n\n\nreturns = compute_log_returns(df.mid)\nreturns\n\n\ncompute_ofi(df)\n\n\nX = compute_ofi(df)\nY = compute_log_returns(df.mid)\nY = Y.fillna(0)\nX, Y = restrict_common_index(X, Y)\n\nrun_regressions(X, Y)\n\n\nLoad with ArcticDB\n\nclosing_prices = df.resample(\"D\").transform(\"last\").price\n\n\ndf[\"closing_price\"] = closing_prices\n\n\ndf.eval(\"pnl_to_close = (closing_price - price) * 1e4 * direction\", inplace=True)\ndf.eval(\"hit_ratio = (pnl_to_close &gt; 0)\", inplace=True)\n\n\nsummary_statistics = df.groupby(by=\"_500us_num_trades\").agg(\n    ppt_mean=(\"pnl_to_close\", \"mean\"),\n    ppt_std=(\"pnl_to_close\", \"std\"),\n    hit_ratio=(\"hit_ratio\", \"mean\"),\n)\n\n\nimport matplotlib.pyplot as plt\n\nfor col in summary_statistics:\n    summary_statistics[col].plot(title=col, kind=\"bar\")\n    plt.show()\n\n\n\nData analysis\n\ndf = pd.read_pickle(\"../features/XLC_all\")\n# df = df.rename(columns={'pnl_to_close':'pnl_to_close_bps', 'hit_ratio':'hit'})\n\n\ndf = sparse_to_dense(df)\n\n\ndf = quick_marginalize(df)\n\n\ndf._500us_distinct_tickers[df._500us_distinct_tickers != 0].hist(bins=25)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndef add_decile_tags(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = df.copy(deep=True)\n    feature_columns = [\n        col for col in df.columns if (col.startswith(\"_\") and (\"neighbors\" not in col))\n    ]\n\n    for col in feature_columns:\n        df[col + \"_decile\"] = 0\n        mask = df[col] != 0\n        df.loc[mask, col + \"_decile\"] = 1 + pd.qcut(\n            df.loc[mask, col], 10, labels=False, duplicates=\"drop\"\n        )\n\n    return df\n\n\ndf = add_decile_tags(df)\n\n\ndf._500us_notional_decile.hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndef pnl_groupby_summaries(df: pd.DataFrame) -&gt; None:\n    decile_columns = df.filter(regex=\"decile\")\n    for feature in decile_columns:\n        display(\n            df.groupby(feature).agg(\n                ppt_mean=(\"pnl_to_close\", \"mean\"),\n                ppt_std=(\"pnl_to_close\", \"std\"),\n                hit_ratio=(\"hit_ratio\", \"mean\"),\n            )\n        )\n\n\npnl_groupby_summaries(df)\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_ss_bf_decile\n\n\n\n\n\n\n\n0\n-0.643133\n61.770854\n0.492517\n\n\n1\n-0.812760\n61.355928\n0.488561\n\n\n2\n-4.045905\n56.422537\n0.475604\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_ss_af_decile\n\n\n\n\n\n\n\n0\n-0.558275\n61.898632\n0.493068\n\n\n1\n-1.550585\n60.343574\n0.483882\n\n\n2\n-3.307661\n53.813348\n0.465782\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_os_bf_decile\n\n\n\n\n\n\n\n0\n-0.865763\n61.721163\n0.490340\n\n\n1\n3.411774\n61.212043\n0.526734\n\n\n2\n1.061222\n48.415640\n0.528889\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_os_af_decile\n\n\n\n\n\n\n\n0\n-0.800908\n61.803316\n0.490813\n\n\n1\n2.242885\n58.917906\n0.519649\n\n\n2\n6.493440\n62.372479\n0.521565\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_ss_bf_decile\n\n\n\n\n\n\n\n0\n-0.643133\n61.770854\n0.492517\n\n\n1\n-1.607361\n58.570349\n0.485398\n\n\n2\n-0.202579\n60.670307\n0.491769\n\n\n3\n0.459644\n62.920067\n0.494993\n\n\n4\n-0.878494\n59.112991\n0.489934\n\n\n5\n0.524772\n60.142107\n0.498243\n\n\n6\n-0.307312\n60.133286\n0.492372\n\n\n7\n-1.883595\n60.466047\n0.484558\n\n\n8\n-0.396542\n63.332272\n0.497536\n\n\n9\n-1.893847\n63.643417\n0.474785\n\n\n10\n-2.946519\n62.792145\n0.471998\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_ss_af_decile\n\n\n\n\n\n\n\n0\n-0.558275\n61.898632\n0.493068\n\n\n1\n-2.834461\n59.781180\n0.479300\n\n\n2\n-1.676672\n59.687747\n0.492979\n\n\n3\n-0.519953\n62.552460\n0.487992\n\n\n4\n-1.882139\n59.100325\n0.485865\n\n\n5\n0.651685\n58.987017\n0.502371\n\n\n6\n-1.792374\n60.077847\n0.489332\n\n\n7\n-1.196554\n60.304223\n0.485316\n\n\n8\n-1.320801\n58.007007\n0.473067\n\n\n9\n-3.139148\n60.940594\n0.468450\n\n\n10\n-2.519271\n61.252315\n0.466683\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_os_bf_decile\n\n\n\n\n\n\n\n0\n-0.865763\n61.721163\n0.490340\n\n\n1\n1.879327\n59.058280\n0.528149\n\n\n2\n-1.010121\n57.983218\n0.498677\n\n\n3\n2.351129\n59.422650\n0.525898\n\n\n4\n1.636776\n59.863851\n0.522950\n\n\n5\n1.631330\n61.765873\n0.505833\n\n\n6\n2.777754\n60.737137\n0.524187\n\n\n7\n4.157988\n64.720751\n0.530263\n\n\n8\n5.333960\n61.155336\n0.529869\n\n\n9\n8.037108\n64.221179\n0.558027\n\n\n10\n6.992994\n60.732431\n0.543786\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_os_af_decile\n\n\n\n\n\n\n\n0\n-0.800908\n61.803316\n0.490813\n\n\n1\n1.059336\n55.607049\n0.505418\n\n\n2\n-1.294143\n58.499800\n0.499650\n\n\n3\n6.056673\n60.348068\n0.555148\n\n\n4\n1.044281\n63.378526\n0.513116\n\n\n5\n1.258529\n57.722351\n0.511701\n\n\n6\n-1.118839\n59.609804\n0.491517\n\n\n7\n1.821312\n60.444207\n0.517127\n\n\n8\n4.910002\n55.860535\n0.531212\n\n\n9\n3.334583\n58.881109\n0.536351\n\n\n10\n6.098730\n58.564813\n0.535583\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_ss_bf_decile\n\n\n\n\n\n\n\n0\n-0.643133\n61.770854\n0.492517\n\n\n1\n-0.238655\n59.964180\n0.492070\n\n\n2\n-0.698099\n60.716125\n0.489884\n\n\n3\n-1.615163\n61.585581\n0.479361\n\n\n4\n-0.305285\n62.194145\n0.500743\n\n\n5\n-1.533414\n62.873323\n0.481745\n\n\n6\n-4.590517\n65.550885\n0.462319\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_ss_af_decile\n\n\n\n\n\n\n\n0\n-0.558275\n61.898632\n0.493068\n\n\n1\n-1.224653\n59.724444\n0.487099\n\n\n2\n-1.687725\n61.219458\n0.480627\n\n\n3\n-1.162881\n59.974689\n0.488891\n\n\n4\n-2.528587\n59.858981\n0.470873\n\n\n5\n-4.276536\n61.198019\n0.460205\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_os_bf_decile\n\n\n\n\n\n\n\n0\n-0.865763\n61.721163\n0.490340\n\n\n1\n3.258337\n59.520706\n0.525802\n\n\n2\n2.942281\n61.704734\n0.522799\n\n\n3\n3.340639\n64.642508\n0.530925\n\n\n4\n4.895420\n67.398296\n0.534227\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_os_af_decile\n\n\n\n\n\n\n\n0\n-0.800908\n61.803316\n0.490813\n\n\n1\n2.047822\n58.137372\n0.515397\n\n\n2\n1.759318\n61.019819\n0.535476\n\n\n3\n5.163472\n58.253835\n0.548089\n\n\n4\n3.474080\n64.121195\n0.519828\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_ss_bf_decile\n\n\n\n\n\n\n\n0\n-0.631044\n61.876847\n0.492849\n\n\n1\n-0.779497\n61.237509\n0.488243\n\n\n2\n-2.715008\n54.799858\n0.481268\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_ss_af_decile\n\n\n\n\n\n\n\n0\n-0.530377\n62.050068\n0.493392\n\n\n1\n-1.256457\n60.183095\n0.485887\n\n\n2\n-3.746779\n55.287757\n0.466834\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_os_bf_decile\n\n\n\n\n\n\n\n0\n-0.955352\n61.775687\n0.489621\n\n\n1\n3.367115\n60.746194\n0.525280\n\n\n2\n2.646373\n46.972708\n0.543316\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_os_af_decile\n\n\n\n\n\n\n\n0\n-0.856681\n61.841239\n0.490261\n\n\n1\n2.167623\n59.501224\n0.519928\n\n\n2\n2.670279\n52.618826\n0.494295\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_ss_bf_decile\n\n\n\n\n\n\n\n0\n-0.631044\n61.876847\n0.492849\n\n\n1\n-0.919583\n57.741603\n0.489872\n\n\n2\n-0.322816\n63.998686\n0.494706\n\n\n3\n-0.436388\n62.511782\n0.488534\n\n\n4\n0.291303\n59.657833\n0.498777\n\n\n5\n-0.378357\n59.337204\n0.490922\n\n\n6\n-0.268779\n59.909848\n0.494324\n\n\n7\n-1.831407\n59.007541\n0.477963\n\n\n8\n0.530948\n61.526356\n0.495012\n\n\n9\n-2.157468\n62.839037\n0.477545\n\n\n10\n-3.459582\n61.809048\n0.470608\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_ss_af_decile\n\n\n\n\n\n\n\n0\n-0.530377\n62.050068\n0.493392\n\n\n1\n-2.413272\n59.922772\n0.486446\n\n\n2\n-1.585356\n59.684135\n0.486037\n\n\n3\n-1.609443\n60.689192\n0.487798\n\n\n4\n-0.297070\n58.898632\n0.497876\n\n\n5\n-0.319447\n59.990132\n0.493737\n\n\n6\n-0.547390\n59.536933\n0.495046\n\n\n7\n-0.580197\n59.334533\n0.486146\n\n\n8\n-2.790066\n58.038621\n0.471752\n\n\n9\n-1.739835\n60.676215\n0.477906\n\n\n10\n-2.677771\n61.197072\n0.460859\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_os_bf_decile\n\n\n\n\n\n\n\n0\n-0.955352\n61.775687\n0.489621\n\n\n1\n0.878289\n59.304286\n0.512208\n\n\n2\n0.531704\n57.798467\n0.518673\n\n\n3\n1.218857\n59.089928\n0.514667\n\n\n4\n2.426727\n59.789895\n0.528364\n\n\n5\n2.835986\n60.323044\n0.522732\n\n\n6\n2.523363\n61.007708\n0.520747\n\n\n7\n5.998150\n61.729782\n0.536164\n\n\n8\n3.395071\n61.110598\n0.523200\n\n\n9\n7.043703\n62.970041\n0.540587\n\n\n10\n6.623635\n60.445671\n0.540373\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_os_af_decile\n\n\n\n\n\n\n\n0\n-0.856681\n61.841239\n0.490261\n\n\n1\n0.557025\n56.218909\n0.497423\n\n\n2\n0.529107\n60.984008\n0.505389\n\n\n3\n1.936321\n58.918019\n0.532927\n\n\n4\n0.329554\n64.498620\n0.516046\n\n\n5\n0.933136\n58.690229\n0.501524\n\n\n6\n2.259697\n57.958455\n0.523492\n\n\n7\n3.354150\n59.645778\n0.528530\n\n\n8\n2.248975\n57.914844\n0.517985\n\n\n9\n3.337420\n58.288166\n0.535091\n\n\n10\n6.366916\n59.017748\n0.531927\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_ss_bf_decile\n\n\n\n\n\n\n\n0\n-0.631044\n61.876847\n0.492849\n\n\n1\n-0.454989\n60.118915\n0.490237\n\n\n2\n-0.418848\n61.038676\n0.493222\n\n\n3\n-0.768238\n60.872930\n0.488464\n\n\n4\n0.036106\n61.070397\n0.493534\n\n\n5\n-0.350965\n62.047722\n0.493732\n\n\n6\n-1.308659\n60.814645\n0.486699\n\n\n7\n-4.521073\n62.717787\n0.459659\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_ss_af_decile\n\n\n\n\n\n\n\n0\n-0.530377\n62.050068\n0.493392\n\n\n1\n-1.268697\n59.373215\n0.489180\n\n\n2\n-1.224155\n59.625411\n0.481525\n\n\n3\n-1.022584\n61.107965\n0.487728\n\n\n4\n-0.681189\n60.528049\n0.489369\n\n\n5\n-1.329320\n59.330652\n0.482285\n\n\n6\n-4.449813\n60.671548\n0.452751\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_os_bf_decile\n\n\n\n\n\n\n\n0\n-0.955352\n61.775687\n0.489621\n\n\n1\n3.234752\n58.724422\n0.524910\n\n\n2\n2.877069\n60.792566\n0.518821\n\n\n3\n2.644395\n63.800922\n0.530985\n\n\n4\n4.042088\n62.450491\n0.529729\n\n\n5\n4.624610\n66.430469\n0.532577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_os_af_decile\n\n\n\n\n\n\n\n0\n-0.856681\n61.841239\n0.490261\n\n\n1\n2.001831\n58.755650\n0.515055\n\n\n2\n1.804954\n60.104695\n0.520194\n\n\n3\n2.274179\n60.081775\n0.534969\n\n\n4\n4.023642\n61.206150\n0.529127\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_decile\n\n\n\n\n\n\n\n0\n-0.624363\n62.073346\n0.492292\n\n\n1\n-0.771295\n59.199156\n0.491436\n\n\n2\n-0.305855\n59.160893\n0.498517\n\n\n3\n-0.780987\n60.360073\n0.488227\n\n\n4\n-0.786848\n59.707173\n0.494127\n\n\n5\n-0.901237\n61.304476\n0.486110\n\n\n6\n-1.394785\n62.053705\n0.487051\n\n\n7\n-1.862932\n63.895073\n0.482005\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_ss_decile\n\n\n\n\n\n\n\n0\n-0.583798\n61.985974\n0.492952\n\n\n1\n-0.622676\n59.481844\n0.491386\n\n\n2\n-0.508815\n60.361324\n0.493580\n\n\n3\n-0.882116\n59.136526\n0.487251\n\n\n4\n-0.851718\n59.018502\n0.494147\n\n\n5\n-0.926985\n61.954130\n0.488736\n\n\n6\n-1.763713\n62.105273\n0.480357\n\n\n7\n-4.007048\n63.285106\n0.462489\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_os_decile\n\n\n\n\n\n\n\n0\n-0.929842\n61.809814\n0.489757\n\n\n1\n2.374314\n58.810657\n0.516661\n\n\n2\n2.998964\n60.442060\n0.527387\n\n\n3\n3.344102\n61.183125\n0.545944\n\n\n4\n3.038497\n60.106806\n0.535006\n\n\n5\n4.635688\n67.032428\n0.521683\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_af_decile\n\n\n\n\n\n\n\n0\n-0.622813\n61.971757\n0.492287\n\n\n1\n-0.817132\n59.319436\n0.492457\n\n\n2\n-1.253124\n59.864659\n0.490510\n\n\n3\n0.021069\n61.741601\n0.492768\n\n\n4\n-1.182568\n60.471182\n0.488137\n\n\n5\n-1.603393\n59.761835\n0.486254\n\n\n6\n-2.526486\n61.173489\n0.473428\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_num_trades_bf_decile\n\n\n\n\n\n\n\n0\n-0.664331\n61.829221\n0.492096\n\n\n1\n-0.474160\n59.376197\n0.493041\n\n\n2\n-0.504745\n60.225088\n0.492815\n\n\n3\n-1.437510\n61.429477\n0.485275\n\n\n4\n0.241796\n61.766851\n0.500943\n\n\n5\n-1.043944\n62.469156\n0.490358\n\n\n6\n-1.752811\n67.079999\n0.483199\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_decile\n\n\n\n\n\n\n\n0\n-0.624363\n62.073346\n0.492292\n\n\n1\n-1.761678\n60.174418\n0.483405\n\n\n2\n-0.587186\n59.107845\n0.498931\n\n\n3\n-0.562783\n61.054809\n0.496260\n\n\n4\n-1.172567\n59.212871\n0.487761\n\n\n5\n0.328366\n59.735689\n0.500334\n\n\n6\n-0.805646\n60.452556\n0.490416\n\n\n7\n-1.150675\n58.426758\n0.487176\n\n\n8\n-0.985196\n60.300657\n0.495292\n\n\n9\n-0.796025\n60.643618\n0.483119\n\n\n10\n-1.383282\n63.000604\n0.483453\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_ss_decile\n\n\n\n\n\n\n\n0\n-0.583798\n61.985974\n0.492952\n\n\n1\n-2.593199\n59.694679\n0.480081\n\n\n2\n-0.474464\n59.605587\n0.493179\n\n\n3\n-0.356999\n61.088360\n0.493008\n\n\n4\n-0.532972\n59.912918\n0.492108\n\n\n5\n-0.054631\n59.562982\n0.498121\n\n\n6\n0.277186\n59.295282\n0.500432\n\n\n7\n-1.445407\n58.426776\n0.482279\n\n\n8\n-1.452874\n61.897539\n0.489251\n\n\n9\n-1.438291\n61.036911\n0.474517\n\n\n10\n-2.950965\n63.156705\n0.471510\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_os_decile\n\n\n\n\n\n\n\n0\n-0.929842\n61.809814\n0.489757\n\n\n1\n0.651053\n57.549163\n0.496467\n\n\n2\n0.633741\n59.928029\n0.506411\n\n\n3\n2.459156\n60.192744\n0.537383\n\n\n4\n0.270969\n59.628485\n0.503838\n\n\n5\n1.614457\n58.817251\n0.506260\n\n\n6\n2.105182\n61.321893\n0.522257\n\n\n7\n2.762148\n62.013939\n0.519637\n\n\n8\n4.511833\n58.598578\n0.534283\n\n\n9\n5.557047\n60.705294\n0.552326\n\n\n10\n7.107907\n61.111440\n0.540800\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_af_decile\n\n\n\n\n\n\n\n0\n-0.622813\n61.971757\n0.492287\n\n\n1\n-1.971418\n59.453257\n0.483909\n\n\n2\n-0.750454\n59.641926\n0.499896\n\n\n3\n0.302845\n61.484574\n0.500570\n\n\n4\n-1.847975\n59.435802\n0.489428\n\n\n5\n0.472649\n58.862096\n0.499119\n\n\n6\n-1.783717\n60.445102\n0.492045\n\n\n7\n-0.741502\n60.202519\n0.490102\n\n\n8\n-1.136949\n58.666341\n0.481946\n\n\n9\n-1.569738\n59.665126\n0.482171\n\n\n10\n-1.405234\n60.765520\n0.478022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_notional_bf_decile\n\n\n\n\n\n\n\n0\n-0.664331\n61.829221\n0.492096\n\n\n1\n-1.299757\n59.059844\n0.495386\n\n\n2\n-0.405030\n60.212036\n0.494756\n\n\n3\n-0.276251\n62.615726\n0.492496\n\n\n4\n-1.141867\n58.157430\n0.487067\n\n\n5\n0.305562\n60.263198\n0.497576\n\n\n6\n-0.736978\n60.128309\n0.494151\n\n\n7\n-1.632006\n60.556523\n0.482265\n\n\n8\n-0.244806\n62.535670\n0.496225\n\n\n9\n-1.186544\n62.135430\n0.484081\n\n\n10\n-0.986609\n63.429730\n0.486810\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_decile\n\n\n\n\n\n\n\n0\n-0.624363\n62.073346\n0.492292\n\n\n1\n-1.227008\n59.633737\n0.488654\n\n\n2\n-0.224429\n62.098079\n0.493227\n\n\n3\n-0.876075\n62.277790\n0.491838\n\n\n4\n1.765962\n60.542632\n0.507381\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_ss_decile\n\n\n\n\n\n\n\n0\n-0.583798\n61.985974\n0.492952\n\n\n1\n-0.774826\n60.053791\n0.490555\n\n\n2\n-1.641327\n61.941860\n0.477445\n\n\n3\n-4.015630\n61.446165\n0.468617\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_os_decile\n\n\n\n\n\n\n\n0\n-0.929842\n61.809814\n0.489757\n\n\n1\n2.533058\n59.730576\n0.520881\n\n\n2\n5.133906\n63.005490\n0.532945\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_af_decile\n\n\n\n\n\n\n\n0\n-0.622813\n61.971757\n0.492287\n\n\n1\n-1.219873\n59.934065\n0.488896\n\n\n2\n0.745686\n59.213199\n0.498069\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_250us_distinct_tickers_bf_decile\n\n\n\n\n\n\n\n0\n-0.664331\n61.829221\n0.492096\n\n\n1\n-1.205755\n60.666768\n0.487608\n\n\n2\n2.404990\n62.575532\n0.512286\n\n\n3\n4.268441\n63.900911\n0.537923\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_decile\n\n\n\n\n\n\n\n0\n-0.620705\n62.241559\n0.492495\n\n\n1\n-0.794000\n59.759304\n0.492391\n\n\n2\n-0.552811\n59.919713\n0.492704\n\n\n3\n-1.243655\n60.074425\n0.485892\n\n\n4\n-0.369341\n59.229205\n0.494718\n\n\n5\n-0.232508\n61.109111\n0.490330\n\n\n6\n-0.400918\n59.191122\n0.494637\n\n\n7\n-1.373648\n60.658896\n0.487495\n\n\n8\n-1.981550\n63.073542\n0.478495\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_ss_decile\n\n\n\n\n\n\n\n0\n-0.561270\n62.172412\n0.493420\n\n\n1\n-0.848067\n59.671323\n0.489566\n\n\n2\n0.020212\n60.087895\n0.497073\n\n\n3\n-1.380355\n59.710729\n0.486354\n\n\n4\n0.893635\n59.555662\n0.507159\n\n\n5\n-1.227093\n60.816321\n0.485904\n\n\n6\n-0.353219\n60.514143\n0.490522\n\n\n7\n-1.176014\n61.057991\n0.485871\n\n\n8\n-4.356785\n61.385492\n0.455455\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_os_decile\n\n\n\n\n\n\n\n0\n-1.046610\n61.879410\n0.488671\n\n\n1\n2.598178\n58.831561\n0.520584\n\n\n2\n1.388084\n59.694840\n0.507341\n\n\n3\n2.737855\n59.678416\n0.530951\n\n\n4\n3.696714\n59.989764\n0.533094\n\n\n5\n3.759433\n61.863665\n0.536187\n\n\n6\n4.039673\n64.946298\n0.526656\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_af_decile\n\n\n\n\n\n\n\n0\n-0.587478\n62.125014\n0.492544\n\n\n1\n-1.113409\n59.367993\n0.491887\n\n\n2\n-0.997767\n58.838169\n0.487190\n\n\n3\n-0.869232\n60.666104\n0.489730\n\n\n4\n-0.052012\n60.649465\n0.493777\n\n\n5\n-0.762628\n59.659274\n0.493069\n\n\n6\n-2.445965\n61.347069\n0.468964\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_bf_decile\n\n\n\n\n\n\n\n0\n-0.668649\n61.925253\n0.492249\n\n\n1\n-0.538032\n59.952511\n0.491761\n\n\n2\n-0.299802\n60.643783\n0.496218\n\n\n3\n-1.287512\n60.394182\n0.485155\n\n\n4\n-0.212191\n60.688700\n0.494354\n\n\n5\n-1.146913\n61.073612\n0.484358\n\n\n6\n-0.092052\n61.518328\n0.497092\n\n\n7\n-2.373215\n63.827114\n0.479336\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_decile\n\n\n\n\n\n\n\n0\n-0.620705\n62.241559\n0.492495\n\n\n1\n-1.737734\n59.449735\n0.486342\n\n\n2\n-1.074176\n62.670226\n0.492326\n\n\n3\n0.025192\n60.625263\n0.499924\n\n\n4\n-0.614110\n60.044256\n0.496633\n\n\n5\n-0.801568\n59.466447\n0.493838\n\n\n6\n-0.111053\n58.976801\n0.496239\n\n\n7\n-1.006608\n57.990565\n0.481425\n\n\n8\n-0.502523\n59.481311\n0.492872\n\n\n9\n-0.756644\n61.013727\n0.485783\n\n\n10\n-1.766824\n62.147439\n0.479265\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_ss_decile\n\n\n\n\n\n\n\n0\n-0.561270\n62.172412\n0.493420\n\n\n1\n-1.863055\n58.888619\n0.486284\n\n\n2\n-1.249291\n62.612207\n0.488324\n\n\n3\n0.189270\n60.810309\n0.496620\n\n\n4\n-0.328625\n59.565858\n0.498071\n\n\n5\n-0.872963\n59.632551\n0.492663\n\n\n6\n0.384466\n59.283316\n0.497085\n\n\n7\n-1.646605\n57.275756\n0.480803\n\n\n8\n-0.432175\n60.536473\n0.485001\n\n\n9\n-1.196030\n61.456614\n0.480045\n\n\n10\n-3.407939\n61.671914\n0.468860\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_os_decile\n\n\n\n\n\n\n\n0\n-1.046610\n61.879410\n0.488671\n\n\n1\n0.608009\n57.868357\n0.502373\n\n\n2\n1.213149\n58.919654\n0.515229\n\n\n3\n0.964962\n62.022977\n0.516856\n\n\n4\n1.005730\n60.088004\n0.519020\n\n\n5\n2.178224\n59.371388\n0.516182\n\n\n6\n3.162189\n60.533363\n0.528195\n\n\n7\n3.360963\n59.145323\n0.520612\n\n\n8\n3.825312\n59.067154\n0.533107\n\n\n9\n4.677211\n59.846586\n0.536342\n\n\n10\n6.527061\n61.477747\n0.536865\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_af_decile\n\n\n\n\n\n\n\n0\n-0.587478\n62.125014\n0.492544\n\n\n1\n-1.903612\n59.641853\n0.486272\n\n\n2\n-1.577016\n60.150047\n0.491786\n\n\n3\n-1.143377\n61.440408\n0.495120\n\n\n4\n-0.107671\n59.068153\n0.498640\n\n\n5\n-0.235300\n59.691610\n0.497094\n\n\n6\n-0.917223\n59.612335\n0.495474\n\n\n7\n-0.553782\n59.167483\n0.488227\n\n\n8\n-2.063076\n58.191175\n0.482530\n\n\n9\n-1.035980\n59.779924\n0.483736\n\n\n10\n-1.277087\n60.858042\n0.474165\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_notional_bf_decile\n\n\n\n\n\n\n\n0\n-0.668649\n61.925253\n0.492249\n\n\n1\n-0.785802\n59.048124\n0.495797\n\n\n2\n-0.278989\n63.251210\n0.497261\n\n\n3\n-0.688745\n61.896634\n0.489579\n\n\n4\n-0.574269\n59.815153\n0.488596\n\n\n5\n-0.589105\n58.980227\n0.496314\n\n\n6\n-0.192829\n59.345051\n0.496417\n\n\n7\n-1.358694\n59.674096\n0.481068\n\n\n8\n0.593934\n60.252274\n0.495297\n\n\n9\n-1.503131\n62.581222\n0.485840\n\n\n10\n-1.802396\n62.680478\n0.481171\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_decile\n\n\n\n\n\n\n\n0\n-0.620705\n62.241559\n0.492495\n\n\n1\n-1.092466\n59.828786\n0.489363\n\n\n2\n-1.211530\n61.456763\n0.483649\n\n\n3\n-0.552845\n61.253291\n0.495493\n\n\n4\n0.813567\n60.707730\n0.500691\n\n\n5\n1.154696\n59.047758\n0.503376\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_ss_decile\n\n\n\n\n\n\n\n0\n-0.561270\n62.172412\n0.493420\n\n\n1\n-0.639238\n59.944723\n0.491008\n\n\n2\n-1.763453\n61.602485\n0.478763\n\n\n3\n-2.008124\n63.390048\n0.480476\n\n\n4\n-3.581870\n54.043554\n0.468186\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_os_decile\n\n\n\n\n\n\n\n0\n-1.046610\n61.879410\n0.488671\n\n\n1\n2.566729\n59.844122\n0.521527\n\n\n2\n4.542225\n62.615163\n0.530370\n\n\n3\n2.876447\n54.978770\n0.525470\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_af_decile\n\n\n\n\n\n\n\n0\n-0.587478\n62.125014\n0.492544\n\n\n1\n-1.296133\n59.827035\n0.488025\n\n\n2\n0.072548\n61.747507\n0.497917\n\n\n3\n0.362832\n55.800555\n0.495359\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_distinct_tickers_bf_decile\n\n\n\n\n\n\n\n0\n-0.668649\n61.925253\n0.492249\n\n\n1\n-1.307315\n60.883753\n0.486054\n\n\n2\n2.477785\n60.123702\n0.513252\n\n\n3\n2.324241\n60.065732\n0.519392\n\n\n\n\n\n\n\n\ndef plot_grouped_statistics(group) -&gt; None:\n    col1 = \"ppt_mean\"\n    col2 = \"hit_ratio\"\n\n    fig, ax1 = plt.subplots()\n\n    width = 0.35\n    x = np.arange(len(group.index))\n\n    ax1.bar(x - width / 2, group[col1], width, color=\"b\", label=col1)\n    ax1.set_xlabel(\"feature deciles\")\n    ax1.set_ylabel(col1, color=\"b\")\n    ax1.tick_params(axis=\"y\", labelcolor=\"b\")\n\n    ax2 = ax1.twinx()\n    ax2.bar(x + width / 2, group[col2] - 0.5, width, bottom=0.5, color=\"r\", label=col2)\n    ax2.set_ylabel(col2, color=\"r\")\n    ax2.tick_params(axis=\"y\", labelcolor=\"r\")\n\n    plt.xticks(x, group.index)\n    plt.legend()\n    plt.show()\n\n\ndef get_group(df, feature_name):\n    return df.groupby(feature_name).agg(\n        ppt_mean=(\"pnl_to_close\", \"mean\"),\n        ppt_std=(\"pnl_to_close\", \"std\"),\n        hit_ratio=(\"hit_ratio\", \"mean\"),\n    )\n\n\nplot_grouped_statistics(get_group(df, \"_250us_notional_os_decile\"))\n\n\n\n\n\nplot_grouped_statistics(get_group(df, \"_250us_notional_ss_decile\"))\n\n\n\n\n\nfeature_name = \"_250us_notional_os_decile\"\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(1, 11):\n    df.query(f\"{feature_name} == {i}\").groupby(\n        lambda x: x.date()\n    ).pnl_to_close.mean().plot(ax=ax, label=f\"decile {i}\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\nfeature_name = \"_250us_notional_os_decile\"\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(1, 11):\n    df.query(f\"{feature_name} == {i}\").groupby(\n        lambda x: x.date()\n    ).pnl_to_close.sum().cumsum().plot(ax=ax, label=f\"decile {i}\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\nfeature_name = \"_250us_notional_os_decile\"\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(1, 11):\n    df.query(f\"{feature_name} == {i}\").groupby(\n        lambda x: x.date()\n    ).pnl_to_close.mean().cumsum().plot(ax=ax, label=f\"decile {i}\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\nfor i in range(1, 11):\n    sharpe = (\n        df.query(f\"{feature_name} == {i}\").pnl_to_close.mean()\n        / df.query(f\"{feature_name} == {i}\").pnl_to_close.std()\n    )\n    print(f\"sharpe for decile {i}: {sharpe}\")\n\nsharpe for decile 1: 0.01131299019947174\nsharpe for decile 2: 0.010575027888550928\nsharpe for decile 3: 0.040854689142390745\nsharpe for decile 4: 0.004544291490442815\nsharpe for decile 5: 0.027448700970349613\nsharpe for decile 6: 0.03433001638137391\nsharpe for decile 7: 0.04454075665938627\nsharpe for decile 8: 0.07699561228596234\nsharpe for decile 9: 0.09154139206805635\nsharpe for decile 10: 0.1163105720546142\n\n\n\n# TODO: denoise by suctracting SPOOS return\n\n\ndf_grouped = df.groupby(df.index.date)[\"pnl_to_close\"].sum()\ndf_grouped.plot(title=\"Average PNL (bps) by Day\")\n\n&lt;Axes: title={'center': 'Average PNL (bps) by Day'}&gt;\n\n\n\n\n\n\ndf_grouped[df_grouped &gt; 200_000]\n\n2021-02-23    210520.562122\n2021-02-24    247849.271511\n2021-12-15    270619.406684\nName: pnl_to_close, dtype: float64\n\n\n\ndf_grouped[df_grouped &lt; -200_000]\n\n2021-01-27   -297916.178854\n2021-02-26   -393937.111535\n2021-03-03   -203526.073030\n2021-03-16   -213817.381550\n2021-12-01   -333587.389163\nName: pnl_to_close, dtype: float64\n\n\n\n\nOFI\n\ndef plot_count_histograms_per_tag(df, feature_name) -&gt; None:\n    X = pd.concat(\n        [\n            df.query(f\"{feature_name} == {i}\").resample(\"5min\").size()\n            for i in range(0, 11)\n        ],\n        axis=1,\n    ).fillna(0)\n\n    for column in X.columns:\n        x = X[column]\n        # filter non-iso\n        x = x[x &gt; 0]\n\n        plt.figure(figsize=(3, 2))\n        plt.hist(x, bins=np.arange(x.min(), x.max()), density=True)\n\n        plt.title(f\"Histogram of {column}\")\n        plt.xlabel(column)\n        plt.ylabel(\"Frequency\")\n        plt.show()\n\n\nplot_count_histograms_per_tag(df, \"_250us_notional_os_decile\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef compute_log_returns2(mid):\n    zz = pd.DataFrame(mid)\n\n    markouts_str = [\"30S\", \"1min\", \"2min\"]\n\n    Markout = namedtuple(\"Markout\", [\"str\", \"int\"])\n    markouts = [Markout(x, markout_to_int(x)) for x in markouts_str]\n    for markout in markouts:\n        zz[f\"_{markout.str}\"] = zz.mid.shift(-markout.int)\n\n    logs = np.log(zz)\n\n    logs = logs.resample(\"5min\", label=\"right\", closed=\"right\").last()\n\n    # not sure how to drop all resampled buckets where there was a NaN before\n    # for now simply drop last row of resampled dataframe\n    logs = logs.iloc[:-1]\n\n    markouts_columns = logs.filter(regex=\"^_\").columns.to_list()\n    for markout_column in markouts_columns:\n        logs[markout_column] = logs[markout_column] - logs[\"mid\"]\n    # markouts_columns\n    # markouts_columns = logs.filter(regex=\"^_\").columns.to_list()\n    # for markout_column in markouts_columns:\n    #     logs[markout_column] = logs[markout_column] - logs['mid']\n    logs[\"contemp\"] = logs.mid - logs.mid.shift(1)\n\n    logs.dropna(inplace=True)\n    logs = logs.drop(\"mid\", axis=1)\n    return logs\n\n\ncompute_log_returns(df.mid)\n\n\n\n\n\n\n\n\n_30S\n_1min\n_2min\ncontemp\n\n\ndatetime\n\n\n\n\n\n\n\n\n2021-01-04 09:40:00\n-0.000519\n-0.000445\n-0.000519\n-0.001631\n\n\n2021-01-04 09:45:00\n-0.000074\n-0.000074\n-0.000149\n-0.002154\n\n\n2021-01-04 09:50:00\n-0.000074\n-0.000074\n-0.000149\n0.000297\n\n\n2021-01-04 09:55:00\n0.000670\n0.000670\n0.000596\n-0.001934\n\n\n2021-01-04 10:00:00\n0.000075\n0.000075\n0.000075\n-0.001640\n\n\n...\n...\n...\n...\n...\n\n\n2021-12-31 15:35:00\n0.000064\n0.000000\n-0.000064\n-0.001536\n\n\n2021-12-31 15:40:00\n-0.000064\n-0.000064\n0.000000\n-0.000705\n\n\n2021-12-31 15:45:00\n-0.000064\n-0.000064\n-0.000064\n0.000320\n\n\n2021-12-31 15:50:00\n0.000000\n-0.000064\n-0.000449\n-0.000705\n\n\n2021-12-31 15:55:00\n0.000000\n-0.000064\n-0.000064\n-0.003340\n\n\n\n\n18426 rows × 4 columns\n\n\n\n\ncompute_log_returns2(df.mid)\n\n\n\n\n\n\n\n\n_30S\n_1min\n_2min\ncontemp\n\n\ndatetime\n\n\n\n\n\n\n\n\n2021-01-04 09:40:00\n-0.000519\n-0.000445\n-0.000519\n-0.001631\n\n\n2021-01-04 09:45:00\n-0.000074\n-0.000074\n-0.000149\n-0.002154\n\n\n2021-01-04 09:50:00\n-0.000074\n-0.000074\n-0.000149\n0.000297\n\n\n2021-01-04 09:55:00\n0.000670\n0.000670\n0.000596\n-0.001934\n\n\n2021-01-04 10:00:00\n0.000075\n0.000075\n0.000075\n-0.001640\n\n\n...\n...\n...\n...\n...\n\n\n2021-12-31 15:35:00\n0.000064\n0.000000\n-0.000064\n-0.001536\n\n\n2021-12-31 15:40:00\n-0.000064\n-0.000064\n0.000000\n-0.000705\n\n\n2021-12-31 15:45:00\n-0.000064\n-0.000064\n-0.000064\n0.000320\n\n\n2021-12-31 15:50:00\n0.000000\n-0.000064\n-0.000449\n-0.000705\n\n\n2021-12-31 15:55:00\n0.000000\n-0.000064\n-0.000064\n-0.003340\n\n\n\n\n18426 rows × 4 columns\n\n\n\n\nX = pd.concat(\n    [compute_ofi(df.query(f\"_250us_notional_os_decile == {i}\")) for i in range(0, 11)],\n    axis=1,\n)\nX = X.fillna(0)  # for no order flow, set 0 OFI\nX.columns = [f\"_{i}\" for i in range(11)]\n\nY = compute_log_returns2(df.mid)\nX, Y = restrict_common_index(X, Y)\n\nrun_regressions(X, Y)\n\n\n\n\n\n\n\n\nr2\ncoef\nintercept\n\n\ntarget_markout\n\n\n\n\n\n\n\n_30S\n0.001811\n[2.200118744539078e-05, -3.6208686711013614e-0...\n0.0\n\n\n_1min\n0.001893\n[2.6182811554683554e-05, -4.04457648403342e-06...\n0.0\n\n\n_2min\n0.001680\n[2.9314378749474886e-05, -2.590898591698459e-0...\n0.0\n\n\ncontemp\n0.015981\n[-0.00025494788194808634, -3.750410431682322e-...\n0.0\n\n\n\n\n\n\n\n\nX = pd.concat(\n    [compute_ofi(df.query(f\"_250us_notional_os_decile == {i}\")) for i in range(0, 11)],\n    axis=1,\n)\nX = X.fillna(0)  # for no order flow, set 0 OFI\nX.columns = [f\"_{i}\" for i in range(11)]\n\nY = compute_log_returns2(df.mid)\nX, Y = restrict_common_index(X, Y)\n\nX = sm.add_constant(X)\n# Y = Y.contemp\nY = Y._1min\n\nmodel = sm.OLS(Y, X)\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\n_1min\nR-squared:\n0.002\n\n\nModel:\nOLS\nAdj. R-squared:\n0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.193\n\n\nDate:\nTue, 21 Nov 2023\nProb (F-statistic):\n0.000239\n\n\nTime:\n14:28:56\nLog-Likelihood:\n1.0592e+05\n\n\nNo. Observations:\n18426\nAIC:\n-2.118e+05\n\n\nDf Residuals:\n18414\nBIC:\n-2.117e+05\n\n\nDf Model:\n11\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n3.282e-06\n7.42e-06\n0.442\n0.658\n-1.13e-05\n1.78e-05\n\n\n_0\n2.392e-05\n1.51e-05\n1.585\n0.113\n-5.66e-06\n5.35e-05\n\n\n_1\n-3.565e-06\n1.48e-05\n-0.241\n0.809\n-3.25e-05\n2.54e-05\n\n\n_2\n-3.361e-06\n1.43e-05\n-0.235\n0.814\n-3.14e-05\n2.47e-05\n\n\n_3\n-2.284e-05\n1.43e-05\n-1.603\n0.109\n-5.08e-05\n5.1e-06\n\n\n_4\n1.13e-05\n1.41e-05\n0.801\n0.423\n-1.64e-05\n3.89e-05\n\n\n_5\n2.346e-05\n1.41e-05\n1.660\n0.097\n-4.24e-06\n5.12e-05\n\n\n_6\n-3.896e-05\n1.44e-05\n-2.703\n0.007\n-6.72e-05\n-1.07e-05\n\n\n_7\n3.936e-06\n1.43e-05\n0.275\n0.783\n-2.41e-05\n3.19e-05\n\n\n_8\n-2.099e-05\n1.46e-05\n-1.438\n0.151\n-4.96e-05\n7.63e-06\n\n\n_9\n-5.749e-05\n1.5e-05\n-3.832\n0.000\n-8.69e-05\n-2.81e-05\n\n\n_10\n7.132e-06\n1.56e-05\n0.458\n0.647\n-2.34e-05\n3.77e-05\n\n\n\n\n\n\nOmnibus:\n9515.212\nDurbin-Watson:\n1.992\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n56763414.673\n\n\nSkew:\n0.621\nProb(JB):\n0.00\n\n\nKurtosis:\n274.907\nCond. No.\n3.10\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ndef plot_ofi_histograms_per_tag(df, feature_name) -&gt; None:\n    X = pd.concat(\n        [compute_ofi(df.query(f\"{feature_name} == {i}\")) for i in range(0, 11)], axis=1\n    )\n    X = X.fillna(0)  # for no order flow, set 0 OFI\n    X.columns = [f\"_{i}\" for i in range(11)]\n\n    Y = compute_log_returns(df.mid)\n    X, Y = restrict_common_index(X, Y)\n\n    for column in X.columns:\n        plt.figure(figsize=(3, 2))\n        plt.hist(X[column], bins=30, density=True)\n        plt.title(f\"Histogram of {column}\")\n        plt.xlabel(column)\n        plt.ylabel(\"Frequency\")\n        plt.show()\n\n\nplot_ofi_histograms_per_tag(df, \"_250us_notional_ss_decile\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# unconditional ofi\nX = compute_ofi(df)\nX = X.fillna(0)  # for no order flow, set 0 OFI\n\nY = compute_log_returns2(df.mid)\nX, Y = restrict_common_index(X, Y)\n\nX = sm.add_constant(X)\n# Y = Y.contemp\nY = Y._1min\n\nmodel = sm.OLS(Y, X)\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\n_1min\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.101\n\n\nDate:\nTue, 21 Nov 2023\nProb (F-statistic):\n0.294\n\n\nTime:\n14:29:16\nLog-Likelihood:\n1.0590e+05\n\n\nNo. Observations:\n18426\nAIC:\n-2.118e+05\n\n\nDf Residuals:\n18424\nBIC:\n-2.118e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n1.898e-05\n5.89e-06\n3.222\n0.001\n7.43e-06\n3.05e-05\n\n\nofi\n1.606e-05\n1.53e-05\n1.049\n0.294\n-1.39e-05\n4.6e-05\n\n\n\n\n\n\nOmnibus:\n9944.354\nDurbin-Watson:\n1.989\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n57145617.692\n\n\nSkew:\n0.766\nProb(JB):\n0.00\n\n\nKurtosis:\n275.819\nCond. No.\n2.72\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nPreliminary evidence groupby by quantile by year\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(0, 11):\n    df.query(f\"_500us_notional_os_decile == {i}\").pnl_to_close.cumsum().plot(\n        ax=ax, label=f\"decile {i}\"\n    )\n\nplt.legend()\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(1, 11):\n    df.query(f\"_500us_notional_os_decile == {i}\").pnl_to_close.cumsum().plot(\n        ax=ax, label=f\"decile {i}\"\n    )\n\nplt.legend()\nplt.show()\n\n\n\n\n\ndf.eval(\"ret = pnl_to_close / 1e4\", inplace=True)\n\n\ndf1 = df.query(f\"_500us_notional_os_decile == 10\").ret.copy(deep=True)\ndaily_returns = df1.groupby(df1.index.date).mean()\ndaily_returns.mean() / daily_returns.std()\n\n0.01860449561186662\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(1, 11):\n    df.query(f\"_500us_notional_os_decile == {i}\").ret.cumprod().plot(\n        ax=ax, label=f\"decile {i}\"\n    )\n\nplt.legend()\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(1, 11):\n    df.query(f\"_500us_notional_os_decile == {i}\").pnl_to_close.cumsum().plot(\n        ax=ax, label=f\"decile {i}\"\n    )\n\nplt.legend()\nplt.show()\n\n\n\nSharpe\n\ndf1 = df.query(f\"_500us_notional_os_decile == 10\").pnl_to_close.copy(deep=True)\ndaily_returns = df1.groupby(df1.index.date).mean()\n\n\ndf1 = df.query(f\"_500us_notional_os_decile == 10\").pnl_to_close.copy(deep=True)\ndaily_returns = df1.groupby(df1.index.date).mean()\n\n\ndaily_returns\n\n2021-01-04    55.462690\n2021-01-05    -6.151922\n2021-01-06     3.395875\n2021-01-07    -0.043274\n2021-01-08   -12.136896\n                ...    \n2021-12-27     3.808475\n2021-12-28     4.436923\n2021-12-29     6.358496\n2021-12-30     3.419003\n2021-12-31    44.445922\nName: pnl_to_close, Length: 250, dtype: float64\n\n\n\ndaily_returns.mean() / daily_returns.std()\n\n0.018604495611866625\n\n\n\n\nOFIs\n\ndf.mid.resample(\"5min\").last().shift(1)\n\ndatetime\n2021-01-04 09:30:00       NaN\n2021-01-04 09:35:00    67.505\n2021-01-04 09:40:00    67.395\n2021-01-04 09:45:00    67.250\n2021-01-04 09:50:00    67.270\n                        ...  \n2021-12-31 15:35:00    78.050\n2021-12-31 15:40:00    77.995\n2021-12-31 15:45:00    78.020\n2021-12-31 15:50:00    77.965\n2021-12-31 15:55:00    77.705\nFreq: 5T, Name: mid, Length: 104046, dtype: float64\n\n\n\ndf.mid.resample(\"5min\").last()\n\ndatetime\n2021-01-04 09:30:00    67.505\n2021-01-04 09:35:00    67.395\n2021-01-04 09:40:00    67.250\n2021-01-04 09:45:00    67.270\n2021-01-04 09:50:00    67.140\n                        ...  \n2021-12-31 15:35:00    77.995\n2021-12-31 15:40:00    78.020\n2021-12-31 15:45:00    77.965\n2021-12-31 15:50:00    77.705\n2021-12-31 15:55:00    77.685\nFreq: 5T, Name: mid, Length: 104046, dtype: float64\n\n\n\nY\n\n\n\n\n\n\n\n\n_30S\n_1min\n_2min\ncontemp\n\n\ndatetime\n\n\n\n\n\n\n\n\n2021-01-04 09:35:00\n-0.000519\n-0.000519\n-0.000667\n0.001631\n\n\n2021-01-04 09:40:00\n-0.000519\n-0.000445\n-0.000519\n0.002154\n\n\n2021-01-04 09:45:00\n-0.000074\n-0.000074\n-0.000149\n-0.000297\n\n\n2021-01-04 09:50:00\n-0.000074\n-0.000074\n-0.000149\n0.001934\n\n\n2021-01-04 09:55:00\n0.000670\n0.000670\n0.000596\n0.001640\n\n\n...\n...\n...\n...\n...\n\n\n2021-12-31 15:30:00\n0.000000\n0.000064\n0.000128\n0.001536\n\n\n2021-12-31 15:35:00\n0.000064\n0.000000\n-0.000064\n0.000705\n\n\n2021-12-31 15:40:00\n-0.000064\n-0.000064\n0.000000\n-0.000320\n\n\n2021-12-31 15:45:00\n-0.000064\n-0.000064\n-0.000064\n0.000705\n\n\n2021-12-31 15:50:00\n0.000000\n-0.000064\n-0.000449\n0.003340\n\n\n\n\n18426 rows × 4 columns\n\n\n\n\nnp.log(67.470) - np.log(67.360)\n\n0.0016316847052628702\n\n\n\ndf.mid.resample(\"5min\").first()\n\ndatetime\n2021-01-04 09:30:00    67.695\n2021-01-04 09:35:00    67.470\n2021-01-04 09:40:00    67.360\n2021-01-04 09:45:00    67.245\n2021-01-04 09:50:00    67.265\n                        ...  \n2021-12-31 15:35:00    78.055\n2021-12-31 15:40:00    77.990\n2021-12-31 15:45:00    78.015\n2021-12-31 15:50:00    77.965\n2021-12-31 15:55:00    77.705\nFreq: 5T, Name: mid, Length: 104046, dtype: float64\n\n\n\nY\n\n\n\n\n\n\n\n\n_30S\n_1min\n_2min\ncontemp\n\n\ndatetime\n\n\n\n\n\n\n\n\n2021-01-04 09:40:00\n-0.000519\n-0.000445\n-0.000519\n-0.001631\n\n\n2021-01-04 09:45:00\n-0.000074\n-0.000074\n-0.000149\n-0.002154\n\n\n2021-01-04 09:50:00\n-0.000074\n-0.000074\n-0.000149\n0.000297\n\n\n2021-01-04 09:55:00\n0.000670\n0.000670\n0.000596\n-0.001934\n\n\n2021-01-04 10:00:00\n0.000075\n0.000075\n0.000075\n-0.001640\n\n\n...\n...\n...\n...\n...\n\n\n2021-12-31 15:35:00\n0.000064\n0.000000\n-0.000064\n-0.001536\n\n\n2021-12-31 15:40:00\n-0.000064\n-0.000064\n0.000000\n-0.000705\n\n\n2021-12-31 15:45:00\n-0.000064\n-0.000064\n-0.000064\n0.000320\n\n\n2021-12-31 15:50:00\n0.000000\n-0.000064\n-0.000449\n-0.000705\n\n\n2021-12-31 15:55:00\n0.000000\n-0.000064\n-0.000064\n-0.003340\n\n\n\n\n18426 rows × 4 columns\n\n\n\n\nX.sample(20)\n\n\n\n\n\n\n\n\nofi\n\n\ndatetime\n\n\n\n\n\n2021-12-17 12:50:00\n0.099842\n\n\n2021-11-11 12:35:00\n0.149293\n\n\n2021-08-18 13:25:00\n0.537679\n\n\n2021-07-02 13:00:00\n0.808090\n\n\n2021-04-15 09:55:00\n0.028968\n\n\n2021-02-11 12:25:00\n-0.518424\n\n\n2021-04-29 13:35:00\n0.471068\n\n\n2021-05-06 12:30:00\n-0.837211\n\n\n2021-02-26 09:55:00\n0.379111\n\n\n2021-11-15 14:20:00\n0.465241\n\n\n2021-05-03 15:20:00\n0.346192\n\n\n2021-09-03 15:50:00\n0.225279\n\n\n2021-01-04 10:30:00\n0.048287\n\n\n2021-07-06 10:15:00\n-0.019906\n\n\n2021-07-13 12:00:00\n-0.447236\n\n\n2021-08-30 12:05:00\n-0.393038\n\n\n2021-02-08 10:15:00\n0.558504\n\n\n2021-04-01 14:15:00\n0.390543\n\n\n2021-12-08 14:40:00\n-0.361169\n\n\n2021-08-19 13:55:00\n0.194927\n\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\nofi\n\n\ndatetime\n\n\n\n\n\n2021-01-04 09:40:00\n-0.215316\n\n\n2021-01-04 09:45:00\n0.181602\n\n\n2021-01-04 09:50:00\n-0.032230\n\n\n2021-01-04 09:55:00\n0.489251\n\n\n2021-01-04 10:00:00\n0.701882\n\n\n...\n...\n\n\n2021-12-31 15:35:00\n0.121631\n\n\n2021-12-31 15:40:00\n-0.351283\n\n\n2021-12-31 15:45:00\n-0.159085\n\n\n2021-12-31 15:50:00\n-0.175439\n\n\n2021-12-31 15:55:00\n0.090304\n\n\n\n\n18426 rows × 1 columns\n\n\n\n\nX\n\n\n\n\n\n\n\n\nofi\n\n\ndatetime\n\n\n\n\n\n2021-01-04 09:40:00\n-0.215316\n\n\n2021-01-04 09:45:00\n0.181602\n\n\n2021-01-04 09:50:00\n-0.032230\n\n\n2021-01-04 09:55:00\n0.489251\n\n\n2021-01-04 10:00:00\n0.701882\n\n\n...\n...\n\n\n2021-12-31 15:35:00\n0.121631\n\n\n2021-12-31 15:40:00\n-0.351283\n\n\n2021-12-31 15:45:00\n-0.159085\n\n\n2021-12-31 15:50:00\n-0.175439\n\n\n2021-12-31 15:55:00\n0.090304\n\n\n\n\n18426 rows × 1 columns\n\n\n\n\nX = compute_ofi(df)\nX = pd.DataFrame(X)  # tmp fix\nY = compute_log_returns(df.mid)\nX, Y = restrict_common_index(X, Y)\n# print(X)\n# print(Y)\nrun_regressions(X, Y)\n\n\n\n\n\n\n\n\nr2\ncoef\nintercept\n\n\ntarget_markout\n\n\n\n\n\n\n\n_30S\n-0.000536\n[2.5592530391572006e-05]\n0.0\n\n\n_1min\n-0.000504\n[2.881596606563757e-05]\n0.0\n\n\n_2min\n-0.000627\n[3.1450230298814306e-05]\n0.0\n\n\ncontemp\n0.014343\n[-0.00029700015237814586]\n0.0\n\n\n\n\n\n\n\n\nX = pd.concat(\n    [compute_ofi(df.query(f\"_500us_notional_os_decile == {i}\")) for i in range(0, 11)],\n    axis=1,\n)\nX = X.fillna(0)  # for no order flow, set 0 OFI\nX.columns = range(11)\n\nY = compute_log_returns(df.mid)\nX, Y = restrict_common_index(X, Y)\n\nrun_regressions(X, Y)\n\n\n\n\n\n\n\n\nr2\ncoef\nintercept\n\n\ntarget_markout\n\n\n\n\n\n\n\n_30S\n0.001481\n[2.2768590911369735e-05, 2.9548178006308924e-0...\n0.000006\n\n\n_1min\n0.001482\n[2.6745891593117772e-05, 2.889654969636313e-05...\n0.000007\n\n\n_2min\n0.001649\n[2.6859770789570454e-05, 3.4577257730876217e-0...\n0.000011\n\n\ncontemp\n0.015877\n[-0.0002389747605278782, -3.221983993141733e-0...\n-0.000004\n\n\n\n\n\n\n\n\nX.columns = range(11)\nrun_regressions()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021-01-04 09:35:00\n-0.213995\n-1.000000\n-1.000000\nNaN\n1.000000\n-1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2021-01-04 09:40:00\n-0.088624\n0.000000\n0.000000\n-0.600000\n-1.000000\n-1.000000\nNaN\nNaN\n-1.000000\nNaN\nNaN\n\n\n2021-01-04 09:45:00\n0.208393\n0.000000\n1.000000\n0.000000\n1.000000\n-1.000000\nNaN\nNaN\n0.000000\nNaN\n-1.000000\n\n\n2021-01-04 09:50:00\n-0.152088\n1.000000\n0.000000\n-1.000000\n1.000000\n0.000000\nNaN\nNaN\n1.000000\nNaN\n0.000000\n\n\n2021-01-04 09:55:00\n0.489251\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\nNaN\nNaN\n0.000000\nNaN\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-12-31 15:40:00\n-0.170790\n0.000000\n0.000000\n0.000000\n-1.000000\n-1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2021-12-31 15:45:00\n0.069259\n-0.373041\n-0.666667\n-1.000000\n-0.987780\n-1.000000\n-1.000000\n0.000000\n-1.000000\n0.000000\n0.000000\n\n\n2021-12-31 15:50:00\n0.131204\n-1.000000\n-1.000000\n-1.000000\n0.000000\n0.000000\n-1.000000\n-0.985838\n-1.000000\n0.000000\n0.000000\n\n\n2021-12-31 15:55:00\n0.118661\n-0.356322\n-0.561905\n-1.000000\n-0.776970\n-0.634286\n0.324921\n-0.673004\n0.064639\n-1.000000\n0.000000\n\n\n2021-12-31 16:00:00\n0.034843\n-0.090909\n-0.702823\n-0.693051\n0.555556\nNaN\n0.855648\n0.904123\n-0.221790\n0.671803\n-0.293305\n\n\n\n\n28597 rows × 11 columns\n\n\n\n\n\nOld stuff\n\nQUANTILE = 0.95\ncol_name = \"_500us_distinct_tickers\"\ncol_name_clip = col_name + \"_clip\"\n\n\ndf[col_name_clip] = df[col_name].clip(upper=df[col_name].quantile(QUANTILE))\n\n\ngrouped_df = (\n    df[[\"hit_ratio\", \"pnl_to_close\", col_name_clip]]\n    .groupby(by=col_name_clip)\n    .agg(\n        ppt_mean=(\"pnl_to_close\", \"mean\"),\n        ppt_std=(\"pnl_to_close\", \"std\"),\n        hit_ratio=(\"hit_ratio\", \"mean\"),\n    )\n)\n\n\ndf.columns\n\nIndex(['time', 'event', 'order_id', 'size', 'price', 'direction',\n       'ask_price_1', 'bid_price_1', 'ticker', 'mid', '_250us_neighbors',\n       '_500us_neighbors', '_250us_distinct_tickers_ss_bf',\n       '_250us_distinct_tickers_ss_af', '_250us_distinct_tickers_os_bf',\n       '_250us_distinct_tickers_os_af', '_250us_notional_ss_bf',\n       '_250us_notional_ss_af', '_250us_notional_os_bf',\n       '_250us_notional_os_af', '_250us_num_trades_ss_bf',\n       '_250us_num_trades_ss_af', '_250us_num_trades_os_bf',\n       '_250us_num_trades_os_af', '_500us_distinct_tickers_ss_bf',\n       '_500us_distinct_tickers_ss_af', '_500us_distinct_tickers_os_bf',\n       '_500us_distinct_tickers_os_af', '_500us_notional_ss_bf',\n       '_500us_notional_ss_af', '_500us_notional_os_bf',\n       '_500us_notional_os_af', '_500us_num_trades_ss_bf',\n       '_500us_num_trades_ss_af', '_500us_num_trades_os_bf',\n       '_500us_num_trades_os_af', 'pnl_to_close', 'hit_ratio',\n       '_500us_distinct_tickers', '_500us_num_trades',\n       '_500us_distinct_tickers_clip', '_500us_num_trades_clip'],\n      dtype='object')\n\n\n\ngrouped_df\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_raio\n\n\n_500us_distinct_tickers_clip\n\n\n\n\n\n\n\n0\n-0.620705\n62.241559\n0.492495\n\n\n1\n-1.154729\n60.229855\n0.488880\n\n\n2\n-0.983373\n59.119667\n0.490208\n\n\n3\n-1.211530\n61.456763\n0.483649\n\n\n4\n0.284174\n60.475169\n0.499092\n\n\n\n\n\n\n\n\ngrouped_df = (\n    df[[\"hit_ratio\", \"pnl_to_close\", col_name_clip]]\n    .groupby(by=col_name_clip)\n    .agg(\n        ppt_mean=(\"pnl_to_close\", \"mean\"),\n        ppt_std=(\"pnl_to_close\", \"std\"),\n        hit_ratio=(\"hit_ratio\", \"mean\"),\n    )\n)\n\n\nQUANTILE = 0.95\ncol_name = \"_500us_num_trades\"\ncol_name_clip = col_name + \"_clip\"\n\n\ndf[col_name_clip] = df[col_name].clip(upper=df[col_name].quantile(QUANTILE))\n\n\ngrouped_df = (\n    df[[\"hit_ratio\", \"pnl_to_close\", col_name_clip]]\n    .groupby(by=col_name_clip)\n    .agg(\n        ppt_mean=(\"pnl_to_close\", \"mean\"),\n        ppt_std=(\"pnl_to_close\", \"std\"),\n        hit_ratio=(\"hit_ratio\", \"mean\"),\n    )\n)\n\n\ngrouped_df\n\n\n\n\n\n\n\n\nppt_mean\nppt_std\nhit_ratio\n\n\n_500us_num_trades_clip\n\n\n\n\n\n\n\n0\n-0.620705\n62.241559\n0.492495\n\n\n1\n-0.951186\n59.917935\n0.490033\n\n\n2\n-0.552256\n59.514261\n0.496018\n\n\n3\n-0.552811\n59.919713\n0.492704\n\n\n4\n-1.243655\n60.074425\n0.485892\n\n\n5\n-0.166191\n58.707417\n0.499487\n\n\n6\n-0.618547\n59.863505\n0.488867\n\n\n7\n-0.146460\n60.773337\n0.501561\n\n\n8\n-0.332109\n61.497333\n0.477329\n\n\n9\n-0.791676\n60.327070\n0.498238\n\n\n10\n-0.663020\n57.377079\n0.499149\n\n\n11\n-1.440645\n61.716678\n0.482853"
  },
  {
    "objectID": "flow_decomposition.html",
    "href": "flow_decomposition.html",
    "title": "ETF flow decomposition",
    "section": "",
    "text": "source\n\nget_times\n\n get_times (df:pandas.core.frame.DataFrame)\n\nReturn numpy array of times from the index of the DataFrame.\n\nsource\n\n\nstr_to_time\n\n str_to_time (time:str, convert_to:str)\n\n\nsource\n\n\ndrop_all_neighbor_cols\n\n drop_all_neighbor_cols (df:pandas.core.frame.DataFrame)\n\nDrop neighbor columns inplace.\n\nsource\n\n\nadd_neighbors\n\n add_neighbors (etf_executions:pandas.core.frame.DataFrame,\n                equity_executions:pandas.core.frame.DataFrame,\n                tolerances:list[str])\n\nAnnotate the etf execution dataframe with the indices of the neighbouring equity executions. Note: Building the KDTree on the equity dataframe. Blah\n\nsource\n\n\ncol_to_dtype_inputing_mapping\n\n col_to_dtype_inputing_mapping (col, col_to_dtype_dict)\n\n\nsource\n\n\ncompute_features\n\n compute_features (etf_trade_time, etf_trade_direction,\n                   neigh:Optional[numpy.ndarray],\n                   equity_executions:pandas.core.frame.DataFrame)\n\n\nsource\n\n\ngroupby_index_to_series\n\n groupby_index_to_series (df:pandas.core.frame.DataFrame)\n\nHierachical groupby index with one column to flattened series. Prepending the column name to the index.\n\nsource\n\n\nmulti_index_to_single_index\n\n multi_index_to_single_index (df:pandas.core.frame.DataFrame)\n\n\nsource\n\n\nappend_features\n\n append_features (etf_executions:pandas.core.frame.DataFrame,\n                  equity_executions:pandas.core.frame.DataFrame)\n\nNote that this function is not inplace.\n\nsource\n\n\ncount_non_null\n\n count_non_null (df, tolerance)\n\n\nsource\n\n\ndrop_features\n\n drop_features (df:pandas.core.frame.DataFrame)\n\nDrops all intermediate features, and just leaves the arbitrage tags. Not the nicest way. Could do better regex.\n\nsource\n\n\nsplit_isolated_non_isolated\n\n split_isolated_non_isolated (etf_executions:pandas.core.frame.DataFrame,\n                              tolerance)\n\nReturns a tuple of (isolated, non_isolated). For now, use deep copy, although this may not be great.\n\nsource\n\n\nresample_mid\n\n resample_mid (df:pandas.core.frame.DataFrame, resample_freq='5T')\n\n\nsource\n\n\nrestrict_common_index\n\n restrict_common_index (df1:pandas.core.frame.DataFrame,\n                        df2:pandas.core.frame.DataFrame)\n\nRestrict two dataframes to their common index.\n\nsource\n\n\nmarkout_returns\n\n markout_returns (df, markouts:list[str])\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\n\ndataframe to infer times to markout from\n\n\nmarkouts\nlist\nlist of markouts to compute returns for\n\n\nReturns\nDataFrame\n\n\n\n\n\nsource\n\n\nclip_for_markout\n\n clip_for_markout (df, max_markout)\n\n\nsource\n\n\nclip_df_times\n\n clip_df_times (df:pandas.core.frame.DataFrame,\n                start:datetime.time|None=None,\n                end:datetime.time|None=None)\n\nClip a dataframe or lobster object to a time range."
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "The event column in the LOBSTER data encodes the following.\n\n\n{'UNKNOWN': 0,\n 'SUBMISSION': 1,\n 'CANCELLATION': 2,\n 'DELETION': 3,\n 'EXECUTION': 4,\n 'HIDDEN_EXECUTION': 5,\n 'CROSS_TRADE': 6,\n 'ORIGINAL_TRADING_HALT': 7,\n 'OTHER': 8,\n 'TRADING_HALT': 9,\n 'RESUME_QUOTE': 10,\n 'TRADING_RESUME': 11}\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere, trading halts are split into three categories: trading halt, resume quote and trading resume. To access all three types of trading halt events, use EventGroup.TRADING_HALTS.\n\n\nThe class EventGroup is used to access the following common groups of events.\n\n\n{'CANCELLATIONS': [2, 3],\n 'EXECUTIONS': [4, 5],\n 'HALTS': [9, 10, 11]}\n\n\nThe csv data is assumed to be stored as follows. There should be one folder per ticker, with underscores separating the ticker name, start date, end date and number of levels. Each individual csv filename should also end with the number of levels. All dates should be listed as %YYYY-%MM-%DD convention. It is still possible to work with data that is not stored in this way, but the Data class must have the date_range and levels arguments provided.\ncsv_lobster_data\n├── AAPL_2016-06-21_2016-06-21_10\n│   ├── AAPL_2012-06-21_34200000_57600000_message_10.csv\n│   ├── AAPL_2012-06-21_34200000_57600000_orderbook_10.csv\n│   ├── AAPL_2012-06-22_34200000_57600000_message_10.csv\n│   └── AAPL_2012-06-22_34200000_57600000_orderbook_10.csv\n├── GOOG_2016-06-21_2016-06-22_10\n│   ├── GOOG_2012-06-21_34200000_57600000_message_10.csv\n│   ├── GOOG_2012-06-21_34200000_57600000_orderbook_10.csv\n│   ├── GOOG_2012-06-22_34200000_57600000_message_10.csv\n│   └── GOOG_2012-06-22_34200000_57600000_orderbook_10.csv\n└── LOBSTER_SampleFiles_ReadMe.txt\nThe Data object stores information about the data to be loaded, as well as specifying which preprocessing options are to be applied.\n\n\nCode\n@dataclass\nclass Data:\n    directory_path: str | None = None  # path to data\n    ticker: str | None = None  # ticker name\n    date_range: t.Optional[str | tuple[str, str]] = None\n    levels: t.Optional[int] = None\n    nrows: t.Optional[int] = None\n    load: t.Literal[\"both\", \"messages\", \"book\"] = \"both\"\n    add_ticker_column: bool = False\n    ticker_type: t.Literal[None, \"equity\", \"etf\"] = None\n    clip_trading_hours: bool = True\n    aggregate_duplicates: bool = True\n\n    def __post_init__(self) -&gt; None:\n        if self.directory_path is None:\n            self.directory_path = os.getenv(\"LOBSTER_DATA_PATH\", \"../data\")\n        if self.ticker is None:\n            self.ticker = os.getenv(\"DEFAULT_TICKER\", \"AMZN\")\n\n        # TODO: do this better, maybe pydantic, maybe a decorator, or maybe with\n        # LoadType = t.Literal[\"both\", \"messages\", \"book\"]\n        # TickerTypes = t.Literal[None, \"equity\", \"etf\"]\n        # assert self.ticker_type in get_args(TickerTypes)\n        if self.load not in (\"both\", \"messages\", \"book\"):\n            raise ValueError(f\"Invalid load type: {self.load}\")\n        if self.ticker_type not in (None, \"equity\", \"etf\"):\n            raise ValueError(f\"Invalid ticker type: {self.ticker_type}\")\n\n        # ticker path\n        tickers = glob.glob(f\"{self.directory_path}/*\")\n        # ticker_path = [t for t in tickers if self.ticker in t]\n        ticker_path = [\n            t for t in tickers if os.path.basename(t).startswith(f\"{self.ticker}_\")\n        ]\n\n        if len(ticker_path) != 1:\n            raise ValueError(f\"Expected exactly 1 directory with name {self.ticker}\")\n        self.ticker_path = ticker_path[0]\n\n        # levels\n        if self.levels is None:\n            self.levels = int(self.ticker_path.split(\"_\")[-1])\n\n            if self.levels &lt; 1:\n                raise ValueError(f\"Invalid number of levels: {self.levels}\")\n        if self.levels is None:\n            raise ValueError(\"Unable to infer levels from folder structure.\")\n        self.levels = t.cast(int, self.levels)\n\n        # infer date range from ticker folder name\n        if not self.date_range:\n            self.date_range = tuple(\n                re.findall(pattern=r\"\\d\\d\\d\\d-\\d\\d-\\d\\d\", string=self.ticker_path)\n            )\n            if len(self.date_range) != 2:\n                raise ValueError(\n                    f\"Expected exactly 2 dates in regex match of in {self.ticker_path}\"\n                )\n\n        # book and message paths\n        tickers = glob.glob(f\"{self.ticker_path}/*\")\n        tickers_end = list(map(os.path.basename, tickers))\n\n        if isinstance(self.date_range, tuple):\n            # get all dates in folder\n            dates = {\n                re.findall(pattern=r\"\\d\\d\\d\\d-\\d\\d-\\d\\d\", string=file)[0]\n                for file in tickers_end\n            }\n            # filter for dates within specified range\n            dates = sorted(\n                list(\n                    filter(\n                        lambda date: self.date_range[0] &lt;= date &lt;= self.date_range[1],\n                        dates,\n                    )\n                )\n            )\n\n            self.dates = dates\n            self.date_range = (min(self.dates), max(self.dates))\n\n        elif isinstance(self.date_range, str):\n            self.dates, self.date_range = (\n                [self.date_range],\n                (\n                    self.date_range,\n                    self.date_range,\n                ),\n            )\n\n        # messages and book filepath dictionaries\n        def _create_date_to_path_dict(keyword: str) -&gt; dict:\n            filter_keyword_tickers = list(filter(lambda x: keyword in x, tickers_end))\n            date_path_dict = {}\n            for date in self.dates:\n                filter_date_tickers = list(\n                    filter(lambda x: date in x, filter_keyword_tickers)\n                )\n                if len(filter_date_tickers) != 1:\n                    raise ValueError(f\"Expected exactly 1 match for {date}\")\n                date_path_dict[date] = os.path.join(\n                    self.ticker_path, filter_date_tickers[0]\n                )\n            return date_path_dict\n\n        self.book_paths = _create_date_to_path_dict(\"book\")\n        self.messages_paths = _create_date_to_path_dict(\"message\")\n\n        self.load_book = False\n        self.load_messages = False\n        if self.load in (\"book\", \"both\"):\n            self.load_book = True\n        if self.load in (\"messages\", \"both\"):\n            self.load_messages = True\n\n\n\nsource\n\nData\n\n Data (directory_path:str|None=None, ticker:str|None=None,\n       date_range:Union[str,tuple[str,str],NoneType]=None,\n       levels:Optional[int]=None, nrows:Optional[int]=None,\n       load:Literal['both','messages','book']='both',\n       add_ticker_column:bool=False,\n       ticker_type:Literal[None,'equity','etf']=None,\n       clip_trading_hours:bool=True, aggregate_duplicates:bool=True)\n\nThe Lobster loads the csv data into its messages and book attributes. The data to be loaded is passed in as a Data object.\n\nsource\n\n\nclip_times\n\n clip_times (df:pandas.core.frame.DataFrame,\n             start:datetime.time|None=None, end:datetime.time|None=None)\n\nClip a dataframe or lobster object to a time range.\n\n\nCode\n@dataclass\nclass Lobster:\n    \"Lobster data class for a single symbol of Lobster data.\"\n\n    data: Data | None = None\n\n    def process_messages(self, date, filepath):\n        df = pd.read_csv(\n            filepath,\n            header=None,\n            nrows=self.data.nrows,\n            usecols=list(range(6)),\n            names=[\"time\", \"event\", \"order_id\", \"size\", \"price\", \"direction\"],\n            index_col=False,\n            dtype={\n                \"time\": \"float64\",\n                \"event\": \"int8\",\n                \"price\": \"int64\",\n                \"direction\": \"int8\",\n                \"order_id\": \"int32\",\n                \"size\": \"int64\",\n            },\n        )\n\n        # set index as datetime\n        # df.rename(columns={'time':'seconds_since_midnight'})\n        df[\"datetime\"] = pd.to_datetime(date, format=\"%Y-%m-%d\") + df.time.apply(\n            lambda x: pd.to_timedelta(x, unit=\"s\")\n        )\n        df.set_index(\"datetime\", drop=True, inplace=True)\n        return df\n\n    def __post_init__(self):\n        if self.data is None:\n            self.data = Data()\n\n        if self.data.load_messages:\n            # dfs = []\n            # for date, filepath in self.data.messages_paths.items():\n            #     # load messages\n            #     df = pd.read_csv(\n            #         filepath,\n            #         header=None,\n            #         nrows=self.data.nrows,\n            #         usecols=list(range(6)),\n            #         names=[\"time\", \"event\", \"order_id\", \"size\", \"price\", \"direction\"],\n            #         index_col=False,\n            #         dtype={\n            #             \"time\": \"float64\",\n            #             \"event\": \"int8\",\n            #             \"price\": \"int64\",\n            #             \"direction\": \"int8\",\n            #             \"order_id\": \"int32\",\n            #             \"size\": \"int64\",\n            #         },\n            #     )\n\n            #     # set index as datetime\n            #     # df.rename(columns={'time':'seconds_since_midnight'})\n            #     df[\"datetime\"] = pd.to_datetime(date, format=\"%Y-%m-%d\") + df.time.apply(lambda x: pd.to_timedelta(x, unit=\"s\"))\n            #     df.set_index(\"datetime\", drop=True, inplace=True)\n            #     dfs.append(df)\n            # df = pd.concat(dfs)\n\n            # refactor for memory use\n            dfs = (\n                self.process_messages(date=date, filepath=filepath)\n                for date, filepath in self.data.messages_paths.items()\n            )\n            df = pd.concat(dfs)\n\n            # direction for cross trades is set to zero, and order_id is left unchanged\n            if (\n                not df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"]\n                .eq(-1)\n                .all()\n            ):\n                raise ValueError(\"All cross trades must have direction -1\")\n            df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"] = 0\n\n            # seems as though this is not true?\n            # if not df.loc[df.event.eq(Event.CROSS_TRADE.value), \"order_id\"].eq(-1).all():\n            #     raise ValueError(\"All cross trades must have order_id -1\")\n\n            if (\n                not df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"]\n                .eq(-1)\n                .all()\n            ):\n                raise ValueError(\"All trading halts must have direction -1\")\n            df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"] = 0\n\n            # # process trading halts and map to new trading halts\n            def _trading_halt_type(price):\n                return {\n                    -1: Event.TRADING_HALT.value,\n                    0: Event.RESUME_QUOTE.value,\n                    1: Event.TRADING_RESUME.value,\n                }[price]\n\n            df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"event\"] = df.loc[\n                df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"price\"\n            ].apply(_trading_halt_type)\n\n            # implentation of above without apply\n            # trading_halt_mask = (df.event == Event.TRADING_HALT.value)\n            # halt_type_mapping = {\n            #     -1: Event.TRADING_HALT.value,\n            #     0: Event.RESUME_QUOTE.value,\n            #     1: Event.TRADING_RESUME.value,\n            # }\n            # df.loc[trading_halt_mask, \"event\"] = df.loc[trading_halt_mask, \"price\"].map(halt_type_mapping)\n\n            # use 0 as NaN for size and direction\n            df.loc[\n                df.event.isin(EventGroup.HALTS.value),\n                [\"order_id\", \"size\", \"price\"],\n            ] = [0, 0, np.nan]\n\n            # set price in dollars\n            df.price = df.price.apply(lambda x: x / 10_000).astype(\"float64\")\n\n            if self.data.ticker_type:\n                # TODO change to get Literal Values from args?\n\n                if self.data.ticker_type not in [\n                    \"equity\",\n                    \"etf\",\n                ]:\n                    raise ValueError(\"ticker_type must be either `equity` or `etf`\")\n                # assert self.data.ticker_type in [\n                #     \"equity\",\n                #     \"etf\",\n                # ], \"ticker_type must be either `equity` or `etf`\"\n                df = df.assign(ticker_type=self.data.ticker_type).astype(\n                    dtype={\n                        \"ticker_type\": pd.CategoricalDtype(categories=[\"equity\", \"etf\"])\n                    }\n                )\n\n            self.messages = df\n\n        if self.data.load_book:\n            col_names = []\n            for level in range(1, self.data.levels + 1):\n                for col_type in [\"ask_price\", \"ask_size\", \"bid_price\", \"bid_size\"]:\n                    col_name = f\"{col_type}_{level}\"\n                    col_names.append(col_name)\n\n            # for now just use float64\n            # col_dtypes = {\n            #     col_name: pd.Int64Dtype() if (\"size\" in col_name) else \"float\"\n            #     for col_name in col_names\n            # }\n\n            dfs = []\n            for filename in self.data.book_paths.values():\n                df = pd.read_csv(\n                    filename,\n                    header=None,\n                    nrows=self.data.nrows,\n                    usecols=list(range(4 * self.data.levels)),\n                    names=col_names,\n                    dtype=\"float64\",\n                    na_values=[\"-9999999999\", \"9999999999\", \"0\"],\n                )\n\n                dfs.append(df)\n            df = pd.concat(dfs)\n\n            df.set_index(self.messages.index, inplace=True, drop=True)\n\n            price_cols = df.columns.str.contains(\"price\")\n            df.loc[:, price_cols] = df.loc[:, price_cols] / 10_000\n\n            self.book = df\n\n        # data cleaning on messages done only now, as book infers times from messages file\n        # TODO: think if leaving bool flags good idea\n        if self.data.aggregate_duplicates:\n            self.aggregate_duplicates()\n        if self.data.clip_trading_hours:\n            self.clip_trading_hours()\n        if self.data.add_ticker_column:\n            self.add_ticker_column()\n\n    def clip_trading_hours(self) -&gt; t.Self:\n        if hasattr(self, \"book\"):\n            self.book = _clip_to_trading_hours(self.book)\n        if hasattr(self, \"messages\"):\n            self.messages = _clip_to_trading_hours(self.messages)\n        return self\n\n    def aggregate_duplicates(self) -&gt; t.Self:\n        self.messages = _aggregate_duplicates(self.messages)\n        return self\n\n    # TODO: write decorator to simplify the \"both\", \"messages\", \"book\" logic that is common to a few methods\n    def add_ticker_column(\n        self, to: t.Literal[\"both\", \"messages\", \"book\"] = \"messages\"\n    ) -&gt; t.Self:\n        if to in (\"both\", \"messages\"):\n            self.messages = self.messages.assign(ticker=self.data.ticker).astype(\n                {\"ticker\": \"category\"}\n            )\n        if to in (\"both\", \"book\"):\n            self.book = self.book.assign(ticker=self.data.ticker).astype(\n                {\"ticker\": \"category\"}\n            )\n        return self\n\n    def __repr__(self) -&gt; str:\n        return f\"Lobster data for ticker: {self.data.ticker} for date range: {self.data.date_range[0]} to {self.data.date_range[1]}.\"\n\n\n\nsource\n\n\nLobster\n\n Lobster (data:__main__.Data|None=None)\n\nLobster data class for a single symbol of Lobster data.\n\n\nCode\nclass MPLobster:\n    \"Lobster data class for a single symbol of Lobster data.\"\n\n    MAX_WORKERS: int = 70\n\n    @staticmethod\n    def process_messages(date, filepath):\n        print(f\"start {date}\")\n        df = pd.read_csv(\n            filepath,\n            header=None,\n            nrows=None,\n            usecols=list(range(6)),\n            names=[\"time\", \"event\", \"order_id\", \"size\", \"price\", \"direction\"],\n            index_col=False,\n            dtype={\n                \"time\": \"float64\",\n                \"event\": \"int8\",\n                \"price\": \"int64\",\n                \"direction\": \"int8\",\n                \"order_id\": \"int32\",\n                \"size\": \"int64\",\n            },\n        )\n\n        # set index as datetime\n        df[\"datetime\"] = pd.to_datetime(date, format=\"%Y-%m-%d\") + df.time.apply(\n            lambda x: pd.to_timedelta(x, unit=\"s\")\n        )\n        df.set_index(\"datetime\", drop=True, inplace=True)\n        print(f\"finished {date}\")\n        return df\n\n    @staticmethod\n    def process_all_messages(messages_paths, max_workers=70):\n        print(max_workers)\n        mp.set_start_method(\"spawn\")\n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            dfs = list(\n                executor.map(\n                    MPLobster.process_messages,\n                    messages_paths.keys(),\n                    messages_paths.values(),\n                )\n            )\n            # dfs = (self.process_messages(date=date, filepath=filepath) for date, filepath in self.data.messages_paths.items())\n        df = pd.concat(dfs)\n        del dfs\n        gc.collect()\n\n        # direction for cross trades is set to zero, and order_id is left unchanged\n        if not df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"].eq(-1).all():\n            raise ValueError(\"All cross trades must have direction -1\")\n        df.loc[df.event.eq(Event.CROSS_TRADE.value), \"direction\"] = 0\n\n        if (\n            not df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"]\n            .eq(-1)\n            .all()\n        ):\n            raise ValueError(\"All trading halts must have direction -1\")\n        df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"direction\"] = 0\n\n        # process trading halts and map to new trading halts\n        def _trading_halt_type(price):\n            return {\n                -1: Event.TRADING_HALT.value,\n                0: Event.RESUME_QUOTE.value,\n                1: Event.TRADING_RESUME.value,\n            }[price]\n\n        df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"event\"] = df.loc[\n            df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"price\"\n        ].apply(_trading_halt_type)\n\n        # use 0 as NaN for size and direction\n        df.loc[\n            df.event.isin(EventGroup.HALTS.value),\n            [\"order_id\", \"size\", \"price\"],\n        ] = [0, 0, np.nan]\n\n        # set price in dollars\n        df[\"price\"] = (df.price / 10_000).astype(\"float\")\n        return df\n\n    @staticmethod\n    def process_books(filename):\n        print(f\"start {filename}\")\n        col_names = []\n        for level in range(1, 10 + 1):\n            for col_type in [\"ask_price\", \"ask_size\", \"bid_price\", \"bid_size\"]:\n                col_name = f\"{col_type}_{level}\"\n                col_names.append(col_name)\n\n        df = pd.read_csv(\n            filename,\n            header=None,\n            nrows=None,\n            usecols=None,\n            names=col_names,\n            dtype=\"float64\",\n            na_values=[\"-9999999999\", \"9999999999\", \"0\"],\n        )\n        print(f\"end {filename}\")\n        return df\n\n    @staticmethod\n    def process_all_books(book_paths: dict, max_workers=70):\n        # check if set\n        if mp.get_start_method(allow_none=True) is None:\n            mp.set_start_method(\"spawn\")\n        print(max_workers)\n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            dfs = list(executor.map(MPLobster.process_books, book_paths.values()))\n        df = pd.concat(dfs)\n        del dfs\n\n        # for now just don't grab message index..\n        # df.set_index(self.messages.index, inplace=True, drop=True)\n\n        price_cols = df.columns.str.contains(\"price\")\n        df.loc[:, price_cols] = df.loc[:, price_cols] / 10_000\n        return df\n\n    def __init__(self, data):\n        self.data = data\n\n        if self.data.load_messages:\n            self.messages = MPLobster.process_all_messages(\n                messages_paths=self.data.messages_paths\n            )\n            gc.collect()\n\n        if self.data.load_book:\n            book = MPLobster.process_all_books(book_paths=self.data.book_paths)\n            # set index here so that process_all_books can be a static_method\n            book.set_index(self.messages.index, inplace=True, drop=True)\n            self.book = book\n\n        # everything below here not that important\n        # TODO: think if leaving bool flags good idea\n        if self.data.aggregate_duplicates:\n            self.aggregate_duplicates()\n        if self.data.clip_trading_hours:\n            self.clip_trading_hours()\n        if self.data.add_ticker_column:\n            self.add_ticker_column()\n\n    def clip_trading_hours(self) -&gt; t.Self:\n        if hasattr(self, \"book\"):\n            self.book = _clip_to_trading_hours(self.book)\n        if hasattr(self, \"messages\"):\n            self.messages = _clip_to_trading_hours(self.messages)\n        return self\n\n    def aggregate_duplicates(self) -&gt; t.Self:\n        self.messages = _aggregate_duplicates(self.messages)\n        return self\n\n    # TODO: write decorator to simplify the \"both\", \"messages\", \"book\" logic that is common to a few methods\n    def add_ticker_column(\n        self, to: t.Literal[\"both\", \"messages\", \"book\"] = \"messages\"\n    ) -&gt; t.Self:\n        if to in (\"both\", \"messages\"):\n            self.messages = self.messages.assign(ticker=self.data.ticker).astype(\n                {\"ticker\": \"category\"}\n            )\n        if to in (\"both\", \"book\"):\n            self.book = self.book.assign(ticker=self.data.ticker).astype(\n                {\"ticker\": \"category\"}\n            )\n        return self\n\n    def __repr__(self) -&gt; str:\n        return f\"Lobster data for ticker: {self.data.ticker} for date range: {self.data.date_range[0]} to {self.data.date_range[1]}.\"\n\n\n\nsource\n\n\nMPLobster\n\n MPLobster (data)\n\nLobster data class for a single symbol of Lobster data.\nWhen importing the Data class into another python notebook or script, it may be convenient to overwrite defaults arguments such as directory_path. This way these arguments need only be specified once. This can be achieved by using functools.partial or by using inheritance.\n\nMyData = partial(\n    Data,\n    directory_path=\"/my/local/data/path\",\n    ticker=\"AIG\",\n    clip_trading_hours=True,\n    load=\"both\",\n)\n\n\nclass MyData(Data):\n    def __init__(\n        self,\n        directory_path=\"/my/local/data/path\",\n        ticker=\"AIG\",\n        clip_trading_hours=True,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            directory_path=directory_path,\n            ticker=ticker,\n            clip_trading_hours=clip_trading_hours,\n            *args,\n            **kwargs,\n        )\n\n\nData()\n\nData(directory_path='../data', ticker='AMZN', date_range=('2012-06-21', '2012-06-21'), levels=5, nrows=None, load='both', add_ticker_column=False, ticker_type=None, clip_trading_hours=True, aggregate_duplicates=True)\n\n\n\nsource\n\n\ninfer_ticker_dict\n\n infer_ticker_dict (files_path:str='/nfs/home/nicolasp/home/data/tmp')\n\nInfer from folder structure the ticker to date_range mapping.\n\nsource\n\n\nFolderInfo\n\n FolderInfo (full:str, ticker:str, ticker_till_end:str, start_date:str,\n             end_date:str)"
  },
  {
    "objectID": "config.html",
    "href": "config.html",
    "title": "Config",
    "section": "",
    "text": "Metadata for NASDAQ exchange.\n\nsource\n\nETFMembers\n\n ETFMembers (mapping:dict=&lt;factory&gt;)\n\n\nsource\n\n\nETFMapping\n\nsource\n\n\nNASDAQExchange\n\n NASDAQExchange (exchange_open:str='9:30', exchange_close:str='4:00',\n                 trading_days:list=&lt;factory&gt;,\n                 trading_days_df:pandas.core.frame.DataFrame=&lt;factory&gt;)\n\nThe following structured configs are used to provide the defaults across this project.\n\n\nCode\n@dataclass(frozen=True)\nclass ArcticDBConfig:\n    db_path: str = \"/nfs/home/nicolasp/home/data/arctic\"\n    library: str = \"2021\"\n    columns_per_segment: int = 63\n\n\n@dataclass\nclass DataConfig:\n    date_range: tuple[str, str] = MISSING\n    csv_files_path: str = MISSING\n\n\n@dataclass\nclass ServerDataConfig(DataConfig):\n    date_range: tuple[str, str] = (\"2020-01-02\", \"2020-01-02\")\n    csv_files_path: str = \"/nfs/home/nicolasp/home/data/tmp\"\n    zip_files_path: str = \"/nfs/lobster_data/lobster_raw\"\n\n\n@dataclass\nclass LocalDataConfig(DataConfig):\n    date_range: tuple[str, str] = (\"2019-01-02\", \"2019-01-02\")\n    csv_files_path: str = \"/home/petit/Documents/data/lobster/csv\"\n\n\n@dataclass(frozen=True)\nclass SampleDataConfig:\n    ticker: str = \"AMZN\"\n    levels: int = 5\n\n\n@dataclass\nclass HyperparametersConfig:\n    tolerances: list[str] = MISSING\n    resample_freq: str = MISSING\n    markouts: list[str] = MISSING\n\n    def __post_init__(self):\n        self.max_markout = max(self.markouts, key=lambda x: pd.Timedelta(x))\n        self.finest_resample = min(self.markouts, key=lambda x: pd.Timedelta(x))\n\n\n@dataclass\nclass SimpleHyperparametersConfig(HyperparametersConfig):\n    tolerances: list[str] = field(default_factory=lambda: [\"125us\", \"500us\"])\n    resample_freq: str = \"5min\"\n    markouts: list[str] = field(default_factory=lambda: [\"30S\", \"1min\"])\n\n\n@dataclass\nclass FullHyperparametersConfig(HyperparametersConfig):\n    tolerances: list[str] = field(default_factory=lambda: [\"125us\", \"250us\", \"500us\"])\n    resample_freq: str = \"5min\"\n    markouts: list[str] = field(default_factory=lambda: [\"30S\", \"1min\", \"2min\", \"4min\"])\n\n\n@dataclass\nclass UniverseConfig:\n    etfs: list[str] = MISSING\n    # etfs: list[str] = field(default_factory=lambda: [\"XLE\"])\n\n    def __post_init__(self):\n        etf_to_equities = {\"SPY\": [\"AAPL\", \"MSFT\", \"AMZN\", \"NVDA\", \"GOOGL\", \"GOOG\", \"BRK.B\", \"META\", \"TSLA\", \"UNH\", \"XOM\", \"JNJ\", \"JPM\", \"V\", \"PG\", \"LLY\", \"MA\", \"AVGO\", \"HD\", \"MRK\", \"CVX\", \"PEP\", \"ABBV\", \"KO\", \"COST\", \"PFE\", \"CRM\", \"MCD\", \"WMT\", \"TMO\", \"CSCO\", \"BAC\", \"AMD\", \"ACN\", \"ADBE\", \"ABT\", \"LIN\", \"CMCSA\", \"DIS\", \"ORCL\", \"NFLX\", \"WFC\", \"TXN\", \"DHR\", \"VZ\", \"NEE\", \"PM\", \"BMY\", \"RTX\", \"NKE\", \"HON\", \"UPS\", \"COP\", \"LOW\", \"UNP\", \"SPGI\", \"INTU\", \"AMGN\", \"QCOM\", \"IBM\", \"INTC\", \"SBUX\", \"BA\", \"PLD\", \"MDT\", \"GE\", \"AMAT\", \"GS\", \"CAT\", \"MS\", \"T\", \"NOW\", \"ELV\", \"ISRG\", \"MDLZ\", \"LMT\", \"BKNG\", \"BLK\", \"GILD\", \"DE\", \"SYK\", \"AXP\", \"TJX\", \"ADI\", \"ADP\", \"CVS\", \"MMC\", \"C\", \"VRTX\", \"AMT\", \"SCHW\", \"LRCX\", \"TMUS\", \"MO\", \"CB\", \"REGN\", \"ZTS\", \"MU\", \"SO\", \"PGR\", \"CI\", \"BSX\", \"FISV\", \"ETN\", \"BDX\", \"DUK\", \"PYPL\", \"SNPS\", \"EQIX\", \"CSX\", \"EOG\", \"TGT\", \"SLB\", \"AON\", \"CL\", \"CME\", \"HUM\", \"NOC\", \"ITW\", \"CDNS\", \"APD\", \"KLAC\", \"WM\", \"ICE\", \"ORLY\", \"CMG\", \"HCA\", \"ATVI\", \"MCK\", \"MMM\", \"SHW\", \"FDX\", \"EW\", \"GIS\", \"MPC\", \"PXD\", \"MCO\", \"CCI\", \"NSC\", \"FCX\", \"PNC\", \"ROP\", \"MSI\", \"KMB\", \"AZO\", \"MAR\", \"GD\", \"DG\", \"GM\", \"EMR\", \"SRE\", \"PSA\", \"F\", \"PSX\", \"NXPI\", \"EL\", \"DXCM\", \"APH\", \"MNST\", \"VLO\", \"FTNT\", \"AJG\", \"BIIB\", \"OXY\", \"ADSK\", \"USB\", \"AEP\", \"D\", \"PH\", \"JCI\", \"MRNA\", \"ECL\", \"TDG\", \"MCHP\", \"TFC\", \"ADM\", \"TRV\", \"CTAS\", \"AIG\", \"EXC\", \"CTVA\", \"ANET\", \"TT\", \"HSY\", \"COF\", \"IDXX\", \"TEL\", \"STZ\", \"CPRT\", \"HLT\", \"MSCI\", \"PCAR\", \"IQV\", \"O\", \"YUM\", \"AFL\", \"HES\", \"SYY\", \"DOW\", \"ON\", \"A\", \"WMB\", \"ROST\", \"XEL\", \"CNC\", \"WELL\", \"MET\", \"PAYX\", \"CARR\", \"VRSK\", \"NUE\", \"OTIS\", \"CHTR\", \"LHX\", \"AME\", \"DHI\", \"SPG\", \"ED\", \"EA\", \"NEM\", \"AMP\", \"KMI\", \"KR\", \"RMD\", \"CTSH\", \"FIS\", \"CSGP\", \"ROK\", \"DVN\", \"PPG\", \"FAST\", \"DD\", \"ILMN\", \"VICI\", \"KHC\", \"PEG\", \"CMI\", \"GWW\", \"BK\", \"PRU\", \"ALL\", \"MTD\", \"RSG\", \"GEHC\", \"DLTR\", \"KEYS\", \"ODFL\", \"BKR\", \"LEN\", \"ABC\", \"AWK\", \"HAL\", \"WEC\", \"CEG\", \"ZBH\", \"ACGL\", \"HPQ\", \"ANSS\", \"KDP\", \"DFS\", \"IT\", \"PCG\", \"DLR\", \"GPN\", \"VMC\", \"OKE\", \"EFX\", \"WST\", \"EIX\", \"ULTA\", \"MLM\", \"PWR\", \"ES\", \"WBD\", \"APTV\", \"FANG\", \"ALB\", \"SBAC\", \"AVB\", \"CBRE\", \"STT\", \"EBAY\", \"GLW\", \"URI\", \"TSCO\", \"XYL\", \"WTW\", \"TROW\", \"IR\", \"CDW\", \"FTV\", \"DAL\", \"CHD\", \"GPC\", \"ENPH\", \"LYB\", \"MPWR\", \"MKC\", \"CAH\", \"HIG\", \"TTWO\", \"WBA\", \"WY\", \"AEE\", \"BAX\", \"DTE\", \"VRSN\", \"MTB\", \"ALGN\", \"EQR\", \"FE\", \"STE\", \"IFF\", \"FSLR\", \"ETR\", \"CTRA\", \"DRI\", \"HOLX\", \"CLX\", \"EXR\", \"FICO\", \"PODD\", \"PPL\", \"INVH\", \"DOV\", \"HPE\", \"LH\", \"TDY\", \"COO\", \"LVS\", \"EXPD\", \"OMC\", \"NDAQ\", \"RJF\", \"CNP\", \"ARE\", \"BR\", \"K\", \"LUV\", \"FITB\", \"FLT\", \"VTR\", \"NVR\", \"RCL\", \"WAB\", \"MAA\", \"BALL\", \"CMS\", \"SEDG\", \"CAG\", \"ATO\", \"RF\", \"TYL\", \"GRMN\", \"HWM\", \"SWKS\", \"MOH\", \"SJM\", \"STLD\", \"IRM\", \"TRGP\", \"CINF\", \"LW\", \"UAL\", \"WAT\", \"PFG\", \"TER\", \"IEX\", \"PHM\", \"NTRS\", \"NTAP\", \"HBAN\", \"BRO\", \"MRO\", \"TSN\", \"FDS\", \"DGX\", \"RVTY\", \"AMCR\", \"EPAM\", \"IPG\", \"J\", \"EXPE\", \"JBHT\", \"RE\", \"CBOE\", \"AKAM\", \"BG\", \"BBY\", \"PTC\", \"LKQ\", \"SNA\", \"PAYC\", \"AVY\", \"ZBRA\", \"AES\", \"EQT\", \"ESS\", \"EVRG\", \"TXT\", \"CFG\", \"SYF\", \"AXON\", \"FMC\", \"TECH\", \"LNT\", \"POOL\", \"MGM\", \"CF\", \"WDC\", \"HST\", \"PKG\", \"UDR\", \"CHRW\", \"STX\", \"NDSN\", \"INCY\", \"MOS\", \"LYV\", \"TRMB\", \"KMX\", \"SWK\", \"WRB\", \"TAP\", \"CPT\", \"MAS\", \"BWA\", \"L\", \"CCL\", \"BF.B\", \"IP\", \"HRL\", \"VTRS\", \"TFX\", \"KIM\", \"NI\", \"DPZ\", \"APA\", \"ETSY\", \"JKHY\", \"LDOS\", \"WYNN\", \"PEAK\", \"CE\", \"CPB\", \"MKTX\", \"HSIC\", \"CRL\", \"TPR\", \"EMN\", \"GEN\", \"JNPR\", \"GL\", \"QRVO\", \"MTCH\", \"CDAY\", \"AAL\", \"PNR\", \"ALLE\", \"KEY\", \"FOXA\", \"ROL\", \"CZR\", \"FFIV\", \"PNW\", \"REG\", \"AOS\", \"BBWI\", \"UHS\", \"XRAY\", \"BIO\", \"HII\", \"NRG\", \"HAS\", \"RHI\", \"GNRC\", \"WHR\", \"NWSA\", \"PARA\", \"WRK\", \"BEN\", \"AAP\", \"BXP\", \"IVZ\", \"CTLT\", \"AIZ\", \"FRT\", \"NCLH\", \"SEE\", \"VFC\", \"ALK\", \"DXC\", \"DVA\", \"CMA\", \"OGN\", \"MHK\", \"RL\", \"ZION\", \"FOX\", \"LNC\", \"NWL\", \"NWS\", \"DISH\", \"VNT\"], \"XLF\": [\"BRK.B\", \"JPM\", \"V\", \"MA\", \"BAC\", \"WFC\", \"SPGI\", \"GS\", \"MS\", \"BLK\", \"AXP\", \"MMC\", \"C\", \"SCHW\", \"CB\", \"PGR\", \"FISV\", \"PYPL\", \"AON\", \"CME\", \"ICE\", \"MCO\", \"PNC\", \"AJG\", \"USB\", \"TFC\", \"TRV\", \"AIG\", \"COF\", \"MSCI\", \"AFL\", \"MET\", \"AMP\", \"FIS\", \"BK\", \"PRU\", \"ALL\", \"ACGL\", \"DFS\", \"GPN\", \"STT\", \"WTW\", \"TROW\", \"HIG\", \"MTB\", \"NDAQ\", \"RJF\", \"FLT\", \"FITB\", \"RF\", \"CINF\", \"PFG\", \"RE\", \"HBAN\", \"NTRS\", \"BRO\", \"FDS\", \"CBOE\", \"CFG\", \"SYF\", \"WRB\", \"L\", \"JKHY\", \"MKTX\", \"GL\", \"KEY\", \"BEN\", \"IVZ\", \"AIZ\", \"CMA\", \"ZION\", \"LNC\"], \"XLB\": [\"LIN\", \"APD\", \"SHW\", \"FCX\", \"ECL\", \"CTVA\", \"DOW\", \"NUE\", \"NEM\", \"PPG\", \"DD\", \"VMC\", \"MLM\", \"ALB\", \"LYB\", \"IFF\", \"BALL\", \"STLD\", \"AMCR\", \"AVY\", \"FMC\", \"CF\", \"PKG\", \"MOS\", \"IP\", \"CE\", \"EMN\", \"WRK\", \"SEE\"], \"XLK\": [\"MSFT\", \"AAPL\", \"NVDA\", \"AVGO\", \"CRM\", \"CSCO\", \"AMD\", \"ACN\", \"ADBE\", \"ORCL\", \"TXN\", \"INTU\", \"QCOM\", \"IBM\", \"INTC\", \"AMAT\", \"NOW\", \"ADI\", \"LRCX\", \"MU\", \"SNPS\", \"CDNS\", \"KLAC\", \"ROP\", \"MSI\", \"NXPI\", \"APH\", \"FTNT\", \"ADSK\", \"MCHP\", \"ANET\", \"TEL\", \"ON\", \"CTSH\", \"KEYS\", \"IT\", \"HPQ\", \"ANSS\", \"GLW\", \"CDW\", \"ENPH\", \"MPWR\", \"VRSN\", \"FSLR\", \"FICO\", \"HPE\", \"TDY\", \"SEDG\", \"TYL\", \"SWKS\", \"TER\", \"NTAP\", \"EPAM\", \"AKAM\", \"PTC\", \"ZBRA\", \"WDC\", \"STX\", \"TRMB\", \"JNPR\", \"GEN\", \"QRVO\", \"FFIV\", \"DXC\"], \"XLV\": [\"UNH\", \"JNJ\", \"LLY\", \"MRK\", \"ABBV\", \"PFE\", \"TMO\", \"ABT\", \"DHR\", \"BMY\", \"AMGN\", \"MDT\", \"ELV\", \"ISRG\", \"GILD\", \"SYK\", \"CVS\", \"VRTX\", \"REGN\", \"ZTS\", \"BSX\", \"CI\", \"BDX\", \"HUM\", \"HCA\", \"MCK\", \"EW\", \"DXCM\", \"BIIB\", \"MRNA\", \"IDXX\", \"IQV\", \"A\", \"CNC\", \"RMD\", \"ILMN\", \"MTD\", \"GEHC\", \"ABC\", \"ZBH\", \"WST\", \"CAH\", \"BAX\", \"ALGN\", \"STE\", \"HOLX\", \"PODD\", \"LH\", \"COO\", \"MOH\", \"WAT\", \"DGX\", \"RVTY\", \"TECH\", \"INCY\", \"TFX\", \"VTRS\", \"HSIC\", \"CRL\", \"UHS\", \"BIO\", \"XRAY\", \"CTLT\", \"DVA\", \"OGN\"], \"XLI\": [\"RTX\", \"HON\", \"UPS\", \"UNP\", \"BA\", \"GE\", \"CAT\", \"LMT\", \"DE\", \"ADP\", \"ETN\", \"CSX\", \"NOC\", \"ITW\", \"WM\", \"MMM\", \"FDX\", \"NSC\", \"GD\", \"EMR\", \"PH\", \"JCI\", \"TDG\", \"CTAS\", \"TT\", \"CPRT\", \"PCAR\", \"PAYX\", \"CARR\", \"VRSK\", \"OTIS\", \"LHX\", \"AME\", \"CSGP\", \"ROK\", \"FAST\", \"CMI\", \"GWW\", \"RSG\", \"ODFL\", \"EFX\", \"PWR\", \"URI\", \"XYL\", \"IR\", \"FTV\", \"DAL\", \"DOV\", \"EXPD\", \"BR\", \"LUV\", \"WAB\", \"HWM\", \"UAL\", \"IEX\", \"J\", \"JBHT\", \"SNA\", \"PAYC\", \"AXON\", \"TXT\", \"CHRW\", \"NDSN\", \"SWK\", \"MAS\", \"LDOS\", \"CDAY\", \"PNR\", \"AAL\", \"ALLE\", \"ROL\", \"AOS\", \"HII\", \"GNRC\", \"RHI\", \"ALK\", \"GEHC\"], \"XLU\": [\"NEE\", \"SO\", \"DUK\", \"SRE\", \"AEP\", \"D\", \"EXC\", \"XEL\", \"ED\", \"PEG\", \"AWK\", \"WEC\", \"CEG\", \"PCG\", \"EIX\", \"ES\", \"AEE\", \"DTE\", \"FE\", \"ETR\", \"PPL\", \"CNP\", \"CMS\", \"ATO\", \"AES\", \"EVRG\", \"LNT\", \"NI\", \"PNW\", \"NRG\"], \"XLY\": [\"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"LOW\", \"SBUX\", \"BKNG\", \"TJX\", \"ORLY\", \"CMG\", \"MAR\", \"AZO\", \"GM\", \"F\", \"HLT\", \"YUM\", \"ROST\", \"DHI\", \"LEN\", \"ULTA\", \"APTV\", \"EBAY\", \"TSCO\", \"GPC\", \"DRI\", \"LVS\", \"RCL\", \"NVR\", \"GRMN\", \"PHM\", \"EXPE\", \"BBY\", \"LKQ\", \"POOL\", \"MGM\", \"KMX\", \"CCL\", \"BWA\", \"ETSY\", \"DPZ\", \"WYNN\", \"TPR\", \"CZR\", \"BBWI\", \"HAS\", \"WHR\", \"AAP\", \"NCLH\", \"VFC\", \"MHK\", \"RL\", \"NWL\"], \"XLP\": [\"PG\", \"PEP\", \"KO\", \"COST\", \"MDLZ\", \"WMT\", \"PM\", \"MO\", \"TGT\", \"CL\", \"GIS\", \"KMB\", \"DG\", \"EL\", \"MNST\", \"ADM\", \"HSY\", \"STZ\", \"SYY\", \"KR\", \"KHC\", \"DLTR\", \"KDP\", \"CHD\", \"MKC\", \"WBA\", \"CLX\", \"K\", \"CAG\", \"SJM\", \"LW\", \"TSN\", \"BG\", \"TAP\", \"BF.B\", \"HRL\", \"CPB\"], \"XLE\": [\"XOM\", \"CVX\", \"EOG\", \"COP\", \"SLB\", \"MPC\", \"PXD\", \"PSX\", \"VLO\", \"OXY\", \"HES\", \"WMB\", \"KMI\", \"DVN\", \"BKR\", \"HAL\", \"OKE\", \"FANG\", \"CTRA\", \"TRGP\", \"MRO\", \"EQT\", \"APA\"], \"XLC\": [\"META\", \"GOOGL\", \"GOOG\", \"NFLX\", \"CMCSA\", \"ATVI\", \"TMUS\", \"VZ\", \"DIS\", \"CHTR\", \"EA\", \"T\", \"WBD\", \"TTWO\", \"OMC\", \"IPG\", \"LYV\", \"MTCH\", \"FOXA\", \"PARA\", \"NWSA\", \"FOX\", \"NWS\", \"DISH\"], \"IYR\": [\"PLD\", \"AMT\", \"EQIX\", \"CCI\", \"PSA\", \"O\", \"WELL\", \"SPG\", \"CSGP\", \"VICI\", \"DLR\", \"SBAC\", \"AVB\", \"CBRE\", \"WY\", \"EQR\", \"EXR\", \"INVH\", \"ARE\", \"VTR\", \"MAA\", \"SUI\", \"IRM\", \"WPC\", \"ESS\", \"UDR\", \"GLPI\", \"HST\", \"CPT\", \"KIM\", \"ELS\", \"LSI\", \"PEAK\", \"AMH\", \"REXR\", \"CUBE\", \"NLY\", \"REG\", \"LAMR\", \"COLD\", \"NNN\", \"Z\", \"EGP\", \"FR\", \"HR\", \"JLL\", \"BXP\", \"OHI\", \"FRT\", \"STAG\", \"BRX\", \"ADC\", \"STWD\", \"SRC\", \"AGNC\", \"AIRC\", \"MPW\", \"RYN\", \"RITM\", \"PCH\", \"BXMT\", \"NSA\", \"DOC\", \"CUZ\", \"KRC\", \"LXP\", \"HHC\", \"ZG\", \"OFC\", \"SBRA\", \"XTSLA\", \"EQC\", \"NHI\", \"VNO\", \"HIW\", \"DEI\", \"SLG\", \"JBGS\", \"OPEN\", \"MLPFT\", \"MARGIN_USD\", \"USD\"]}  # fmt: skip\n        self.equities = list(\n            it.chain.from_iterable([etf_to_equities[etf] for etf in self.etfs])\n        )\n\n\n@dataclass\nclass SimpleLocalUniverseConfig:\n    etfs: list[str] = field(default_factory=lambda: [\"SPY\"])\n    equities: list[str] = field(default_factory=lambda: [\"AIG\", \"GE\"])\n\n\n@dataclass\nclass SimpleServerUniverseConfig:\n    etfs: list[str] = field(default_factory=lambda: [\"XLE\"])\n    equities: list[str] = field(default_factory=lambda: [\"APA\", \"BKR\"])\n\n\ndefaults_simple_local = [\n    {\"hyperparameters\": \"simple\"},\n    {\"universe\": \"simple_local\"},\n    {\"data_config\": \"local\"},\n    \"_self_\",\n]\n\n\n@dataclass\nclass MainConfig:\n    defaults: list[t.Any] = field(default_factory=lambda: defaults_simple_local)\n    data_config: DataConfig = MISSING\n    hyperparameters: HyperparametersConfig = MISSING\n    universe: UniverseConfig = MISSING\n    db: ArcticDBConfig = field(default_factory=ArcticDBConfig)\n    sample_data: SampleDataConfig = field(default_factory=SampleDataConfig)\n\n\n\nsource\n\n\nMainConfig\n\n MainConfig (defaults:list[typing.Any]=&lt;factory&gt;,\n             data_config:__main__.DataConfig='???',\n             hyperparameters:__main__.HyperparametersConfig='???',\n             universe:__main__.UniverseConfig='???',\n             db:__main__.ArcticDBConfig=&lt;factory&gt;,\n             sample_data:__main__.SampleDataConfig=&lt;factory&gt;)\n\n\nsource\n\n\nSimpleServerUniverseConfig\n\n SimpleServerUniverseConfig (etfs:list[str]=&lt;factory&gt;,\n                             equities:list[str]=&lt;factory&gt;)\n\n\nsource\n\n\nSimpleLocalUniverseConfig\n\n SimpleLocalUniverseConfig (etfs:list[str]=&lt;factory&gt;,\n                            equities:list[str]=&lt;factory&gt;)\n\n\nsource\n\n\nUniverseConfig\n\n UniverseConfig (etfs:list[str]='???')\n\n\nsource\n\n\nFullHyperparametersConfig\n\n FullHyperparametersConfig (tolerances:list[str]=&lt;factory&gt;,\n                            resample_freq:str='5min',\n                            markouts:list[str]=&lt;factory&gt;)\n\n\nsource\n\n\nSimpleHyperparametersConfig\n\n SimpleHyperparametersConfig (tolerances:list[str]=&lt;factory&gt;,\n                              resample_freq:str='5min',\n                              markouts:list[str]=&lt;factory&gt;)\n\n\nsource\n\n\nHyperparametersConfig\n\n HyperparametersConfig (tolerances:list[str]='???',\n                        resample_freq:str='???', markouts:list[str]='???')\n\n\nsource\n\n\nSampleDataConfig\n\n SampleDataConfig (ticker:str='AMZN', levels:int=5)\n\n\nsource\n\n\nLocalDataConfig\n\n LocalDataConfig (date_range:tuple[str,str]=('2019-01-02', '2019-01-02'),\n                  csv_files_path:str='/home/petit/Documents/data/lobster/c\n                  sv')\n\n\nsource\n\n\nServerDataConfig\n\n ServerDataConfig (date_range:tuple[str,str]=('2020-01-02', '2020-01-02'),\n                   csv_files_path:str='/nfs/home/nicolasp/home/data/tmp',\n                   zip_files_path:str='/nfs/lobster_data/lobster_raw')\n\n\nsource\n\n\nDataConfig\n\n DataConfig (date_range:tuple[str,str]='???', csv_files_path:str='???')\n\n\nsource\n\n\nArcticDBConfig\n\n ArcticDBConfig (db_path:str='/nfs/home/nicolasp/home/data/arctic',\n                 library:str='2021', columns_per_segment:int=63)\n\n\n\nCode\ndef register_configs(config_name=\"config\") -&gt; None:\n    \"\"\"Register `MainConfig` class instance into `config_name` name so that hydra is able to access it.\"\"\"\n    cs = ConfigStore.instance()\n\n    cs.store(group=\"hyperparameters\", name=\"simple\", node=SimpleHyperparametersConfig)\n    cs.store(group=\"hyperparameters\", name=\"full\", node=FullHyperparametersConfig)\n\n    cs.store(group=\"universe\", name=\"simple_local\", node=SimpleLocalUniverseConfig)\n    cs.store(group=\"universe\", name=\"simple_server\", node=SimpleServerUniverseConfig)\n    cs.store(group=\"universe\", name=\"XLE\", node=UniverseConfig(etfs=[\"XLE\"]))\n    cs.store(group=\"universe\", name=\"SPY\", node=UniverseConfig(etfs=[\"SPY\"]))\n\n    cs.store(group=\"data_config\", name=\"local\", node=LocalDataConfig)\n    cs.store(group=\"data_config\", name=\"server\", node=ServerDataConfig)\n\n    cs.store(name=config_name, node=MainConfig)\n\n\ndef get_config(overrides: list[str] | None = None) -&gt; MainConfig:\n    \"\"\"For config access from Jupyter notebooks. See the [Hydra Compose API](https://hydra.cc/docs/advanced/compose_api/).\"\"\"\n    config_name = \"config\"\n    register_configs(config_name=config_name)\n    with initialize(version_base=None, config_path=None):\n        cfg = OmegaConf.to_object(compose(config_name=config_name, overrides=overrides))\n        cfg = t.cast(MainConfig, cfg)\n    return cfg\n\n\n\nsource\n\n\nget_config\n\n get_config (overrides:list[str]|None=None)\n\nFor config access from Jupyter notebooks. See the Hydra Compose API.\n\nsource\n\n\nregister_configs\n\n register_configs (config_name='config')\n\nRegister MainConfig class instance into config_name name so that hydra is able to access it.\nThe following Overrides dataclass provides several simple default configurations for testing scripts on server or local data, and on a smaller universe or the full universe.\n\n\nCode\n@dataclass\nclass Overrides:\n    \"\"\"Common overrides for running scripts or notebooks on the server or locally.\"\"\"\n\n    simple_local = None  # simple_local is the default\n    simple_server = [\n        \"data_config=server\",\n        \"hyperparameters=simple\",\n        \"universe=simple_server\",\n    ]\n    full_server = [\n        \"data_config=server\",\n        \"hyperparameters=full\",\n        \"universe=XLE\",\n    ]\n\n\n\nsource\n\n\nOverrides\n\n Overrides ()\n\nCommon overrides for running scripts or notebooks on the server or locally."
  },
  {
    "objectID": "arctic_cli.html",
    "href": "arctic_cli.html",
    "title": "ArcticDB CLI",
    "section": "",
    "text": "source\n\nArcticLibraryInfo\n\n ArcticLibraryInfo (ticker:str, dates_ndarray:numpy.ndarray,\n                    dates_series:pandas.core.series.Series)\n\n\nsource\n\n\nget_library_info\n\n get_library_info (arctic_library:arcticdb.version_store.library.Library,\n                   tickers:list[str]|None=None)\n\nReturn information about ticker info in database.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\narctic_library\nLibrary\n\narcticdb library\n\n\ntickers\nlist[str] | None\nNone\ntickers to filter on\n\n\nReturns\nlist\n\n\n\n\n\n\n\n\n\n\n &lt;Command single-write&gt; (*args:Any, **kwargs:Any)\n\nWrite single ticker to database.\n\n\n\n\n\n &lt;Command diff&gt; (*args:Any, **kwargs:Any)\n\nTickers still to be written to database.\n\n\n\n\n\n &lt;Command attach&gt; (*args:Any, **kwargs:Any)\n\nAttach extra matadata to JSON objects read from stdin.\n\n\n\n\n\n &lt;Command finfo&gt; (*args:Any, **kwargs:Any)\n\nOutput json objects with folder information.\n\n\n\n\n\n &lt;Command filter&gt; (*args:Any, **kwargs:Any)\n\nFilter by ticker. Reads json objects from stdin and writes filtered objects to stdout.\n\n\n\n\n\n &lt;Command query&gt; (*args:Any, **kwargs:Any)\n\nWrite a custom query using a string template. Reads json objects from stdin and writes queries to stdout.\n\n\n\n\n\n &lt;Command library&gt; (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n &lt;Group rm&gt; (*args:Any, **kwargs:Any)\n\nRemove.\n\n\n\n\n\n &lt;Command dates&gt; (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n &lt;Command versions&gt; (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n &lt;Command symbols&gt; (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n &lt;Command libraries&gt; (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n &lt;Group ls&gt; (*args:Any, **kwargs:Any)\n\nList information about a library\n\n\n\n\n\n &lt;Command create&gt; (*args:Any, **kwargs:Any)\n\nCreate a blank library\n\n\n\n\n\n &lt;Command echo&gt; (*args:Any, **kwargs:Any)\n\nDebugging tool that echoes back the arctic object.\n\n\n\n\n\n &lt;Group arctic&gt; (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n &lt;Command pfmt&gt; (*args:Any, **kwargs:Any)\n\nSimple jq like utility to pretty format json objects.\n\n\n\n\n\n &lt;Command etf&gt; (*args:Any, **kwargs:Any)\n\nOutput constituents of ETF including the ETF itself\n\nsource\n\n\nClickCtx\n\n ClickCtx (*args, **kwargs)\n\nBase class for protocol classes.\nProtocol classes are defined as::\nclass Proto(Protocol):\n    def meth(self) -&gt; int:\n        ...\nSuch classes are primarily used with static type checkers that recognize structural subtyping (static duck-typing).\nFor example::\nclass C:\n    def meth(self) -&gt; int:\n        return 0\n\ndef func(x: Proto) -&gt; int:\n    return x.meth()\n\nfunc(C())  # Passes static type check\nSee PEP 544 for details. Protocol classes decorated with @typing.runtime_checkable act as simple-minded runtime protocols that check only the presence of given attributes, ignoring their type signatures. Protocol classes can be generic, they are defined as::\nclass GenProto(Protocol[T]):\n    def meth(self) -&gt; T:\n        ...\n\nsource\n\n\nClickCtxObj\nPurely for type hinting. for instance arctic_library not always there.\n\nsource\n\n\nConsoleNotify\n\n ConsoleNotify ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nOptions\n\n Options ()\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting started",
    "section": "",
    "text": "$ pip install lobster-tools"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Getting started",
    "section": "",
    "text": "$ pip install lobster-tools"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Getting started",
    "section": "How to use",
    "text": "How to use\n\ndata = Data(ticker=\"AMZN\", date_range=\"2012-06-21\", load=\"both\")\nlobster = Lobster(data=data)\n\n/nfs/home/nicolasp/anaconda3/envs/lob/lib/python3.11/site-packages/lobster_tools/preprocessing.py:320: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[]' has dtype incompatible with int8, please explicitly cast to a compatible dtype first.\n  df.loc[df.event.eq(Event.ORIGINAL_TRADING_HALT.value), \"event\"] = df.loc[\n\n\n\nlobster.messages.head()\n\n\n\n\n\ntime\nevent\norder_id\nsize\nprice\ndirection\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n2012-06-21 09:30:00.017459617\n34200.017460\n5\n0\n1\n223.82\n-1\n\n\n2012-06-21 09:30:00.189607670\n34200.189608\n1\n11885113\n21\n223.81\n1\n\n\n2012-06-21 09:30:00.189607670\n34200.189608\n1\n3911376\n20\n223.96\n-1\n\n\n2012-06-21 09:30:00.189607670\n34200.189608\n1\n11534792\n100\n223.75\n1\n\n\n2012-06-21 09:30:00.189607670\n34200.189608\n1\n1365373\n13\n224.00\n-1\n\n\n\n\n\n\nlobster.book.head()\n\n\n\n\n\nask_price_1\nask_size_1\nbid_price_1\nbid_size_1\nask_price_2\nask_size_2\nbid_price_2\nbid_size_2\nask_price_3\nask_size_3\nbid_price_3\nbid_size_3\nask_price_4\nask_size_4\nbid_price_4\nbid_size_4\nask_price_5\nask_size_5\nbid_price_5\nbid_size_5\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012-06-21 09:30:00.017459617\n223.95\n100.0\n223.18\n100.0\n223.99\n100.0\n223.07\n200.0\n224.00\n220.0\n223.04\n100.0\n224.25\n100.0\n223.00\n10.0\n224.40\n547.0\n222.62\n100.0\n\n\n2012-06-21 09:30:00.189607670\n223.95\n100.0\n223.81\n21.0\n223.99\n100.0\n223.18\n100.0\n224.00\n220.0\n223.07\n200.0\n224.25\n100.0\n223.04\n100.0\n224.40\n547.0\n223.00\n10.0\n\n\n2012-06-21 09:30:00.189607670\n223.95\n100.0\n223.81\n21.0\n223.96\n20.0\n223.18\n100.0\n223.99\n100.0\n223.07\n200.0\n224.00\n220.0\n223.04\n100.0\n224.25\n100.0\n223.00\n10.0\n\n\n2012-06-21 09:30:00.189607670\n223.95\n100.0\n223.81\n21.0\n223.96\n20.0\n223.75\n100.0\n223.99\n100.0\n223.18\n100.0\n224.00\n220.0\n223.07\n200.0\n224.25\n100.0\n223.04\n100.0\n\n\n2012-06-21 09:30:00.189607670\n223.95\n100.0\n223.81\n21.0\n223.96\n20.0\n223.75\n100.0\n223.99\n100.0\n223.18\n100.0\n224.00\n233.0\n223.07\n200.0\n224.25\n100.0\n223.04\n100.0\n\n\n\n\n\n\n(\n    lobster.messages.query(f\"event == {Event.HIDDEN_EXECUTION.value}\")\n    .query(f\"direction == -1\")\n    .head()\n)\n\n\n\n\n\ntime\nevent\norder_id\nsize\nprice\ndirection\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n2012-06-21 09:30:00.017459617\n34200.017460\n5\n0\n1\n223.82\n-1\n\n\n2012-06-21 09:30:00.372779672\n34200.372780\n5\n0\n100\n223.84\n-1\n\n\n2012-06-21 09:30:00.375671205\n34200.375671\n5\n0\n100\n223.84\n-1\n\n\n2012-06-21 09:30:00.383971366\n34200.383971\n5\n0\n100\n223.86\n-1\n\n\n2012-06-21 09:30:00.385815710\n34200.385816\n5\n0\n100\n223.86\n-1"
  }
]